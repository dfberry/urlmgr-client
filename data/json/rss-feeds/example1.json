{
    "status": "ok",
    "feed": {
        "url": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/feeds/posts/default",
        "title": "Project 31-A",
        "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/",
        "author": "Wayne Walter Berry",
        "description": "A Windows Programming Blog",
        "image": ""
    },
    "items": [
        {
            "title": "Docker for Angular 2 devs",
            "pubDate": "2016-10-20 17:05:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/10/docker-for-angular-2-devs.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-7909625889795073320",
            "author": "Dina Berry",
            "thumbnail": "https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s72-c/Snip20161019_5.png",
            "description": "\n<b>Docker is a Virtual Environment </b><br>\nDocker containers are great for adding new developers to existing projects, or for learning new technologies without polluting your existing developer machine/host. Docker allows you to put a fence around the environment while still using it. <br><br><b>Why Docker for Angular 2?</b><br>\nDocker is an easy way to get up and going on a new stack, environment, tool, or operating system without having to learn how to install and configure the new stack. A collection of docker images are available from <a href=\"https://hub.docker.com/\">Docker Hub</a> ranging from simple to incredibly complex -- saving you the time and energy.<br><br>\nAngular 2 examples frequently include a Dockerfile in the repository which makes getting the example up and running much quicker -- if you don't have to focus on package installation and configuration.<br><br>\nThe base Angular 2 development stack uses Node, TypeScript, Typings, and a build system (such as <a href=\"https://github.com/systemjs/systemjs\">SystemJs</a> or <a href=\"https://webpack.github.io/\">Webpack</a>). Instead of learning each stack element before/while learning Angular 2, just focus on Angular 2 itself -- by using a Dockerfile to bring up a working environment.<br><br>\nThe repositories for Angular 2 projects will have a package.json file at the root which is common for NodeJs/NPM package management projects. The Docker build will install the packages in the package management system as part of the build. The build can also transpile the typescript code , and start a static file web server -- if the package.json has a start script. <br><br>\nIn order to get a new development environment up and a new project in the browser, you just need to build the Dockerfile, then run it. Running these two commands at the terminal/cli saves you time in find and learning the Angular 2 stack, and then building and running the project.<br><br><b>The Angular 2 Quickstart</b><br>\nFor this article, I use the <a href=\"https://github.com/angular/quickstart\">Angular 2 Quickstart repository</a> including the <a href=\"https://github.com/angular/quickstart/blob/master/Dockerfile\">Dockerfile</a> found in the repository.<br><br>\nI use a Macintosh laptop. If you are using a Windows-based computer/host, you may have more or different issues than this article.<br><br><b>Docker via Terminal/Cli</b><br>\nI prefer the code-writing environment and web browser already installed and configured on my developer laptop/host. I configure the Docker container to share the hosted files. The changes are reflected in the container – and I run the Angular 2 project in watch mode so the changes immediately force a recompile in the container.<br><br><b>Viewing the Angular Project in a Browser</b><br>\nSince the Angular 2 project is a website, I access the container by the port and map the container's port to the host's port – so access to the running Angular project is from a web browser on the host laptop with http://localhost:3000. <br><br><b>Install Docker </b><br>\nBefore you install Docker, make sure you have a bit of space on the computer. Docker, like <a href=\"https://www.vagrantup.com/\">Vagrant</a><br>\nand <a href=\"https://www.virtualbox.org/wiki/Downloads\">VirtualBox</a>, uses a lot of space. <br><br>\nGo to <a href=\"https://www.docker.com/\">Docker</a> and install it. Start Docker up.<br><br><b>Check Docker </b><br>\nOpen a terminal/cli and check the install worked and Docker started by requesting the Docker version <br><br><div>\n<pre><code>\ndocker –v\n&gt;Docker version 1.12.1, build 6f9534c \n</code></pre>\n</div>\n<br>\nIf you get a docker version as a response, you installed and started Docker correctly. <br><br><b>Images and Containers </b><br>\nDocker Images are defined in the Dockerfile and represent the virtual machine to be built. The instantiation of the image is a container. You can have many containers based on one image. <br><br>\nEach image is named and each container can also be named. You can use these names to indicate ownership (who created it), as well as base image (node), and purpose (xyzProject). <br><br>\nPick a naming schema for your images and containers and stick with it.<br><br>\nI like to name my images with my github name and the general name such as dfberry/quickstart. I like to name the containers with as specific a name as possible such as ng2-quickstart.<br><br>\nThe list of containers (running or stopped) shows both names which can help you organize find the container you want.<br><br><b>The Angular 2 Docker Image </b><br>\nThe fastest way to get going with Docker for Angular 2 projects is to use the latest node as your base image -- which is also what the Angular 2 quickstart uses.<br><br>\nThe image has the latest node, npm, and git. <a href=\"https://hub.docker.com/\">Docker hub</a> hosts the <a href=\"https://hub.docker.com/_/node/\">base image</a> and <a href=\"https://nodejs.org/\">Node</a> keeps it up to date. <br><br>\nDocker's philosophy is that the containers are meant to execute then terminate with the least privileges possible. In order to make a container work as a development container (i.e. stay up and running), I'll show some not-best-practice choices. This will allow you to get up and going quickly. When you understand the Docker flow, you can implement your own security. <br><br><b>The Docker Images </b><br>\nDocker provides no images on installation. I can see that using the command <br><br><div>\n<pre><code>\ndocker images \n</code></pre>\n</div>\n<br>\nWhen I build the nodejs image, it will appear in the list with information about the image.<br><br><div class=\"separator\">\n<a href=\"https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s1600/Snip20161019_5.png\" imageanchor=\"1\"><img border=\"0\" height=\"82\" src=\"https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s640/Snip20161019_5.png\" width=\"640\"></a>\n</div>\n<br><br>\nFor now, the two most important columns are the REPOSITORY and IMAGE ID. The REPOSITORY field is the image name I used to build the image. My naming schema indicates my user account (dfberry) and the base image or purpose (node). This helps me find it in the image list.<br><br>\nThe IMAGE ID is the unique id used to identify the image. <br><br><b>The Dockerfile </b><br>\nIn order to create a docker image, you need a Dockerfile (notice the filename has no extension). This is the file the docker cli will assume you want to use. For this example, the Dockerfile is small. It has the following features:<br><ul>\n<li>creates a group</li>\n<li>creates a user</li>\n<li>creates a directory structure with appropriate permissions</li>\n<li>copies over the package.json file from the host</li>\n<li>installs the npm packages listed in the package.json</li>\n<li>runs the package.json's \"start\" script – which should start the website</li>\n</ul>\n<br>\nFor now, make sure this is the only Dockerfile in the root of the project, or anywhere below the root.<br><br><div>\n<pre><code>\n# To build and run with Docker:\n#\n# $ docker build -t ng-quickstart .\n# $ docker run -it --rm -p 3000:3000 -p 3001:3001 ng-quickstart\n#\nFROM node:latest\n\nRUN mkdir -p /quickstart /home/nodejs &amp;&amp; \\\ngroupadd -r nodejs &amp;&amp; \\\nuseradd -r -g nodejs -d /home/nodejs -s /sbin/nologin nodejs &amp;&amp; \\\nchown -R nodejs:nodejs /home/nodejs\n\nWORKDIR /quickstart\nCOPY package.json typings.json /quickstart/\nRUN npm install --unsafe-perm=true\n\nCOPY . /quickstart\nRUN chown -R nodejs:nodejs /quickstart\nUSER nodejs\n\nCMD npm start\n</code></pre>\n</div>\n<br>\nThe nodejs base image will install nodejs, npm and git. The image will just be used for building and hosting the Angular 2 project.<br><br>\nIf you have scripts that do the bulk of your build/startup/run process, change the Dockerfile to copy that file to the container and execute it as part of the build.<br><br><b> Build the Image</b><br><a href=\"https://www.blogger.com/(https://docs.docker.com/engine/reference/commandline/build/)\">Usage: docker build [OPTIONS] PATH | URL | - </a><br><br>\nIn order to build the image, use the docker cli. <br><br><div>\n<pre><code>\ndocker build –t &lt;user&gt;/&lt;yourimagename&gt; .\nExample $: docker build –t dfberry/ng-quickstart .\n</code></pre>\n</div>\n<br><br>\nIf you don't want to annotate the user, just leave that off.<br><br><div>\n<pre><code>\ndocker build –t &lt;yourimagename&gt; .\nExample $: docker build –t ng-quickstart .\n</code></pre>\n</div>\n<br>\nNote: the '.' at the end of the string is the url/location of the Dockerfile. I could have used a Github repository url instead of the local folder.<br><br>\nIn the above examples, the REPOSITORY name is 'ng-quickstart'. If you don't use the –t naming param, your image will have a name of &lt;none&gt; which is annoying when they pile up on a team server.<br><br>\nThe build will give you some feedback to let you know how it is going.<br><br><div>\n<pre><code>\nSending build context to Docker daemon 3.072 kB \nStep 1 : FROM node:latest </code></pre>\n<pre><code>\n... </code></pre>\n<pre><code>\nRemoving intermediate container 2cb50f334393 \nSuccessfully built 1265b22b5b90\n</code></pre>\n</div>\n<br>\nSince the build can return a lot of information, I didn't include the entire response. <br><br>\nThe build of the quickstart takes less than a minute on my Mac.<br><br>\nThe last line gives you the IMAGE ID. Remember to view all docker images after building to check it worked as expected.<br><br><div>\n<pre><code>\ndocker images\n</code></pre>\n</div>\n<br><b> Run the Container</b> <br><a href=\"https://docs.docker.com/engine/reference/run/\">Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] </a><br><br>\nNow that the image is built, I want to run the image to see the website.<br><br>\nIf you don't have an Angular 2/Typescript website, use the <a href=\"https://github.com/angular/quickstart\">ng2 Quickstart</a>. <br><br><b>Run switches</b><br>\nThe run command has a lot of switches and configurations. I'll walk you through the choices for this container.<br><br>\nI want to name the container so that I remember the purpose. This is optional but helpful when you have a long list of containers.<br><br><div>\n<pre><code>\n--name ng2-quickstart \n</code></pre>\n</div>\n<br>\nI want to make sure the container's web ports are matched to my host machine's port so I can see the website as http://localhost:3000. Make sure the port isn't already in use on your host machine.<br><br><div>\n<pre><code>\n-p 3000:3000 \n</code></pre>\n</div>\n<br>\nI want to map my HOST directory (/Users/dfberry/quickstart to the container's directory (/home/nodejs/quickstart) that was created in the build so I can edit on my laptop and the changes are reflected in the container. The /home/nodejs/quickstart directory was created as part of the Dockerfile.<br><br><div>\n<pre><code>\n-v /Users/dfberry/quickstart/:/home/nodejs/quickstart \n</code></pre>\n</div>\n<br>\nI want the terminal/cli to show the container's responses including transpile status and the file requests.<br><br><div>\n<pre><code>\n-it \n</code></pre>\n</div>\n<br><br>\nThe full command is:<br><br><div>\n<pre><code>\ndocker run -it -p 3000:3000 -v /Users/dfberry/quickstart:/home/nodejs/quickstart --name ng2-quickstart dfberry/ng-quickstart\n</code></pre>\n</div>\n<br>\nNotice the image is named dfberry/ng-quickstart while the container is named ng2-quickstart.<br><br>\nRun the \"docker run\" command at the terminal/cli.<br><br>\nThe container should be up and the website should be transpiled and running.<br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-NCPqsoE-BjU/WAfcGB3Iq1I/AAAAAAAAAmQ/aB5GLmIc2pwhndXcqsYL1pu_jGg24zorQCLcB/s1600/Snip20161019_7.png\" imageanchor=\"1\"><img border=\"0\" height=\"336\" src=\"https://2.bp.blogspot.com/-NCPqsoE-BjU/WAfcGB3Iq1I/AAAAAAAAAmQ/aB5GLmIc2pwhndXcqsYL1pu_jGg24zorQCLcB/s640/Snip20161019_7.png\" width=\"640\"></a>\n</div>\n<br>\nAt this point, you should be able to work on the website code on your host with your usual editing software and the changes will reflect in the container (re-transpile changes) and in the web browser.<br><br><b>List Docker Containers</b><br>\nIn order to see all the containers, use<br><br><div>\n<pre><code>\ndocker ps -a\n</code></pre>\n</div>\n<br>\nIf you only want to see the running containers, leave the -a off. <br><br><br><div>\n<pre><code>\ndocker ps\n</code></pre>\n</div>\n<br><div class=\"separator\">\n<a href=\"https://1.bp.blogspot.com/-2vRCvFThxXc/WAfnWwDVQ9I/AAAAAAAAAmo/L8uWsElKrdkACeaKo1WBBAOT7pDmhFn5ACLcB/s1600/Snip20161019_9.png\" imageanchor=\"1\"><img border=\"0\" height=\"17\" src=\"https://1.bp.blogspot.com/-2vRCvFThxXc/WAfnWwDVQ9I/AAAAAAAAAmo/L8uWsElKrdkACeaKo1WBBAOT7pDmhFn5ACLcB/s640/Snip20161019_9.png\" width=\"640\"></a>\n</div>\n<br><br>\nAt this point, the easy docker commands are done and you can work on the website and forget about Docker for a while. When you are ready to stop the container, stop Docker or get out of interactive mode, read on.<br><br><b>Interactive Mode (-it) versus Detached Mode (-d) </b><br>\nInteractive Mode means the terminal/cli shows what is happening to your website in the container. Detached mode means the terminal/cli doesn't show what is happening and the terminal/cli returns to your control for other commands on your host.<br><br>\nTo move from interactive to detached mode, use control + p + control + q.<br><br>\nThis leaves the container up but you have no visual/textual feedback about how the website is working from the container. You can use the developer tools/F12 in the browser to get a sense, but won't be able to see http requests and transpiles.<br><br>\nYou are either comfortable with that or not.<br><br>\nIf you want the interactive mode and the website transpile/http request information, don't exit interactive mode. Instead, use control + c. This command stops and removes the container from Docker, but doesn't remove the image. You can re-enter interactive mode with the same run command above.<br><br>\nIf you are more comfortable in detached mode, where the website gives transpiles and http request information via a different method such as logging to a file or cloud service, change the docker run command.<br><br>\nInstead of using –it as part of the \"docker run\" command, use –d for detached mode.<br><b><br></b>\n<b>Exec Mode to run commands on container</b> <br><a href=\"https://docs.docker.com/engine/reference/commandline/exec/\">Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]</a><br><br>\nWhen you want to connect to the container, you the same -it for interactive mode but with \"docker exec.\"  The end command tells the docker container what environment to enter in the container -- such as the bash shell. <br><br><div>\n<pre><code>\ndocker exec –it ng2-quickstart /bin/bash\n</code></pre>\n</div>\n<br>\nYou can log in as root if you need elevated privileges.<br><br><div>\n<pre><code>\ndocker exec –it –u root ng2-quickstart /bin/bash \n</code></pre>\n</div>\n<br>\nThe terminal/cli should now show the prompt changed to indicate you are now on the container: <br><br><div>\n<pre><code>\nnodejs@faf83c87c12e:/quickstart$ \n</code></pre>\n</div>\n<br>\nWhen you are done running the commands, use control + p + control + q to exit. The container is still running. <br><br><b>Sudo or Root</b><br>\nIn this particular quickstart nodejs docker container, sudo has not been installed. Sudo may be your first choice and you can install. Or you could use the \"docker exec\" with root. Either way has pros and cons.<br><br><b>Stopping and Starting the Container </b><br>\nWhen you are done with the container, you need to stop it. You can stop, and restart it as you need by container id or name.<br><br><div>\n<pre><code>\ndocker stop ng2-quickstart \ndocker stop 7449222ec26b </code></pre>\n<pre><code>\ndocker start ng2-quickstart \ndocker start 7449222ec26b \n</code></pre>\n</div>\n<br>\nStopping the container may take some time – be patient. Mine takes up to 10 seconds on my Mac. When you restart the container, it is in detached mode. If you really want interactive mode, remove the container, and use docker run again with –it.<br><br><b>Cleanup </b><br>\nMake sure to stop all containers when they are not needed. When you are done with a container, you can remove it<br><br><div>\n<pre><code>\ndocker rm –fv ng2-quickstart \n</code></pre>\n</div>\n<br>\nWhen you are done with an image, you can remove that as well <br><br><div>\n<pre><code>\ndocker rmi ng-quickstart \n</code></pre>\n</div>\n<br><b>Stop Docker</b><br>\nRemember to stop Docker when you are done with the containers for the moment or day.",
            "content": "\n<b>Docker is a Virtual Environment </b><br>\nDocker containers are great for adding new developers to existing projects, or for learning new technologies without polluting your existing developer machine/host. Docker allows you to put a fence around the environment while still using it. <br><br><b>Why Docker for Angular 2?</b><br>\nDocker is an easy way to get up and going on a new stack, environment, tool, or operating system without having to learn how to install and configure the new stack. A collection of docker images are available from <a href=\"https://hub.docker.com/\">Docker Hub</a> ranging from simple to incredibly complex -- saving you the time and energy.<br><br>\nAngular 2 examples frequently include a Dockerfile in the repository which makes getting the example up and running much quicker -- if you don't have to focus on package installation and configuration.<br><br>\nThe base Angular 2 development stack uses Node, TypeScript, Typings, and a build system (such as <a href=\"https://github.com/systemjs/systemjs\">SystemJs</a> or <a href=\"https://webpack.github.io/\">Webpack</a>). Instead of learning each stack element before/while learning Angular 2, just focus on Angular 2 itself -- by using a Dockerfile to bring up a working environment.<br><br>\nThe repositories for Angular 2 projects will have a package.json file at the root which is common for NodeJs/NPM package management projects. The Docker build will install the packages in the package management system as part of the build. The build can also transpile the typescript code , and start a static file web server -- if the package.json has a start script. <br><br>\nIn order to get a new development environment up and a new project in the browser, you just need to build the Dockerfile, then run it. Running these two commands at the terminal/cli saves you time in find and learning the Angular 2 stack, and then building and running the project.<br><br><b>The Angular 2 Quickstart</b><br>\nFor this article, I use the <a href=\"https://github.com/angular/quickstart\">Angular 2 Quickstart repository</a> including the <a href=\"https://github.com/angular/quickstart/blob/master/Dockerfile\">Dockerfile</a> found in the repository.<br><br>\nI use a Macintosh laptop. If you are using a Windows-based computer/host, you may have more or different issues than this article.<br><br><b>Docker via Terminal/Cli</b><br>\nI prefer the code-writing environment and web browser already installed and configured on my developer laptop/host. I configure the Docker container to share the hosted files. The changes are reflected in the container – and I run the Angular 2 project in watch mode so the changes immediately force a recompile in the container.<br><br><b>Viewing the Angular Project in a Browser</b><br>\nSince the Angular 2 project is a website, I access the container by the port and map the container's port to the host's port – so access to the running Angular project is from a web browser on the host laptop with http://localhost:3000. <br><br><b>Install Docker </b><br>\nBefore you install Docker, make sure you have a bit of space on the computer. Docker, like <a href=\"https://www.vagrantup.com/\">Vagrant</a><br>\nand <a href=\"https://www.virtualbox.org/wiki/Downloads\">VirtualBox</a>, uses a lot of space. <br><br>\nGo to <a href=\"https://www.docker.com/\">Docker</a> and install it. Start Docker up.<br><br><b>Check Docker </b><br>\nOpen a terminal/cli and check the install worked and Docker started by requesting the Docker version <br><br><div>\n<pre><code>\ndocker –v\n&gt;Docker version 1.12.1, build 6f9534c \n</code></pre>\n</div>\n<br>\nIf you get a docker version as a response, you installed and started Docker correctly. <br><br><b>Images and Containers </b><br>\nDocker Images are defined in the Dockerfile and represent the virtual machine to be built. The instantiation of the image is a container. You can have many containers based on one image. <br><br>\nEach image is named and each container can also be named. You can use these names to indicate ownership (who created it), as well as base image (node), and purpose (xyzProject). <br><br>\nPick a naming schema for your images and containers and stick with it.<br><br>\nI like to name my images with my github name and the general name such as dfberry/quickstart. I like to name the containers with as specific a name as possible such as ng2-quickstart.<br><br>\nThe list of containers (running or stopped) shows both names which can help you organize find the container you want.<br><br><b>The Angular 2 Docker Image </b><br>\nThe fastest way to get going with Docker for Angular 2 projects is to use the latest node as your base image -- which is also what the Angular 2 quickstart uses.<br><br>\nThe image has the latest node, npm, and git. <a href=\"https://hub.docker.com/\">Docker hub</a> hosts the <a href=\"https://hub.docker.com/_/node/\">base image</a> and <a href=\"https://nodejs.org/\">Node</a> keeps it up to date. <br><br>\nDocker's philosophy is that the containers are meant to execute then terminate with the least privileges possible. In order to make a container work as a development container (i.e. stay up and running), I'll show some not-best-practice choices. This will allow you to get up and going quickly. When you understand the Docker flow, you can implement your own security. <br><br><b>The Docker Images </b><br>\nDocker provides no images on installation. I can see that using the command <br><br><div>\n<pre><code>\ndocker images \n</code></pre>\n</div>\n<br>\nWhen I build the nodejs image, it will appear in the list with information about the image.<br><br><div class=\"separator\">\n<a href=\"https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s1600/Snip20161019_5.png\" imageanchor=\"1\"><img border=\"0\" height=\"82\" src=\"https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s640/Snip20161019_5.png\" width=\"640\"></a>\n</div>\n<br><br>\nFor now, the two most important columns are the REPOSITORY and IMAGE ID. The REPOSITORY field is the image name I used to build the image. My naming schema indicates my user account (dfberry) and the base image or purpose (node). This helps me find it in the image list.<br><br>\nThe IMAGE ID is the unique id used to identify the image. <br><br><b>The Dockerfile </b><br>\nIn order to create a docker image, you need a Dockerfile (notice the filename has no extension). This is the file the docker cli will assume you want to use. For this example, the Dockerfile is small. It has the following features:<br><ul>\n<li>creates a group</li>\n<li>creates a user</li>\n<li>creates a directory structure with appropriate permissions</li>\n<li>copies over the package.json file from the host</li>\n<li>installs the npm packages listed in the package.json</li>\n<li>runs the package.json's \"start\" script – which should start the website</li>\n</ul>\n<br>\nFor now, make sure this is the only Dockerfile in the root of the project, or anywhere below the root.<br><br><div>\n<pre><code>\n# To build and run with Docker:\n#\n# $ docker build -t ng-quickstart .\n# $ docker run -it --rm -p 3000:3000 -p 3001:3001 ng-quickstart\n#\nFROM node:latest\n\nRUN mkdir -p /quickstart /home/nodejs &amp;&amp; \\\ngroupadd -r nodejs &amp;&amp; \\\nuseradd -r -g nodejs -d /home/nodejs -s /sbin/nologin nodejs &amp;&amp; \\\nchown -R nodejs:nodejs /home/nodejs\n\nWORKDIR /quickstart\nCOPY package.json typings.json /quickstart/\nRUN npm install --unsafe-perm=true\n\nCOPY . /quickstart\nRUN chown -R nodejs:nodejs /quickstart\nUSER nodejs\n\nCMD npm start\n</code></pre>\n</div>\n<br>\nThe nodejs base image will install nodejs, npm and git. The image will just be used for building and hosting the Angular 2 project.<br><br>\nIf you have scripts that do the bulk of your build/startup/run process, change the Dockerfile to copy that file to the container and execute it as part of the build.<br><br><b> Build the Image</b><br><a href=\"https://www.blogger.com/(https://docs.docker.com/engine/reference/commandline/build/)\">Usage: docker build [OPTIONS] PATH | URL | - </a><br><br>\nIn order to build the image, use the docker cli. <br><br><div>\n<pre><code>\ndocker build –t &lt;user&gt;/&lt;yourimagename&gt; .\nExample $: docker build –t dfberry/ng-quickstart .\n</code></pre>\n</div>\n<br><br>\nIf you don't want to annotate the user, just leave that off.<br><br><div>\n<pre><code>\ndocker build –t &lt;yourimagename&gt; .\nExample $: docker build –t ng-quickstart .\n</code></pre>\n</div>\n<br>\nNote: the '.' at the end of the string is the url/location of the Dockerfile. I could have used a Github repository url instead of the local folder.<br><br>\nIn the above examples, the REPOSITORY name is 'ng-quickstart'. If you don't use the –t naming param, your image will have a name of &lt;none&gt; which is annoying when they pile up on a team server.<br><br>\nThe build will give you some feedback to let you know how it is going.<br><br><div>\n<pre><code>\nSending build context to Docker daemon 3.072 kB \nStep 1 : FROM node:latest </code></pre>\n<pre><code>\n... </code></pre>\n<pre><code>\nRemoving intermediate container 2cb50f334393 \nSuccessfully built 1265b22b5b90\n</code></pre>\n</div>\n<br>\nSince the build can return a lot of information, I didn't include the entire response. <br><br>\nThe build of the quickstart takes less than a minute on my Mac.<br><br>\nThe last line gives you the IMAGE ID. Remember to view all docker images after building to check it worked as expected.<br><br><div>\n<pre><code>\ndocker images\n</code></pre>\n</div>\n<br><b> Run the Container</b> <br><a href=\"https://docs.docker.com/engine/reference/run/\">Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] </a><br><br>\nNow that the image is built, I want to run the image to see the website.<br><br>\nIf you don't have an Angular 2/Typescript website, use the <a href=\"https://github.com/angular/quickstart\">ng2 Quickstart</a>. <br><br><b>Run switches</b><br>\nThe run command has a lot of switches and configurations. I'll walk you through the choices for this container.<br><br>\nI want to name the container so that I remember the purpose. This is optional but helpful when you have a long list of containers.<br><br><div>\n<pre><code>\n--name ng2-quickstart \n</code></pre>\n</div>\n<br>\nI want to make sure the container's web ports are matched to my host machine's port so I can see the website as http://localhost:3000. Make sure the port isn't already in use on your host machine.<br><br><div>\n<pre><code>\n-p 3000:3000 \n</code></pre>\n</div>\n<br>\nI want to map my HOST directory (/Users/dfberry/quickstart to the container's directory (/home/nodejs/quickstart) that was created in the build so I can edit on my laptop and the changes are reflected in the container. The /home/nodejs/quickstart directory was created as part of the Dockerfile.<br><br><div>\n<pre><code>\n-v /Users/dfberry/quickstart/:/home/nodejs/quickstart \n</code></pre>\n</div>\n<br>\nI want the terminal/cli to show the container's responses including transpile status and the file requests.<br><br><div>\n<pre><code>\n-it \n</code></pre>\n</div>\n<br><br>\nThe full command is:<br><br><div>\n<pre><code>\ndocker run -it -p 3000:3000 -v /Users/dfberry/quickstart:/home/nodejs/quickstart --name ng2-quickstart dfberry/ng-quickstart\n</code></pre>\n</div>\n<br>\nNotice the image is named dfberry/ng-quickstart while the container is named ng2-quickstart.<br><br>\nRun the \"docker run\" command at the terminal/cli.<br><br>\nThe container should be up and the website should be transpiled and running.<br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-NCPqsoE-BjU/WAfcGB3Iq1I/AAAAAAAAAmQ/aB5GLmIc2pwhndXcqsYL1pu_jGg24zorQCLcB/s1600/Snip20161019_7.png\" imageanchor=\"1\"><img border=\"0\" height=\"336\" src=\"https://2.bp.blogspot.com/-NCPqsoE-BjU/WAfcGB3Iq1I/AAAAAAAAAmQ/aB5GLmIc2pwhndXcqsYL1pu_jGg24zorQCLcB/s640/Snip20161019_7.png\" width=\"640\"></a>\n</div>\n<br>\nAt this point, you should be able to work on the website code on your host with your usual editing software and the changes will reflect in the container (re-transpile changes) and in the web browser.<br><br><b>List Docker Containers</b><br>\nIn order to see all the containers, use<br><br><div>\n<pre><code>\ndocker ps -a\n</code></pre>\n</div>\n<br>\nIf you only want to see the running containers, leave the -a off. <br><br><br><div>\n<pre><code>\ndocker ps\n</code></pre>\n</div>\n<br><div class=\"separator\">\n<a href=\"https://1.bp.blogspot.com/-2vRCvFThxXc/WAfnWwDVQ9I/AAAAAAAAAmo/L8uWsElKrdkACeaKo1WBBAOT7pDmhFn5ACLcB/s1600/Snip20161019_9.png\" imageanchor=\"1\"><img border=\"0\" height=\"17\" src=\"https://1.bp.blogspot.com/-2vRCvFThxXc/WAfnWwDVQ9I/AAAAAAAAAmo/L8uWsElKrdkACeaKo1WBBAOT7pDmhFn5ACLcB/s640/Snip20161019_9.png\" width=\"640\"></a>\n</div>\n<br><br>\nAt this point, the easy docker commands are done and you can work on the website and forget about Docker for a while. When you are ready to stop the container, stop Docker or get out of interactive mode, read on.<br><br><b>Interactive Mode (-it) versus Detached Mode (-d) </b><br>\nInteractive Mode means the terminal/cli shows what is happening to your website in the container. Detached mode means the terminal/cli doesn't show what is happening and the terminal/cli returns to your control for other commands on your host.<br><br>\nTo move from interactive to detached mode, use control + p + control + q.<br><br>\nThis leaves the container up but you have no visual/textual feedback about how the website is working from the container. You can use the developer tools/F12 in the browser to get a sense, but won't be able to see http requests and transpiles.<br><br>\nYou are either comfortable with that or not.<br><br>\nIf you want the interactive mode and the website transpile/http request information, don't exit interactive mode. Instead, use control + c. This command stops and removes the container from Docker, but doesn't remove the image. You can re-enter interactive mode with the same run command above.<br><br>\nIf you are more comfortable in detached mode, where the website gives transpiles and http request information via a different method such as logging to a file or cloud service, change the docker run command.<br><br>\nInstead of using –it as part of the \"docker run\" command, use –d for detached mode.<br><b><br></b>\n<b>Exec Mode to run commands on container</b> <br><a href=\"https://docs.docker.com/engine/reference/commandline/exec/\">Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]</a><br><br>\nWhen you want to connect to the container, you the same -it for interactive mode but with \"docker exec.\"  The end command tells the docker container what environment to enter in the container -- such as the bash shell. <br><br><div>\n<pre><code>\ndocker exec –it ng2-quickstart /bin/bash\n</code></pre>\n</div>\n<br>\nYou can log in as root if you need elevated privileges.<br><br><div>\n<pre><code>\ndocker exec –it –u root ng2-quickstart /bin/bash \n</code></pre>\n</div>\n<br>\nThe terminal/cli should now show the prompt changed to indicate you are now on the container: <br><br><div>\n<pre><code>\nnodejs@faf83c87c12e:/quickstart$ \n</code></pre>\n</div>\n<br>\nWhen you are done running the commands, use control + p + control + q to exit. The container is still running. <br><br><b>Sudo or Root</b><br>\nIn this particular quickstart nodejs docker container, sudo has not been installed. Sudo may be your first choice and you can install. Or you could use the \"docker exec\" with root. Either way has pros and cons.<br><br><b>Stopping and Starting the Container </b><br>\nWhen you are done with the container, you need to stop it. You can stop, and restart it as you need by container id or name.<br><br><div>\n<pre><code>\ndocker stop ng2-quickstart \ndocker stop 7449222ec26b </code></pre>\n<pre><code>\ndocker start ng2-quickstart \ndocker start 7449222ec26b \n</code></pre>\n</div>\n<br>\nStopping the container may take some time – be patient. Mine takes up to 10 seconds on my Mac. When you restart the container, it is in detached mode. If you really want interactive mode, remove the container, and use docker run again with –it.<br><br><b>Cleanup </b><br>\nMake sure to stop all containers when they are not needed. When you are done with a container, you can remove it<br><br><div>\n<pre><code>\ndocker rm –fv ng2-quickstart \n</code></pre>\n</div>\n<br>\nWhen you are done with an image, you can remove that as well <br><br><div>\n<pre><code>\ndocker rmi ng-quickstart \n</code></pre>\n</div>\n<br><b>Stop Docker</b><br>\nRemember to stop Docker when you are done with the containers for the moment or day.",
            "enclosure": {
                "thumbnail": "https://4.bp.blogspot.com/-NTfqw9my-dg/WAfZX5h4v5I/AAAAAAAAAmM/9y6W3-YuzHw0CqlsilMWvdu8VJOHx5WQACLcB/s72-c/Snip20161019_5.png"
            },
            "categories": [
                "Docker",
                "angular",
                "typescript",
                "Dina",
                "ng",
                "ng2"
            ]
        },
        {
            "title": "An interesting Interview Question: Fibonacci Sequence",
            "pubDate": "2016-09-05 17:32:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/09/an-interesting-interview-question.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-5860679040618227316",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "https://lh3.googleusercontent.com/-FlpQrnrgHHY/V82sOzg54eI/AAAAAAAALEo/QkkDA-3TQg4/s72-c/image_thumb%25255B1%25255D.png?imgmax=800",
            "description": "\n<p>Write a function to calculate the nth Fibonacci Sequence is a common interview question and often the solution is something like</p> <p> </p> <p>int Fib(int n)</p> <p>{</p> <p>   if(n &lt; 1) return 1;</p> <p>   return Fib(n-1) + Fib(n-2);</p> <p>}</p> <p> </p> <p>The next question is to ask <strong>for n=100, how many items will be on the stack</strong>. The answer is not 100 but actually horrible! It is closer to 2^100.</p> <p>take the first call – we start a stack on Fib(99) and one on Fib(98). There is nothing to allow Fib(99) to <em>borrow </em>the result of Fib(98).  So one step is two stack items to recurse.  Each subsequent call changes one stack item into 2 items.   For example</p> <ul>\n<li>2 –&gt; call [Fib(1), Fib(0)]</li> <li>3 –&gt; calls [ Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]</li> <li>4 –&gt; calls [ Fib(3)-&gt;[[[ Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]], Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]</li> </ul>\n<p>Missing this issue is very often seen with by-rote developers (who are excellent for some tasks).</p> <p> </p> <p>A better solution is to cache the values as each one is computed – effectively creating a lookup table. You are trading stack space for memory space.</p> <p> </p> <p>Placing constraints on memory and stack space may force the developer to do some actual thinking. A solution that conforms to this is shown below</p> <p> </p> <p>  private static long Fibonacci(int n) { <br>        long a = 0L; <br>        long b = 1L; <br>        for (int i = 31; i &gt;= 0; i—)  //31 is arbitrary, see below</p> <p>        { </p> <p>            long d = a * (b * 2 - a); <br>            long e = a * a + b * b; <br>            a = d; <br>            b = e; <br>            if ((((uint)n &gt;&gt; i) &amp; 1) != 0) { <br>                long c = a + b; <br>                a = b; <br>                b = c; <br>                } <br>           } <br>        return a; <br>    }</p> <p> </p> <p>The output of the above shows what is happening  and suggests that the ”31”  taking the log base 2 of N can likely be done to improve efficiency</p> <p><a href=\"https://lh3.googleusercontent.com/-jbpMFGaAn4k/V82sOXqIhkI/AAAAAAAALEk/MjtppB_zIrk/s1600-h/image%25255B3%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-FlpQrnrgHHY/V82sOzg54eI/AAAAAAAALEo/QkkDA-3TQg4/image_thumb%25255B1%25255D.png?imgmax=800\" width=\"394\" height=\"397\"></a></p> <p>for 32:</p> <p><a href=\"https://lh3.googleusercontent.com/-dh81DAIENoU/V82sPW_BKaI/AAAAAAAALEs/COxAT89bJq4/s1600-h/image%25255B10%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-ACC5SuR3fJs/V82sP90PQjI/AAAAAAAALEw/APIdsqyCrjw/image_thumb%25255B4%25255D.png?imgmax=800\" width=\"394\" height=\"155\"></a></p> <p>for 65</p> <p><a href=\"https://lh3.googleusercontent.com/-isdGVl_-ytg/V82sQRe3l6I/AAAAAAAALE0/246DGNFevCc/s1600-h/image%25255B16%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-qPLUMvQCNeY/V82sQ0lTMgI/AAAAAAAALE4/XgzjKCfvp3c/image_thumb%25255B8%25255D.png?imgmax=800\" width=\"627\" height=\"177\"></a></p> <p>for 129</p> <p><a href=\"https://lh3.googleusercontent.com/-3b-my-NNzJI/V82sRPeZIWI/AAAAAAAALE8/nQlW9fueqpE/s1600-h/image%25255B17%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-8zL0c0yDWL8/V82sRiE8M6I/AAAAAAAALFA/1gVDX4d43ag/image_thumb%25255B9%25255D.png?imgmax=800\" width=\"630\" height=\"174\"></a></p> <p> </p> <p>What is the difference in performance for the naive vs the latter?</p> <p>I actually did not wait until the naive solution finished… I aborted at <strong>4 minutes</strong></p> <p><a href=\"https://lh3.googleusercontent.com/-2BlAWMem4II/V82sSDNckRI/AAAAAAAALFE/jliSER_KA_c/s1600-h/image%25255B21%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-CO_cqVnF8UQ/V82sSomkVGI/AAAAAAAALFI/7SZQ3ZyXHSQ/image_thumb%25255B11%25255D.png?imgmax=800\" width=\"571\" height=\"382\"></a></p> <p>The new improved version was 85 ms, over a 3000 fold improvement.</p> <h1>Take Away</h1> <p>This question:</p> <ol>\n<li>Identify if a person knows what recursion is and can code it.</li> <li>Identify if he understands what the consequence of recursion is and how it will be executed(i.e. think about what the code does)</li> <ol>\n<li>Most recursion questions are atomic (i.e. factorial) and not composite (recursion that is not simple)</li> </ol>\n<li>Is able to do analysis of a simple mathematical issue and generate a performing solution.</li> </ol>\n",
            "content": "\n<p>Write a function to calculate the nth Fibonacci Sequence is a common interview question and often the solution is something like</p> <p> </p> <p>int Fib(int n)</p> <p>{</p> <p>   if(n &lt; 1) return 1;</p> <p>   return Fib(n-1) + Fib(n-2);</p> <p>}</p> <p> </p> <p>The next question is to ask <strong>for n=100, how many items will be on the stack</strong>. The answer is not 100 but actually horrible! It is closer to 2^100.</p> <p>take the first call – we start a stack on Fib(99) and one on Fib(98). There is nothing to allow Fib(99) to <em>borrow </em>the result of Fib(98).  So one step is two stack items to recurse.  Each subsequent call changes one stack item into 2 items.   For example</p> <ul>\n<li>2 –&gt; call [Fib(1), Fib(0)]</li> <li>3 –&gt; calls [ Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]</li> <li>4 –&gt; calls [ Fib(3)-&gt;[[[ Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]], Fib(2)-&gt;[Fib(1), Fib(0)], Fib(1) –&gt; Fib(0) ]</li> </ul>\n<p>Missing this issue is very often seen with by-rote developers (who are excellent for some tasks).</p> <p> </p> <p>A better solution is to cache the values as each one is computed – effectively creating a lookup table. You are trading stack space for memory space.</p> <p> </p> <p>Placing constraints on memory and stack space may force the developer to do some actual thinking. A solution that conforms to this is shown below</p> <p> </p> <p>  private static long Fibonacci(int n) { <br>        long a = 0L; <br>        long b = 1L; <br>        for (int i = 31; i &gt;= 0; i—)  //31 is arbitrary, see below</p> <p>        { </p> <p>            long d = a * (b * 2 - a); <br>            long e = a * a + b * b; <br>            a = d; <br>            b = e; <br>            if ((((uint)n &gt;&gt; i) &amp; 1) != 0) { <br>                long c = a + b; <br>                a = b; <br>                b = c; <br>                } <br>           } <br>        return a; <br>    }</p> <p> </p> <p>The output of the above shows what is happening  and suggests that the ”31”  taking the log base 2 of N can likely be done to improve efficiency</p> <p><a href=\"https://lh3.googleusercontent.com/-jbpMFGaAn4k/V82sOXqIhkI/AAAAAAAALEk/MjtppB_zIrk/s1600-h/image%25255B3%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-FlpQrnrgHHY/V82sOzg54eI/AAAAAAAALEo/QkkDA-3TQg4/image_thumb%25255B1%25255D.png?imgmax=800\" width=\"394\" height=\"397\"></a></p> <p>for 32:</p> <p><a href=\"https://lh3.googleusercontent.com/-dh81DAIENoU/V82sPW_BKaI/AAAAAAAALEs/COxAT89bJq4/s1600-h/image%25255B10%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-ACC5SuR3fJs/V82sP90PQjI/AAAAAAAALEw/APIdsqyCrjw/image_thumb%25255B4%25255D.png?imgmax=800\" width=\"394\" height=\"155\"></a></p> <p>for 65</p> <p><a href=\"https://lh3.googleusercontent.com/-isdGVl_-ytg/V82sQRe3l6I/AAAAAAAALE0/246DGNFevCc/s1600-h/image%25255B16%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-qPLUMvQCNeY/V82sQ0lTMgI/AAAAAAAALE4/XgzjKCfvp3c/image_thumb%25255B8%25255D.png?imgmax=800\" width=\"627\" height=\"177\"></a></p> <p>for 129</p> <p><a href=\"https://lh3.googleusercontent.com/-3b-my-NNzJI/V82sRPeZIWI/AAAAAAAALE8/nQlW9fueqpE/s1600-h/image%25255B17%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-8zL0c0yDWL8/V82sRiE8M6I/AAAAAAAALFA/1gVDX4d43ag/image_thumb%25255B9%25255D.png?imgmax=800\" width=\"630\" height=\"174\"></a></p> <p> </p> <p>What is the difference in performance for the naive vs the latter?</p> <p>I actually did not wait until the naive solution finished… I aborted at <strong>4 minutes</strong></p> <p><a href=\"https://lh3.googleusercontent.com/-2BlAWMem4II/V82sSDNckRI/AAAAAAAALFE/jliSER_KA_c/s1600-h/image%25255B21%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-CO_cqVnF8UQ/V82sSomkVGI/AAAAAAAALFI/7SZQ3ZyXHSQ/image_thumb%25255B11%25255D.png?imgmax=800\" width=\"571\" height=\"382\"></a></p> <p>The new improved version was 85 ms, over a 3000 fold improvement.</p> <h1>Take Away</h1> <p>This question:</p> <ol>\n<li>Identify if a person knows what recursion is and can code it.</li> <li>Identify if he understands what the consequence of recursion is and how it will be executed(i.e. think about what the code does)</li> <ol>\n<li>Most recursion questions are atomic (i.e. factorial) and not composite (recursion that is not simple)</li> </ol>\n<li>Is able to do analysis of a simple mathematical issue and generate a performing solution.</li> </ol>\n",
            "enclosure": {
                "thumbnail": "https://lh3.googleusercontent.com/-FlpQrnrgHHY/V82sOzg54eI/AAAAAAAALEo/QkkDA-3TQg4/s72-c/image_thumb%25255B1%25255D.png?imgmax=800"
            },
            "categories": []
        },
        {
            "title": "Apple Store Passbook UML Diagrams and Error Messages",
            "pubDate": "2016-08-28 16:20:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/08/apple-store-passbook-uml-diagrams-and.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-6528865794159524650",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "https://lh3.googleusercontent.com/-8wPvpvltbvU/V8MPXQ-WFhI/AAAAAAAAK-c/uAEczst5iEs/s72-c/image_thumb%25255B3%25255D.png?imgmax=800",
            "description": "\n<p>While working on a recent project, a major stumbling block was a lack of clear documentation of what happened where. This was confirmed when I attempted to search for some of the messages returned to the Log REST points by iPhone.. There were zero hits!</p> <p> </p> <p><a href=\"https://lh3.googleusercontent.com/-chO_aqROjEM/V8MPW2LRqSI/AAAAAAAAK-Y/jquJYxCG1v0/s1600-h/image%25255B5%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-8wPvpvltbvU/V8MPXQ-WFhI/AAAAAAAAK-c/uAEczst5iEs/image_thumb%25255B3%25255D.png?imgmax=800\" width=\"712\" height=\"893\"></a></p> <p> </p> <p>In terms of a Store Card, let us look at the apparent Sequence Diagram</p> <p> </p> <p><a href=\"https://lh3.googleusercontent.com/-PsLU4FR0x0c/V8MPXzrAOKI/AAAAAAAAK-g/aZwtAV5XTdk/s1600-h/image%25255B10%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-rYz3I5RSaw8/V8MPYuAKOZI/AAAAAAAAK-k/zAuk2OZKd3s/image_thumb%25255B6%25255D.png?imgmax=800\" width=\"1126\" height=\"952\"></a></p> <p> </p> <h1>Log Errors Messages Seen and Likely Meaning</h1> <ul>\n<li>Passbook Inactive or Deleted or some one changed Auth Token</li> <ul>\n<li>[2016-08-28 11:57:01 -0400] <strong>Unregister task </strong>(for device ceed8761e584e814ed4fe73cbb334ee9, pass type pass.com.reddwarfdogs.card.dev, serial number 85607BFE98D91A-765F7B05-D5E4-4B32-B16D-69C2038EF522; with web service url <a href=\"https://llc.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Authentication failure</strong>\n</li> <li>[2016-08-28 20:44:25 +0700] <strong>Register task</strong> (for device 19121d6b570b31a3fa56dbd45411c933, pass type pass.com.reddwarfdogs.card.dev, serial number 85607BFE98D91A-765F7B05-D5E4-4B32-B16D-69C2038EF522; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: Authentication <strong>failure</strong>\n</li> <li>[2016-08-24 10:04:38 +0800] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Update requested for <strong>unregistered serial number </strong>8C6772F099D51AA3-7A32F5FB-F7F8-4285-A2A2-79FC66DF942C</li> </ul>\n<li>Bad Record Keeping in your application</li> <ul>\n<li>[2016-08-23 19:58:35 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Server <strong>ignored the 'if-modified-since' header </strong>(Tue, 23 Aug 2016 16:54:10 GMT) and <strong>returned the full unchanged pass data for serial</strong> number '8C6771F89ED51DAA-AAF3100E-C365-4CCD-8C95-ADC974F52894'.</li> <li>[2016-08-23 16:49:38 -0700] Get pass task (pass type pass.com.reddwarfdogs.card.dev, serial number 8C6771F89ED31FAE-57ED753A-8464-408E-95EF-CEF75DBB30D6, if-modified-since Tue, 09 Aug 2016 21:57:32 GMT; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Received invalid pass data (The pass cannot be read because it isn’t valid.)</strong>\n</li> <ul>\n<li>\n<strong>Cause: </strong>Corruption OR change of Certificate used to sign Passbook</li> </ul>\n<li>[2016-08-23 13:56:44 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Server requested update to serial number '8C6771F89ED41BAC-FFBF3B69-98F1-4F2A-A8B7-5AF457558EE7', <strong>but the pass was unchanged</strong>.</li> <li>[2016-08-23 11:58:25 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Device <strong>received spurious push.</strong> Request for passesUpdatedSince '20160823180851' <strong>returned no serial numbers</strong>. (Device = 2c04d18e5f8480f97bb9318b4065dba0)</li> <li>[2016-08-08 10:23:57 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/v1/passbook):\">https://llc.reddwarfdogs.com/v1/passbook):</a> Response to 'What changed?' request included 1 serial numbers <strong>but the lastUpdated tag (20160808172351) remained the same</strong>.</li> <ul>\n<li>\n<strong>Cause</strong>: Duplicate push notification sent to a device or logic error. If the tag is   1234, then the server logic should be &gt; 1234 and NOT &gt;=1234</li> </ul>\n</ul>\n<li>Apple gives little guidance to status code and how the iphone will react</li> <ul>\n<li>[2016-08-23 15:46:33 +0700] Get serial #s task (for device 6f175696d73dec465c561f4d3ee2dfe7, pass type pass.com.reddwarfdogs.card.dev, last updated (null); with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 504</strong>\n</li> <li>[2016-08-23 01:42:53 -0700] Get serial #s task (for device 2c04d18e5f8480f97bb9318b4065dba0, pass type pass.com.reddwarfdogs.card.dev, last updated 20160823083910; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 408</strong>\n</li> <li>[2016-08-08 18:53:00 +0800] Get serial #s task (for device 726996d0f44f44b19f157aa0824f64cf, pass type pass.com.reddwarfdogs.card.dev, last updated (null); with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 596</strong>\n</li> </ul>\n</ul>\n<p><strong>I suspect there are more messages – I have just not stumbled across them yet.</strong></p>\n",
            "content": "\n<p>While working on a recent project, a major stumbling block was a lack of clear documentation of what happened where. This was confirmed when I attempted to search for some of the messages returned to the Log REST points by iPhone.. There were zero hits!</p> <p> </p> <p><a href=\"https://lh3.googleusercontent.com/-chO_aqROjEM/V8MPW2LRqSI/AAAAAAAAK-Y/jquJYxCG1v0/s1600-h/image%25255B5%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-8wPvpvltbvU/V8MPXQ-WFhI/AAAAAAAAK-c/uAEczst5iEs/image_thumb%25255B3%25255D.png?imgmax=800\" width=\"712\" height=\"893\"></a></p> <p> </p> <p>In terms of a Store Card, let us look at the apparent Sequence Diagram</p> <p> </p> <p><a href=\"https://lh3.googleusercontent.com/-PsLU4FR0x0c/V8MPXzrAOKI/AAAAAAAAK-g/aZwtAV5XTdk/s1600-h/image%25255B10%25255D.png\"><img title=\"image\" border=\"0\" alt=\"image\" src=\"https://lh3.googleusercontent.com/-rYz3I5RSaw8/V8MPYuAKOZI/AAAAAAAAK-k/zAuk2OZKd3s/image_thumb%25255B6%25255D.png?imgmax=800\" width=\"1126\" height=\"952\"></a></p> <p> </p> <h1>Log Errors Messages Seen and Likely Meaning</h1> <ul>\n<li>Passbook Inactive or Deleted or some one changed Auth Token</li> <ul>\n<li>[2016-08-28 11:57:01 -0400] <strong>Unregister task </strong>(for device ceed8761e584e814ed4fe73cbb334ee9, pass type pass.com.reddwarfdogs.card.dev, serial number 85607BFE98D91A-765F7B05-D5E4-4B32-B16D-69C2038EF522; with web service url <a href=\"https://llc.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Authentication failure</strong>\n</li> <li>[2016-08-28 20:44:25 +0700] <strong>Register task</strong> (for device 19121d6b570b31a3fa56dbd45411c933, pass type pass.com.reddwarfdogs.card.dev, serial number 85607BFE98D91A-765F7B05-D5E4-4B32-B16D-69C2038EF522; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: Authentication <strong>failure</strong>\n</li> <li>[2016-08-24 10:04:38 +0800] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Update requested for <strong>unregistered serial number </strong>8C6772F099D51AA3-7A32F5FB-F7F8-4285-A2A2-79FC66DF942C</li> </ul>\n<li>Bad Record Keeping in your application</li> <ul>\n<li>[2016-08-23 19:58:35 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Server <strong>ignored the 'if-modified-since' header </strong>(Tue, 23 Aug 2016 16:54:10 GMT) and <strong>returned the full unchanged pass data for serial</strong> number '8C6771F89ED51DAA-AAF3100E-C365-4CCD-8C95-ADC974F52894'.</li> <li>[2016-08-23 16:49:38 -0700] Get pass task (pass type pass.com.reddwarfdogs.card.dev, serial number 8C6771F89ED31FAE-57ED753A-8464-408E-95EF-CEF75DBB30D6, if-modified-since Tue, 09 Aug 2016 21:57:32 GMT; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Received invalid pass data (The pass cannot be read because it isn’t valid.)</strong>\n</li> <ul>\n<li>\n<strong>Cause: </strong>Corruption OR change of Certificate used to sign Passbook</li> </ul>\n<li>[2016-08-23 13:56:44 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Server requested update to serial number '8C6771F89ED41BAC-FFBF3B69-98F1-4F2A-A8B7-5AF457558EE7', <strong>but the pass was unchanged</strong>.</li> <li>[2016-08-23 11:58:25 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/passbook):\">https://llc.reddwarfdogs.com/passbook):</a> Device <strong>received spurious push.</strong> Request for passesUpdatedSince '20160823180851' <strong>returned no serial numbers</strong>. (Device = 2c04d18e5f8480f97bb9318b4065dba0)</li> <li>[2016-08-08 10:23:57 -0700] Web service error for pass.com.reddwarfdogs.card.dev (<a href=\"https://test.openapi.reddwarfdogs.com/v1/passbook):\">https://llc.reddwarfdogs.com/v1/passbook):</a> Response to 'What changed?' request included 1 serial numbers <strong>but the lastUpdated tag (20160808172351) remained the same</strong>.</li> <ul>\n<li>\n<strong>Cause</strong>: Duplicate push notification sent to a device or logic error. If the tag is   1234, then the server logic should be &gt; 1234 and NOT &gt;=1234</li> </ul>\n</ul>\n<li>Apple gives little guidance to status code and how the iphone will react</li> <ul>\n<li>[2016-08-23 15:46:33 +0700] Get serial #s task (for device 6f175696d73dec465c561f4d3ee2dfe7, pass type pass.com.reddwarfdogs.card.dev, last updated (null); with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 504</strong>\n</li> <li>[2016-08-23 01:42:53 -0700] Get serial #s task (for device 2c04d18e5f8480f97bb9318b4065dba0, pass type pass.com.reddwarfdogs.card.dev, last updated 20160823083910; with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 408</strong>\n</li> <li>[2016-08-08 18:53:00 +0800] Get serial #s task (for device 726996d0f44f44b19f157aa0824f64cf, pass type pass.com.reddwarfdogs.card.dev, last updated (null); with web service url <a href=\"https://test.openapi.reddwarfdogs.com/passbook)\">https://llc.reddwarfdogs.com/passbook)</a> encountered error: <strong>Unexpected response code 596</strong>\n</li> </ul>\n</ul>\n<p><strong>I suspect there are more messages – I have just not stumbled across them yet.</strong></p>\n",
            "enclosure": {
                "thumbnail": "https://lh3.googleusercontent.com/-8wPvpvltbvU/V8MPXQ-WFhI/AAAAAAAAK-c/uAEczst5iEs/s72-c/image_thumb%25255B3%25255D.png?imgmax=800"
            },
            "categories": []
        },
        {
            "title": "Solving PushSharp.Apple Disconnect Issue",
            "pubDate": "2016-08-26 23:25:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/08/solving-pushsharpapple-disconnect-issue.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-489355758635191907",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>While doing a load test of a new Apple Passbook application, I suddenly saw some 200K transmissions errors from my WebApi application. Searching the web I found that a “high” rate of connect/disconnect to Apple Push Notification Service being reported as causing APNS to do a forced disconnect.</p> <p> </p> <p>While Apple does have a limit (very very high) on the number of notifications before they will refuse connections for an hour, the limit for connect/disconnect is much lower. After some playing around a bit I found that if I persisted the connection via a static, I no longer have this issue.</p> <p> </p> <p>Below is a sample of the code.</p> <ul>\n<li>\n<strong>Note</strong>: we disconnect and reconnect whenever an error happens (I have not seen an error yet)  </li> </ul>\n<p> </p> <p>using Newtonsoft.Json.Linq;</p> <p>using PushSharp.Apple;</p> <p>using System;</p> <p>using System.Collections.Generic;</p> <p>using System.Security.Cryptography.X509Certificates;</p> <p>using System.Text;</p> <p>namespace RedDwarfDogs.Passbook.Engine.Notification</p> <p>{</p> <p>    public class AppleNotification : INotification</p> <p>    {</p> <p>        private readonly IPassbookSettings _passbookSettings;</p> <p>        private readonly ILogger_logger;</p> <p>        private static ApnsServiceBroker _apnsServiceBroker;</p> <p>        private static object lockObject = new object();</p> <p>        public AppleNotification(ILogger logger,IPassbookSettings passbookSettings)</p> <p>        {</p> <p>            _logger= Guard.EnsureArgumentIsNotNull(logger, \"logger\");</p> <p>            _passbookSettings = Guard.EnsureArgumentIsNotNull(passbookSettings, \"passbookSettings\");</p> <p>        }</p> <p>        public void SendNotification(HashSet&lt;string&gt; deviceTokens)</p> <p>        {</p> <p>            if (deviceTokens == null || deviceTokens.Count == 0)</p> <p>            {</p> <p>                return;</p> <p>            }</p> <p>            try</p> <p>            {</p> <p>                _logger.Write(\"PassbookEngine_SendNotification_Apple\");</p> <p>                // Create a new broker if needed</p> <p>                if (_apnsServiceBroker == null)</p> <p>                {</p> <p>                    X509Certificate2 cert = _passbookSettings.ApplePushCertificate;</p> <p>                    if (cert == null)</p> <p>                        throw new InvalidOperationException(\"pushThumbprint certificate is not installed or has invalid Thumbprint\");</p> <p>                      var config = new ApnsConfiguration(ApnsConfiguration.ApnsServerEnvironment.Production,</p> <p>                        _passbookSettings.ApplePushCertificate, false);</p> <p>                    _logger.Write(\"PassbookEngine_SendNotification_Apple_Connect\");</p> <p>                    _apnsServiceBroker = new ApnsServiceBroker(config);</p> <p>                    // Wire up events</p> <p>                    _apnsServiceBroker.OnNotificationFailed += (notification, aggregateEx) =&gt;</p> <p>                    {</p> <p>                        aggregateEx.Handle(ex =&gt;</p> <p>                        {</p> <p>                            _logger.Write(\"Apple Notification Failed\", \"Direct\", ex);</p> <p>                            _logger.Write(\"PassbookEngine_SendNotification_Apple_Error\");</p> <p>                            // See what kind of exception it was to further diagnose</p> <p>                            if (ex is ApnsNotificationException)</p> <p>                            {</p> <p>                                var notificationException = (ApnsNotificationException)ex;</p> <p>                                var apnsNotification = notificationException.Notification;</p> <p>                                var statusCode = notificationException.ErrorStatusCode;</p> <p>                            }</p> <p>                            _logger.Write(\"SendNotification\", \"PushToken Rejected\", ex);</p> <p>                            // We reset to null to recreate / connect</p> <p>                            Restart();</p> <p>                            return true;</p> <p>                        });</p> <p>                    };</p> <p>                    _apnsServiceBroker.OnNotificationSucceeded += (notification) =&gt;</p> <p>                    {</p> <p>                    };</p> <p>                    // Start the broker</p> <p>                }</p> <p>                var sentTokens = new StringBuilder();</p> <p>                lock (lockObject)</p> <p>                {</p> <p>                    _apnsServiceBroker.Start();</p> <p>                    foreach (var deviceToken in deviceTokens)</p> <p>                    {</p> <p>                        if (string.IsNullOrWhiteSpace(deviceToken) || deviceToken.Length &lt; 32 || deviceToken.Length &gt; 256 || deviceToken.Contains(\"-\"))</p> <p>                        {</p> <p>                            //Invalid Token, keep in Apple's good books                    </p> <p>                            // We use GUID's thus - for faking pushtokens. Do not send them to apple</p> <p>                            // We do not want to be get black listed</p> <p>                        }</p> <p>                        else</p> <p>                        {</p> <p>                            // Queue a notification to send</p> <p>                            var nofification = new ApnsNotification</p> <p>                            {</p> <p>                                DeviceToken = deviceToken,</p> <p>                                Payload = JObject.Parse(\"{\\\"aps\\\":{\\\"badge\\\":7}}\")</p> <p>                            };</p> <p>                            try</p> <p>                            {</p> <p>                                _apnsServiceBroker.QueueNotification(nofification);</p> <p>                                sentTokens.AppendFormat(\"{0} \", deviceToken);</p> <p>                            }</p> <p>                            catch (System.InvalidOperationException)</p> <p>                            {</p> <p>                                // Assuming already in queue</p> <p>                            }</p> <p>                        }</p> <p>                    }</p> <p>                    try</p> <p>                    {</p> <p>                        //duplicate signals may occur</p> <p>                        _apnsServiceBroker.Stop();</p> <p>                    }</p> <p>                    catch { }</p> <p>                }</p> <p>                var auditLog = new Log</p> <p>                {</p> <p>                    Message = sentTokens.ToString(),</p> <p>                    RequestHttpMethod = \"Post\"</p> <p>                };</p> <p>                _logger.Write(\"Passbook\", PassbookLogMessageCategory.SendNotification.ToString(),</p> <p>                    \"PassbookAudit\", \"Passbook\", auditLog);</p> <p>                return;</p> <p>            }</p> <p>            catch (Exception exc)</p> <p>            {</p> <p>                // We swallow notification exceptions - for example APSN is off line. Allow rest of processing to work.</p> <p>                _logger.Write(\"SendNotification\", \"One or more notifications via Apple (APNS) failed\", exc);</p> <p>                Restart();</p> <p>                _apnsServiceBroker = null; //force a reset</p> <p>            }</p> <p>        }</p> <p>        private void Restart()</p> <p>        {</p> <p>            if (_apnsServiceBroker != null)</p> <p>            {</p> <p>                try</p> <p>                {</p> <p>                    //duplicate signals may occur</p> <p>                    _apnsServiceBroker.Stop();</p> <p>                }</p> <p>                catch { }</p> <p>                _logCounterWrapper.Increment(\"PassbookEngine_SendNotification_Apple_Restart\");</p> <p>                _apnsServiceBroker = null;</p> <p>            }</p> <p>        }</p> <p>    }</p> <p>}</p>\n",
            "content": "\n<p>While doing a load test of a new Apple Passbook application, I suddenly saw some 200K transmissions errors from my WebApi application. Searching the web I found that a “high” rate of connect/disconnect to Apple Push Notification Service being reported as causing APNS to do a forced disconnect.</p> <p> </p> <p>While Apple does have a limit (very very high) on the number of notifications before they will refuse connections for an hour, the limit for connect/disconnect is much lower. After some playing around a bit I found that if I persisted the connection via a static, I no longer have this issue.</p> <p> </p> <p>Below is a sample of the code.</p> <ul>\n<li>\n<strong>Note</strong>: we disconnect and reconnect whenever an error happens (I have not seen an error yet)  </li> </ul>\n<p> </p> <p>using Newtonsoft.Json.Linq;</p> <p>using PushSharp.Apple;</p> <p>using System;</p> <p>using System.Collections.Generic;</p> <p>using System.Security.Cryptography.X509Certificates;</p> <p>using System.Text;</p> <p>namespace RedDwarfDogs.Passbook.Engine.Notification</p> <p>{</p> <p>    public class AppleNotification : INotification</p> <p>    {</p> <p>        private readonly IPassbookSettings _passbookSettings;</p> <p>        private readonly ILogger_logger;</p> <p>        private static ApnsServiceBroker _apnsServiceBroker;</p> <p>        private static object lockObject = new object();</p> <p>        public AppleNotification(ILogger logger,IPassbookSettings passbookSettings)</p> <p>        {</p> <p>            _logger= Guard.EnsureArgumentIsNotNull(logger, \"logger\");</p> <p>            _passbookSettings = Guard.EnsureArgumentIsNotNull(passbookSettings, \"passbookSettings\");</p> <p>        }</p> <p>        public void SendNotification(HashSet&lt;string&gt; deviceTokens)</p> <p>        {</p> <p>            if (deviceTokens == null || deviceTokens.Count == 0)</p> <p>            {</p> <p>                return;</p> <p>            }</p> <p>            try</p> <p>            {</p> <p>                _logger.Write(\"PassbookEngine_SendNotification_Apple\");</p> <p>                // Create a new broker if needed</p> <p>                if (_apnsServiceBroker == null)</p> <p>                {</p> <p>                    X509Certificate2 cert = _passbookSettings.ApplePushCertificate;</p> <p>                    if (cert == null)</p> <p>                        throw new InvalidOperationException(\"pushThumbprint certificate is not installed or has invalid Thumbprint\");</p> <p>                      var config = new ApnsConfiguration(ApnsConfiguration.ApnsServerEnvironment.Production,</p> <p>                        _passbookSettings.ApplePushCertificate, false);</p> <p>                    _logger.Write(\"PassbookEngine_SendNotification_Apple_Connect\");</p> <p>                    _apnsServiceBroker = new ApnsServiceBroker(config);</p> <p>                    // Wire up events</p> <p>                    _apnsServiceBroker.OnNotificationFailed += (notification, aggregateEx) =&gt;</p> <p>                    {</p> <p>                        aggregateEx.Handle(ex =&gt;</p> <p>                        {</p> <p>                            _logger.Write(\"Apple Notification Failed\", \"Direct\", ex);</p> <p>                            _logger.Write(\"PassbookEngine_SendNotification_Apple_Error\");</p> <p>                            // See what kind of exception it was to further diagnose</p> <p>                            if (ex is ApnsNotificationException)</p> <p>                            {</p> <p>                                var notificationException = (ApnsNotificationException)ex;</p> <p>                                var apnsNotification = notificationException.Notification;</p> <p>                                var statusCode = notificationException.ErrorStatusCode;</p> <p>                            }</p> <p>                            _logger.Write(\"SendNotification\", \"PushToken Rejected\", ex);</p> <p>                            // We reset to null to recreate / connect</p> <p>                            Restart();</p> <p>                            return true;</p> <p>                        });</p> <p>                    };</p> <p>                    _apnsServiceBroker.OnNotificationSucceeded += (notification) =&gt;</p> <p>                    {</p> <p>                    };</p> <p>                    // Start the broker</p> <p>                }</p> <p>                var sentTokens = new StringBuilder();</p> <p>                lock (lockObject)</p> <p>                {</p> <p>                    _apnsServiceBroker.Start();</p> <p>                    foreach (var deviceToken in deviceTokens)</p> <p>                    {</p> <p>                        if (string.IsNullOrWhiteSpace(deviceToken) || deviceToken.Length &lt; 32 || deviceToken.Length &gt; 256 || deviceToken.Contains(\"-\"))</p> <p>                        {</p> <p>                            //Invalid Token, keep in Apple's good books                    </p> <p>                            // We use GUID's thus - for faking pushtokens. Do not send them to apple</p> <p>                            // We do not want to be get black listed</p> <p>                        }</p> <p>                        else</p> <p>                        {</p> <p>                            // Queue a notification to send</p> <p>                            var nofification = new ApnsNotification</p> <p>                            {</p> <p>                                DeviceToken = deviceToken,</p> <p>                                Payload = JObject.Parse(\"{\\\"aps\\\":{\\\"badge\\\":7}}\")</p> <p>                            };</p> <p>                            try</p> <p>                            {</p> <p>                                _apnsServiceBroker.QueueNotification(nofification);</p> <p>                                sentTokens.AppendFormat(\"{0} \", deviceToken);</p> <p>                            }</p> <p>                            catch (System.InvalidOperationException)</p> <p>                            {</p> <p>                                // Assuming already in queue</p> <p>                            }</p> <p>                        }</p> <p>                    }</p> <p>                    try</p> <p>                    {</p> <p>                        //duplicate signals may occur</p> <p>                        _apnsServiceBroker.Stop();</p> <p>                    }</p> <p>                    catch { }</p> <p>                }</p> <p>                var auditLog = new Log</p> <p>                {</p> <p>                    Message = sentTokens.ToString(),</p> <p>                    RequestHttpMethod = \"Post\"</p> <p>                };</p> <p>                _logger.Write(\"Passbook\", PassbookLogMessageCategory.SendNotification.ToString(),</p> <p>                    \"PassbookAudit\", \"Passbook\", auditLog);</p> <p>                return;</p> <p>            }</p> <p>            catch (Exception exc)</p> <p>            {</p> <p>                // We swallow notification exceptions - for example APSN is off line. Allow rest of processing to work.</p> <p>                _logger.Write(\"SendNotification\", \"One or more notifications via Apple (APNS) failed\", exc);</p> <p>                Restart();</p> <p>                _apnsServiceBroker = null; //force a reset</p> <p>            }</p> <p>        }</p> <p>        private void Restart()</p> <p>        {</p> <p>            if (_apnsServiceBroker != null)</p> <p>            {</p> <p>                try</p> <p>                {</p> <p>                    //duplicate signals may occur</p> <p>                    _apnsServiceBroker.Stop();</p> <p>                }</p> <p>                catch { }</p> <p>                _logCounterWrapper.Increment(\"PassbookEngine_SendNotification_Apple_Restart\");</p> <p>                _apnsServiceBroker = null;</p> <p>            }</p> <p>        }</p> <p>    }</p> <p>}</p>\n",
            "enclosure": [],
            "categories": [
                "ASP.NET",
                ".NET",
                "c#",
                "APNS",
                "Apple Passbook"
            ]
        },
        {
            "title": "Taking Apple PkPasses In-House–Working Notes",
            "pubDate": "2016-08-07 22:48:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/08/taking-apple-pkpasses-in-houseworking.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-3166737776178733027",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>This year I had a explicit, yet vague, project assigned to me: Move our Apple PkPass from a third party provider to our own internal system. The working environment was the Microsoft Stack with C# and a little googling found that the first 90% of the work could be done by nuget, namely:</p> <ul>\n<li><code>Install-Package dotnet-passbook </code></li> <li>Install-Package PushSharp</li> </ul>\n<p>Created a certificate file on the apple developer site and we are done … easy project… not quite</p> <p> </p> <p>Unfortunately both in-house expertise and 3rd part expertise involved in the original project had moved on. Welcome to reverse engineering black boxes.</p> <p> </p> <h1>The Joy of Certificates!</h1> <p>Going to <a href=\"http://www.apple.com/certificateauthority/\">http://www.apple.com/certificateauthority/</a>  open a can of worms. The existing instructions assumed you have a Mac not Windows 10.</p> <p>The existing instructions found on the web(<a href=\"https://tomasmcguinness.com/2012/06/28/generating-an-apple-ios-certificate-using-windows/\">https://tomasmcguinness.com/2012/06/28/generating-an-apple-ios-certificate-using-windows/</a>)  broke due to some change with Windows or Apple in April 2016 ( <a href=\"https://forums.developer.apple.com/thread/46122\">apple forum</a>, <a href=\"http://stackoverflow.com/questions/36472960/apple-push-notifications-certificate-on-windows-server\">stack overflow</a>). The solution was Unix on windows via <a href=\"https://cygwin.com/install.html\">https://cygwin.com/install.html</a> and going the unix route to generate pfx files.</p> <p> </p> <p>The second issue was connected with how we run our IIS servers and the default instructions for installing certificate for dotnet-passbook were not mutually compatible. The instructions said that the certs needed to be install in the <em>Intermediate Certification Authorities </em>– after a few panic hours deploying to load hosts with problems, we discovered that we had to Import to <em>Personal</em> to get <strong>dotnet-passbook</strong> to work.</p> <p>The next issue we encountered was that of invisible characters coming along when we copy the thumbprint to our C# code. We implemented a thumbprint check that verified both the length (40) and also walk the characters insuring that all were in range. After this, we verified that we could find the matching certificate. All of this was done on website load. . an error was thrown, the site would not load.</p> <p> </p> <p>This saved us triage time on every new deployment:with an </p> <ul>\n<li>We identify if a thumbprint is ‘corrupt’</li> <li>We verified that the expected certificate is there</li> </ul>\n<p>The last issue impacts big shops: The certificate should be 100% owned by Dev Ops and never installed on a dev or test machine. This means that alternative certs are needed in those environment. Each cert with have a different thumbprint – hence lots of web.config transformation substituting in the correct thumbprint for the environment. The real life production cert should be owned by dev ops (or security)  with a very strong password that they and they alone know.</p> <p> </p> <h1>The Joys of Authentication Tokens</h1> <p>Security review for in-house required that the authentication tokens be a one way hash (SHA384 or higher) and be unique per PkPasses. The existing design used Guids for serial numbers and thus we used a Guid for the authentication token when the pass was first created.  We can never recreate an existing PkPass because we do not know the authentication token, just the hash.  When a request comes in for the latest path, we hash the authentication token sent in the authentication header and compare it to the hash. We then persist it in memory and insert it into the PkPass Json,  then we Zip and Sign the new PkPass.  Security is happy.</p> <p> </p> <p>Now when it comes to the 3rd party provider, we were fortunate that they stored the authentication tokens in plain text, so it was just a hash and save the hash into our database. If they had hashed (as they should have), then we would need to replicate their hash method. If it was a SHA1 and SHA-2 was required by our security, then we would need to do some fancy footwork to migrate the hash, i.e. </p> <ol>\n<li>add a “SHA” column iWn our table, </li> <li>when a new request comes in examine the SHA value</li> <li>if it is “1” then use the authentication token presented and authenticated to create a SHA-2 hash and update the SHA column to “2”</li> <li>if it is “2” then authenticate appropriately.</li> </ol>\n<p>This will allow us to track the uplift rate to SHA-2. At some point security would likely say “delete the SHA1 PkPass records”. This is easy because we have tracked them.</p> <p> </p> <h1>Push Notifications</h1> <p>This went easy except for missing that a Push Certificate is NOT used for PKPass files. Yes, it is not used.  It is used for registered 3rd party developed Apple applications. The certificate used for connecting to the Apple Push Notification Service (APNS) is the certificate used to sign the PkPass files. There is no separate push notification certificate. Also, using PushSharp, you must set “validate certificate” to false, or an exception will be thrown.</p> <p> </p> <p>The pushTokens are device identifiers and APNS does not provide feedback if the device still exists (one of my old phones exists, but is at the bottom of an outdoor privy in a national park…), is turned off, or is out of communication.  The author of PushSharp, Redth, has done an excellent description of the problem <a href=\"http://redth.codes/the-problem-with-apples-push-notification-ser/\">here</a>. The logical way to keep the history in check is to track when each pass is last retrieved and then periodically delete the push notifications for devices where <em><strong>none of the associated passes</strong></em> have been retrieved in the last year.  You will have “dead” push tokens in some circumstances. </p> <p> </p> <p>I have a pkPass, my iPhone got destroyed. I installed the pkPass on the new phone. The old iPhone push token will never be eliminated while I maintain my PkPass. Why? because we do not know which iPhone is getting updates! </p> <p> </p> <h1>Minor hiccup</h1> <p>The get serial number since API call had a gotcha dealing with modified since <em>query parameters</em>. Apple documentation suggest that a date be used and we originally code it up assuming that this was a http if-modified-since header. QAing on a iPhone clarified that it was a query parameter and not a http header. We simply moved the same date there and encountered two issues:</p> <ul>\n<li>We had a time-offset issue, our code was working off the database local time and our code deeming it to be universal time…. (which a http header would be)</li> <li>Our IIS security settings did not like seeing a “:” in a query parameter. We resolved by used “yyyyMMddHHmmss” format</li> </ul>\n<p>The real gotcha that was stated in the apple documentation was that this is an <em>arbitrary token</em>  that is daisy chained from one call to the next. It did not need to be a date. A date is a logical choice, but it is not required to be a date. </p> <p> </p> <p>The value received in the last get serial numbers response is what is sent in the next get serial numbers request. Daisy chaining. The iPhone does nothing but echo it back.</p> <h1>Avoiding a Migraine</h1> <p>The dotnet-passbook code puts into the Json, the pass type identifier name in the certificate <em>regardless</em> of what you passed in. This is good and wise and secure. It has an unfortunate side effect, the routing </p> <p>devices/{deviceLibraryIdentifier}/registrations/{passTypeIdentifier} and passes/{passTypeIdentifier}/{serialNumber}</p> <p> is determined by this pass type identifier. If you are running a site and passes come from passes/foobar/1234, but your certificate name is “JackShyte” then the Json in the pass returned would read JackShyte. When the iPhone gets a push token, it would then construct the url for the update as passes/JackShyte/1234 … which will likely return a 404. The PkPass will never be updated unless you create additional routings!! </p> <p> </p> <p>The solution that I took was to compare the {passTypeIdentifier} in the routing to the certificate. If they did not match, then 404 immediately and log an exception. While it is technically possible to “unwind” such a foul up, the path is not pretty.</p> <p> </p> <h1>Migration</h1> <p>The key for migration is a stepped approach</p> <ol>\n<li>Deploy your new solution and test it, correct any issues that you find in the production environment</li> <li>Deploy the application or mechanism for creating new PkPasses (this could be part of 1), so all new passes use the in-house system</li> <li>Update your data from the third party provider with authentication tokens (or their hash) and serial numbers. You want to do this after 2, because you want this list to be closed (no new passes created on the third party system)</li> <li>Have the 3rd party provider change the WebServiceUrl to the in-house solution. In theory, a Moved response to the in house system would also work (I have not tested this with an iPhone).</li> <li>Since the 3rd party wants to shut down in time, then you must send out a push notification to every push token you have.  You will likely want to throttle this if you have a large numbers of push tokens (in my case, 30 million) because every push token could result in a request for a new PkPass file.</li> <ol>\n<li>This may need to be repeated to insure adequate coverage for devices off line or abroad without data plans</li> </ol>\n</ol>\n<h1>Bottom Line</h1> <p>The original design worked, but<em> there was a ton of details that had to be sorted out. </em>I have omitted the nightmares that QA had trying to validate stuff, especially the migration portions. </p>\n",
            "content": "\n<p>This year I had a explicit, yet vague, project assigned to me: Move our Apple PkPass from a third party provider to our own internal system. The working environment was the Microsoft Stack with C# and a little googling found that the first 90% of the work could be done by nuget, namely:</p> <ul>\n<li><code>Install-Package dotnet-passbook </code></li> <li>Install-Package PushSharp</li> </ul>\n<p>Created a certificate file on the apple developer site and we are done … easy project… not quite</p> <p> </p> <p>Unfortunately both in-house expertise and 3rd part expertise involved in the original project had moved on. Welcome to reverse engineering black boxes.</p> <p> </p> <h1>The Joy of Certificates!</h1> <p>Going to <a href=\"http://www.apple.com/certificateauthority/\">http://www.apple.com/certificateauthority/</a>  open a can of worms. The existing instructions assumed you have a Mac not Windows 10.</p> <p>The existing instructions found on the web(<a href=\"https://tomasmcguinness.com/2012/06/28/generating-an-apple-ios-certificate-using-windows/\">https://tomasmcguinness.com/2012/06/28/generating-an-apple-ios-certificate-using-windows/</a>)  broke due to some change with Windows or Apple in April 2016 ( <a href=\"https://forums.developer.apple.com/thread/46122\">apple forum</a>, <a href=\"http://stackoverflow.com/questions/36472960/apple-push-notifications-certificate-on-windows-server\">stack overflow</a>). The solution was Unix on windows via <a href=\"https://cygwin.com/install.html\">https://cygwin.com/install.html</a> and going the unix route to generate pfx files.</p> <p> </p> <p>The second issue was connected with how we run our IIS servers and the default instructions for installing certificate for dotnet-passbook were not mutually compatible. The instructions said that the certs needed to be install in the <em>Intermediate Certification Authorities </em>– after a few panic hours deploying to load hosts with problems, we discovered that we had to Import to <em>Personal</em> to get <strong>dotnet-passbook</strong> to work.</p> <p>The next issue we encountered was that of invisible characters coming along when we copy the thumbprint to our C# code. We implemented a thumbprint check that verified both the length (40) and also walk the characters insuring that all were in range. After this, we verified that we could find the matching certificate. All of this was done on website load. . an error was thrown, the site would not load.</p> <p> </p> <p>This saved us triage time on every new deployment:with an </p> <ul>\n<li>We identify if a thumbprint is ‘corrupt’</li> <li>We verified that the expected certificate is there</li> </ul>\n<p>The last issue impacts big shops: The certificate should be 100% owned by Dev Ops and never installed on a dev or test machine. This means that alternative certs are needed in those environment. Each cert with have a different thumbprint – hence lots of web.config transformation substituting in the correct thumbprint for the environment. The real life production cert should be owned by dev ops (or security)  with a very strong password that they and they alone know.</p> <p> </p> <h1>The Joys of Authentication Tokens</h1> <p>Security review for in-house required that the authentication tokens be a one way hash (SHA384 or higher) and be unique per PkPasses. The existing design used Guids for serial numbers and thus we used a Guid for the authentication token when the pass was first created.  We can never recreate an existing PkPass because we do not know the authentication token, just the hash.  When a request comes in for the latest path, we hash the authentication token sent in the authentication header and compare it to the hash. We then persist it in memory and insert it into the PkPass Json,  then we Zip and Sign the new PkPass.  Security is happy.</p> <p> </p> <p>Now when it comes to the 3rd party provider, we were fortunate that they stored the authentication tokens in plain text, so it was just a hash and save the hash into our database. If they had hashed (as they should have), then we would need to replicate their hash method. If it was a SHA1 and SHA-2 was required by our security, then we would need to do some fancy footwork to migrate the hash, i.e. </p> <ol>\n<li>add a “SHA” column iWn our table, </li> <li>when a new request comes in examine the SHA value</li> <li>if it is “1” then use the authentication token presented and authenticated to create a SHA-2 hash and update the SHA column to “2”</li> <li>if it is “2” then authenticate appropriately.</li> </ol>\n<p>This will allow us to track the uplift rate to SHA-2. At some point security would likely say “delete the SHA1 PkPass records”. This is easy because we have tracked them.</p> <p> </p> <h1>Push Notifications</h1> <p>This went easy except for missing that a Push Certificate is NOT used for PKPass files. Yes, it is not used.  It is used for registered 3rd party developed Apple applications. The certificate used for connecting to the Apple Push Notification Service (APNS) is the certificate used to sign the PkPass files. There is no separate push notification certificate. Also, using PushSharp, you must set “validate certificate” to false, or an exception will be thrown.</p> <p> </p> <p>The pushTokens are device identifiers and APNS does not provide feedback if the device still exists (one of my old phones exists, but is at the bottom of an outdoor privy in a national park…), is turned off, or is out of communication.  The author of PushSharp, Redth, has done an excellent description of the problem <a href=\"http://redth.codes/the-problem-with-apples-push-notification-ser/\">here</a>. The logical way to keep the history in check is to track when each pass is last retrieved and then periodically delete the push notifications for devices where <em><strong>none of the associated passes</strong></em> have been retrieved in the last year.  You will have “dead” push tokens in some circumstances. </p> <p> </p> <p>I have a pkPass, my iPhone got destroyed. I installed the pkPass on the new phone. The old iPhone push token will never be eliminated while I maintain my PkPass. Why? because we do not know which iPhone is getting updates! </p> <p> </p> <h1>Minor hiccup</h1> <p>The get serial number since API call had a gotcha dealing with modified since <em>query parameters</em>. Apple documentation suggest that a date be used and we originally code it up assuming that this was a http if-modified-since header. QAing on a iPhone clarified that it was a query parameter and not a http header. We simply moved the same date there and encountered two issues:</p> <ul>\n<li>We had a time-offset issue, our code was working off the database local time and our code deeming it to be universal time…. (which a http header would be)</li> <li>Our IIS security settings did not like seeing a “:” in a query parameter. We resolved by used “yyyyMMddHHmmss” format</li> </ul>\n<p>The real gotcha that was stated in the apple documentation was that this is an <em>arbitrary token</em>  that is daisy chained from one call to the next. It did not need to be a date. A date is a logical choice, but it is not required to be a date. </p> <p> </p> <p>The value received in the last get serial numbers response is what is sent in the next get serial numbers request. Daisy chaining. The iPhone does nothing but echo it back.</p> <h1>Avoiding a Migraine</h1> <p>The dotnet-passbook code puts into the Json, the pass type identifier name in the certificate <em>regardless</em> of what you passed in. This is good and wise and secure. It has an unfortunate side effect, the routing </p> <p>devices/{deviceLibraryIdentifier}/registrations/{passTypeIdentifier} and passes/{passTypeIdentifier}/{serialNumber}</p> <p> is determined by this pass type identifier. If you are running a site and passes come from passes/foobar/1234, but your certificate name is “JackShyte” then the Json in the pass returned would read JackShyte. When the iPhone gets a push token, it would then construct the url for the update as passes/JackShyte/1234 … which will likely return a 404. The PkPass will never be updated unless you create additional routings!! </p> <p> </p> <p>The solution that I took was to compare the {passTypeIdentifier} in the routing to the certificate. If they did not match, then 404 immediately and log an exception. While it is technically possible to “unwind” such a foul up, the path is not pretty.</p> <p> </p> <h1>Migration</h1> <p>The key for migration is a stepped approach</p> <ol>\n<li>Deploy your new solution and test it, correct any issues that you find in the production environment</li> <li>Deploy the application or mechanism for creating new PkPasses (this could be part of 1), so all new passes use the in-house system</li> <li>Update your data from the third party provider with authentication tokens (or their hash) and serial numbers. You want to do this after 2, because you want this list to be closed (no new passes created on the third party system)</li> <li>Have the 3rd party provider change the WebServiceUrl to the in-house solution. In theory, a Moved response to the in house system would also work (I have not tested this with an iPhone).</li> <li>Since the 3rd party wants to shut down in time, then you must send out a push notification to every push token you have.  You will likely want to throttle this if you have a large numbers of push tokens (in my case, 30 million) because every push token could result in a request for a new PkPass file.</li> <ol>\n<li>This may need to be repeated to insure adequate coverage for devices off line or abroad without data plans</li> </ol>\n</ol>\n<h1>Bottom Line</h1> <p>The original design worked, but<em> there was a ton of details that had to be sorted out. </em>I have omitted the nightmares that QA had trying to validate stuff, especially the migration portions. </p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "One Migration Strategy to Microservices",
            "pubDate": "2016-06-06 11:44:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/06/one-migration-strategy-to-microservices.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-1070576772491598701",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>The concepts of microservices is nice, but if you have a complex existing system the path is neither obvious or easy. I have seen Principal Architects throw up their hands and persuade the business that <em><strong>we need to build a new replacement system</strong></em> and that the old system is impossible to work with. This path tends to lead into overruns and often complete failures – I have recently seen that happen at a firm: “Just one year needed to deliver…” and three years later it was killed off because it had not been delivered.  The typical reported in industry literature statistics of 80—90% failure are very believable.</p> <p> </p> <p>Over decades, I have seen many failrues (usually on the side lines).  On the other hand, for a planned phrase migration I have seen repeated success. Often success or failure seem to be determined by the agile-ness of the management and technical leads coupled with the depth of analysis before the project start. Unfortunately deep analysis ends up with a waterfall like specification that result in locked-step development and no agile-ness around issues. Similarly, agile often result in superficial analysis (the time horizon for analysis is often just the end of the next sprint)  with many components failing to fit together properly over time! </p> <p> </p> <p>This post is looking at a heritage system and seeing how it can be converted to a microservices framework in an evolutionary manner. No total rewrite, just a phrased migration ending with a system that is close to a classic pro-forma microservice system.</p> <p> </p> <p>I made several runs at this problem, and what I describe below “feels good” – which to me usually mean a high probability of success with demonstrable steps at regular intervals. </p> <p> </p> <h1>Example System</h1> <p>I am going to borrow a university system template from my days working for <a href=\"http://www.blackboard.com/\">Blackboard</a>.  We have teachers, non-teaching staff, students, classes, building, security access cards, payment cards, etc.  At one point, components were in Delphi, C#, Java, C++ etc with the databases in SQL Server and Oracle. Not only is data shared, but permissions often need to be consistent and appropriate.</p> <p> </p> <p>I have tried a few running starts of microservicing  such a design, and at present, my best suggestion is this:</p> <ul>\n<li>Do NOT extend the microservicing  down to the database – there is a more elegant way to proceed </li> <li>Look at the scope of the microservices API very carefully – this is a narrow path that can explode into infinite microservices or a resurrection of legacy patterns </li> </ul>\n<h2>Elegant Microservice Database Access</h2> <p>Do not touch the database design at the start. You are just compounding the migration path needlessly at the start. Instead, for each microservice create a new database login that is named for the microservice and has (typically) CRUD permissions to:</p> <ul>\n<li>A table </li> <li>A subset of columns in a table </li> <li>An updateable view </li> <li>A subset of columns in an updateable view </li> </ul>\n<p>We term this the Crud-Columns. There is a temptation to incorporate multiple Crud-columns into one microservice – the problem is simple, what is the objective criteria to stop incorporating more Crud-Columns into this single microservice? If you go to one microservice for each Crud-Columns, then by counting the tables you have an estimate of the number of microservices that you will likely end up with…  oh… that may be a lot! At this point of time, you may really want to consider automatic code generation of many microservices – similar to what you see with Entity-Frameworks, except this would be called Microservices-Framework. </p> <p> </p> <p>This microservice may also have Read only permissions to other tables.  This other tables read only access  may be <em>transitory</em> for the migration. Regardless of final resolution, these tables must be directly related to the CRUD columns, and used to determine CUD decisions. At some future time, these rest calls to these read only tables may be redirected elsewhere (for example using a Moved to directive to a reporting microservices).</p> <p> </p> <p>Oh, I have introduced a new term “reporting microservices”.  This is a microservice with one or read Read Api’s – multiple calls may be exposed depending on filtering, sorting or user permissions.</p> <p> </p> <p>Microservices are not domain level APIs but at sub-domains or even sub-sub-domains. You should not be making small steps, instead, put on <a href=\"https://en.wikipedia.org/wiki/Seven-league_boots\">your seven-league boots</a>! </p> <p><img alt=\"American Trucking Industry 1952 Ad - Seven League Boots…\" src=\"http://www.bambootrading.com/2100/2110.JPG\"></p> <p> </p> <h3>Tracking microservices</h3> <p>Consider creating a table where every database column is enumerated out and the microservice having CRUD over it is listed.</p> <p>i.e.</p> <ul>\n<li>Server.Database.Table.Schema.Column –&gt; CRUD – &gt;Microservice Name</li> </ul>\n<p> </p> <p>The ideal (but likely impractical goal) is to have just one Microservice per specified column. That is a microservices may have many CUD columns, but a column will have only one CUD microservice ( N columns :: 1 Microservice). </p> <p> </p> <p> Similarly, a table with</p> <ul>\n<li>Server.Database.Table.Schema.Column –&gt; R– &gt;Microservice </li> </ul>\n<p>can be used as a heat map to refactor as the migration occurs. We want to reduce hot spots (i.e. the number of Read microservices per column).</p> <p> </p> <h2>Building Microservices from Database Logins</h2> <p>Defining the actions that a microservice login can do cascades into a finite set of possible APIs. We are avoiding trying to define a microservice and then get the database access to support it. We are effectively changing the usual process upside down.</p> <p> </p> <p>Instead of the typical path of asking the client what it needs for an API (to keep it’s life simple), we are insuring that there is a <em>collection </em>of APIs that satisfies its needs – although these may be complicated to call. What we need to return to the classical simplicity is intermediate APIs.</p> <p> </p> <h3>Intermediate APIs</h3> <p>Intermediate APIs are APIs are do not have explicit  database CUD rights. They are intended to be helper APIs that talk to the database microservices above and present a simpler API to clients. They will call the above APIs to change the database. They may also be caching APIs and database reporting APIs.</p> <p> </p> <h1>A Walk Thru</h1> <p>Using the university model cited above, the first naïve step could be to create a</p> <ul>\n<li>Teacher API </li> <li>Student API </li> <li>Class API </li> </ul>\n<p>If you bring in column permissions you find that these can be decomposed further. The reason that there may be a single row in the database for each of the above comes from <em>Relational Database Design Normalization theory</em>.  Instead, we should try to decompose according to user permission sets. For example:</p> <ul>\n<li>Teacher API <ul>\n<li>Teacher MetaData API i.e. name, </li> <li>Teacher Address Info API </li> <li>Teacher Salary Info API </li> <li>Teacher HR API </li> <li>Teacher Card Access API </li> </ul>\n</li> <li>Student API <ul>\n<li>Student MetaData API, i.e. name, </li> <li>Student Address Info API </li> <li>Student Tuition API </li> <li>Student Awards API </li> <li>Student Card Access API </li> </ul>\n</li> </ul>\n<p>Our wishful state is that if you are authorized for an API, there is no need to check for further permissions. As I said, wishful. If you apply this concept strictly then you will likely end up with an unmanageable number of APIs that would be counter productive. This would be the case for an enterprise class system. For less complex systems, like customer retail systems, the number of permissions sets may be greatly reduced.</p> <p> </p> <p>With the Blackboard system (when I was working on it), we were enabling support for hundred of thousands permission sets that often contains hundred of permission each (i.e. each person had their own set, each set contains permissions to access building, Uris, copying machines, etc).</p> <p> </p> <p>An Intermediate API may be ClassAssignmentViewer. In this API, information from Student Metadata API, Teacher Metadata API and other APIs. Alternatively, it may be directly read only from the database.</p> <p> </p> <h1>Next Step</h1> <p>Once you have the microservices defined, you can start looking at segmenting the data store to match the microservices. When you leave a classic relational database, you may need to deal with issues such as referential integrity and foreign keys between microservices. If you have the microservice and the database login permissions pre-defined, then these issues are a magnitude simpler.</p> <h1>Bottom Line</h1> <p>The above is a sketch of what I discovered about migration process by trying several different approaches and seeing ongoing headaches, or, massive and risky refactoring.</p> <p> </p> <p>With the above, you can start with a small scope and implement it. The existing system keeps functioning and you have created a parallel access point to the data. As functioning sets are completed, you can cut over to some microservices while the rest is running on the classic big api approach.  You can eventually have the entire system up in parallel and then do a cut over to these microservices stubs. Over time, you may wish to decouple the data stores but that can be done later. You need to isolate the CUD first into microservice to be above to do that step.</p>\n",
            "content": "\n<p>The concepts of microservices is nice, but if you have a complex existing system the path is neither obvious or easy. I have seen Principal Architects throw up their hands and persuade the business that <em><strong>we need to build a new replacement system</strong></em> and that the old system is impossible to work with. This path tends to lead into overruns and often complete failures – I have recently seen that happen at a firm: “Just one year needed to deliver…” and three years later it was killed off because it had not been delivered.  The typical reported in industry literature statistics of 80—90% failure are very believable.</p> <p> </p> <p>Over decades, I have seen many failrues (usually on the side lines).  On the other hand, for a planned phrase migration I have seen repeated success. Often success or failure seem to be determined by the agile-ness of the management and technical leads coupled with the depth of analysis before the project start. Unfortunately deep analysis ends up with a waterfall like specification that result in locked-step development and no agile-ness around issues. Similarly, agile often result in superficial analysis (the time horizon for analysis is often just the end of the next sprint)  with many components failing to fit together properly over time! </p> <p> </p> <p>This post is looking at a heritage system and seeing how it can be converted to a microservices framework in an evolutionary manner. No total rewrite, just a phrased migration ending with a system that is close to a classic pro-forma microservice system.</p> <p> </p> <p>I made several runs at this problem, and what I describe below “feels good” – which to me usually mean a high probability of success with demonstrable steps at regular intervals. </p> <p> </p> <h1>Example System</h1> <p>I am going to borrow a university system template from my days working for <a href=\"http://www.blackboard.com/\">Blackboard</a>.  We have teachers, non-teaching staff, students, classes, building, security access cards, payment cards, etc.  At one point, components were in Delphi, C#, Java, C++ etc with the databases in SQL Server and Oracle. Not only is data shared, but permissions often need to be consistent and appropriate.</p> <p> </p> <p>I have tried a few running starts of microservicing  such a design, and at present, my best suggestion is this:</p> <ul>\n<li>Do NOT extend the microservicing  down to the database – there is a more elegant way to proceed </li> <li>Look at the scope of the microservices API very carefully – this is a narrow path that can explode into infinite microservices or a resurrection of legacy patterns </li> </ul>\n<h2>Elegant Microservice Database Access</h2> <p>Do not touch the database design at the start. You are just compounding the migration path needlessly at the start. Instead, for each microservice create a new database login that is named for the microservice and has (typically) CRUD permissions to:</p> <ul>\n<li>A table </li> <li>A subset of columns in a table </li> <li>An updateable view </li> <li>A subset of columns in an updateable view </li> </ul>\n<p>We term this the Crud-Columns. There is a temptation to incorporate multiple Crud-columns into one microservice – the problem is simple, what is the objective criteria to stop incorporating more Crud-Columns into this single microservice? If you go to one microservice for each Crud-Columns, then by counting the tables you have an estimate of the number of microservices that you will likely end up with…  oh… that may be a lot! At this point of time, you may really want to consider automatic code generation of many microservices – similar to what you see with Entity-Frameworks, except this would be called Microservices-Framework. </p> <p> </p> <p>This microservice may also have Read only permissions to other tables.  This other tables read only access  may be <em>transitory</em> for the migration. Regardless of final resolution, these tables must be directly related to the CRUD columns, and used to determine CUD decisions. At some future time, these rest calls to these read only tables may be redirected elsewhere (for example using a Moved to directive to a reporting microservices).</p> <p> </p> <p>Oh, I have introduced a new term “reporting microservices”.  This is a microservice with one or read Read Api’s – multiple calls may be exposed depending on filtering, sorting or user permissions.</p> <p> </p> <p>Microservices are not domain level APIs but at sub-domains or even sub-sub-domains. You should not be making small steps, instead, put on <a href=\"https://en.wikipedia.org/wiki/Seven-league_boots\">your seven-league boots</a>! </p> <p><img alt=\"American Trucking Industry 1952 Ad - Seven League Boots…\" src=\"http://www.bambootrading.com/2100/2110.JPG\"></p> <p> </p> <h3>Tracking microservices</h3> <p>Consider creating a table where every database column is enumerated out and the microservice having CRUD over it is listed.</p> <p>i.e.</p> <ul>\n<li>Server.Database.Table.Schema.Column –&gt; CRUD – &gt;Microservice Name</li> </ul>\n<p> </p> <p>The ideal (but likely impractical goal) is to have just one Microservice per specified column. That is a microservices may have many CUD columns, but a column will have only one CUD microservice ( N columns :: 1 Microservice). </p> <p> </p> <p> Similarly, a table with</p> <ul>\n<li>Server.Database.Table.Schema.Column –&gt; R– &gt;Microservice </li> </ul>\n<p>can be used as a heat map to refactor as the migration occurs. We want to reduce hot spots (i.e. the number of Read microservices per column).</p> <p> </p> <h2>Building Microservices from Database Logins</h2> <p>Defining the actions that a microservice login can do cascades into a finite set of possible APIs. We are avoiding trying to define a microservice and then get the database access to support it. We are effectively changing the usual process upside down.</p> <p> </p> <p>Instead of the typical path of asking the client what it needs for an API (to keep it’s life simple), we are insuring that there is a <em>collection </em>of APIs that satisfies its needs – although these may be complicated to call. What we need to return to the classical simplicity is intermediate APIs.</p> <p> </p> <h3>Intermediate APIs</h3> <p>Intermediate APIs are APIs are do not have explicit  database CUD rights. They are intended to be helper APIs that talk to the database microservices above and present a simpler API to clients. They will call the above APIs to change the database. They may also be caching APIs and database reporting APIs.</p> <p> </p> <h1>A Walk Thru</h1> <p>Using the university model cited above, the first naïve step could be to create a</p> <ul>\n<li>Teacher API </li> <li>Student API </li> <li>Class API </li> </ul>\n<p>If you bring in column permissions you find that these can be decomposed further. The reason that there may be a single row in the database for each of the above comes from <em>Relational Database Design Normalization theory</em>.  Instead, we should try to decompose according to user permission sets. For example:</p> <ul>\n<li>Teacher API <ul>\n<li>Teacher MetaData API i.e. name, </li> <li>Teacher Address Info API </li> <li>Teacher Salary Info API </li> <li>Teacher HR API </li> <li>Teacher Card Access API </li> </ul>\n</li> <li>Student API <ul>\n<li>Student MetaData API, i.e. name, </li> <li>Student Address Info API </li> <li>Student Tuition API </li> <li>Student Awards API </li> <li>Student Card Access API </li> </ul>\n</li> </ul>\n<p>Our wishful state is that if you are authorized for an API, there is no need to check for further permissions. As I said, wishful. If you apply this concept strictly then you will likely end up with an unmanageable number of APIs that would be counter productive. This would be the case for an enterprise class system. For less complex systems, like customer retail systems, the number of permissions sets may be greatly reduced.</p> <p> </p> <p>With the Blackboard system (when I was working on it), we were enabling support for hundred of thousands permission sets that often contains hundred of permission each (i.e. each person had their own set, each set contains permissions to access building, Uris, copying machines, etc).</p> <p> </p> <p>An Intermediate API may be ClassAssignmentViewer. In this API, information from Student Metadata API, Teacher Metadata API and other APIs. Alternatively, it may be directly read only from the database.</p> <p> </p> <h1>Next Step</h1> <p>Once you have the microservices defined, you can start looking at segmenting the data store to match the microservices. When you leave a classic relational database, you may need to deal with issues such as referential integrity and foreign keys between microservices. If you have the microservice and the database login permissions pre-defined, then these issues are a magnitude simpler.</p> <h1>Bottom Line</h1> <p>The above is a sketch of what I discovered about migration process by trying several different approaches and seeing ongoing headaches, or, massive and risky refactoring.</p> <p> </p> <p>With the above, you can start with a small scope and implement it. The existing system keeps functioning and you have created a parallel access point to the data. As functioning sets are completed, you can cut over to some microservices while the rest is running on the classic big api approach.  You can eventually have the entire system up in parallel and then do a cut over to these microservices stubs. Over time, you may wish to decouple the data stores but that can be done later. You need to isolate the CUD first into microservice to be above to do that step.</p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "Theory about Test Environments",
            "pubDate": "2016-05-29 01:18:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/05/theory-about-test-environments.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-5894616131780103649",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>Often my career has faced dealing with an arbitrary environment to test in. This environment preceded my arrival, and often was still there at my departure with many developers became fatalistic towards this arbitrary environment.  This is not good.</p> <p> </p> <h1>The Rhetorical Goal Recomposed</h1> <p>“We use our test environment to verify that our code changes will work as expected”</p> <p>While this assures upper management, it lacks specifics to evaluate if the test environment is appropriate or complete. A more objective measurement would be:</p> <ul>\n<li>The code changes perform as specified at the six-sigma level of certainty.</li> </ul>\n<p>This then logically cascades into sub-measurements:</p> <ul>\n<li>A1: The code changes perform as specified at the highest projected peak load for the next N year (typically 1-2) at the six-sigma level of certainty.</li> <li>A2: The code changes perform as specified on a fresh created (perfect) environment  at the six-sigma level of certainty.</li> <li>A3: The code changes perform as specified on a copy of production environment with random data at the six-sigma level of certainty.</li> </ul>\n<p>The last one is actually the most critical because too often there is bad data from <em>bad prior released code</em> (which may have be rolled back – but the corrupted data remained!) . There is a corollary:</p> <ul>\n<li>C1: The code changes do not need to perform as specified when the environment have had its data corrupted by arbitrary code and data changes that have not made it to production. In other words, ignore a corrupted test environment</li> </ul>\n<p> </p> <h1>Once thru is not enough!</h1> <p>Today’s systems are often multi-layers with timeouts, blockage under load and other things making the outcome not a certainty but a random event. Above, I cited <a href=\"https://en.wikipedia.org/wiki/Six_Sigma\">six sigma</a> – this is a classic level sought in quality assurance of mechanical processes.</p> <p> </p> <p>“A six sigma process is one in which 99.99966% of all opportunities to produce some feature of a part are statistically expected to be free of defects (3.4 defective features per million opportunities).”</p> <p> </p> <p>To translate this into a single test context – the test must run 1,000,000 times and fail less than4 times. Alternatively, <em><strong><u>250,000 times with no failures</u></strong></em>. </p> <p> </p> <h1>Load testing to reach six-sigma</h1> <p>Load testing will often result in 250,000 calls being made. In some cases, it may mean that the load test may need to run for 24 hours instead of 1 hour. There are some common problem with many load tests: </p> <ul>\n<li>The load test does not run on a full copy of the production environment – violates A3:</li> <li>The same data is used time and again for the tests – thus A3: the use of random data fails. </li> <ul>\n<li>If you have a system that has been running for 5 years, then the data should be selected based on user created data with 1/5 from each year</li> <li>If the system has had N releases, then the data should be selected on user created data with 1/n from each release period</li> </ul>\n</ul>\n<h1>Proposal for a Conforming Pattern</h1> <p><em><u>Preliminary development (PD)</u></em> is done on a virgin system each day. By virgin I mean that databases and other data stores are created from scripts and populated with perfect data. There may be super user data but no common user data.  This should be done by an automated process. I have seen this done in some firms and it has some real benefits:</p> <ul>\n<li>Integration tests must create (instead of borrow) users </li> <ul>\n<li>Integration tests are done immediately after build – the environment is confirmed before any developers arrive at work.</li> <li>Images of this environment could be saved to allow faster restores.</li> </ul>\n<li>Performance is good because the data store is small</li> <li>A test environment is much smaller and can be easily (and cheaply) created on one or more cloud services or even VMs</li> <li>Residue from bad code do not persist (often reducing triage time greatly) – when a developer realized they have accidentally jacked the data then they just blow away the environment and recreate it</li> </ul>\n<p>After the virgin system is built, the developer’s “release folder scripts” are executed – for example, adding new tables, altering stored procedures, adding new data to system tables. Then the integration tests are executed again. <strong><em>Some tests may fail. A simple solution that I have seen is for these tests to call into the data store to get the version number and add an extension to NUnit that indicate that this test applies to before of after this version number. </em></strong>Tests can then be excluded that are expected to fail (and also identified for a new version to be written). </p> <p> </p> <p><u>Integration development(ID) </u>applies to the situation where there may be multiple teams working on stuff that will go out in a single release. Often it is more efficient to keep the teams in complete isolation for preliminary development – if there are complexities and side-effects than only one team suffers. A new environment is created then each teams’ “release folder scripts” are executed and tests are executed. </p> <p>i.e. PD+PD+….+PD = ID</p> <p>This keeps the <em>number of moving code fragments</em> controlled.</p> <p> </p> <h3>Scope of Testing in PD and ID</h3> <p>A2 level is as far as we can do in this environment. We cannot do A1 or A3.</p> <p> </p> <p>SmokeTest development (STD) means that an image of the production data base is made available to the integration team and they can test the code changes using real data. Ideally, they should regress with users  created during each release period so artifact issues can be identified. This may be significant testing, but is not load testing because we do not push up to peak volumes. </p> <p>Tests either creates a new user (in the case of PD and ID) or searches for a random user that was created in release cycle 456 in the case of STD. Of course, code like SELECT TOP 1 *… should not be used, rather all users retrieved and one randomly selected.</p> <p> </p> <p>This gets us close to A3: if we do enough iterations.</p> <p> </p> <h1>Designing Unit Tests for multiple Test Environment</h1> <p>Designing a UserFactory with a signature such as</p> <blockquote> <p>UserFactory.GetUser(UserAttributes[] requiredAttributes)</p> </blockquote> <p>can simplify the development of unit tests that can be used across multiple environments. This UserFactory reads a configuration file which may have  properties such as</p> <ul>\n<li>CreateNewUser=”true”</li> <li>PickExistingUser=”ByCreateDate”</li> <li>PickExistingUser=”ByReleaseDate”</li> <li>PickExistingUser=”ByCreateDateMostInactive”</li> </ul>\n<p>In the first case, a user is created with the desired attributes.  In other cases, the attributes are used to filter the <em>production data to get a list of candidates to randomly pick from</em>. </p> <p> </p> <p>In stressing scenarios when we want to test for side-effects due to concurrent operation by the same user, then we could use the current second to select the same user for all tests starting in the current second.</p> <p> </p> <h1>Developers Hiding Significant Errors – Unintentional</h1> <p>At one firm, we successfully established the following guidance:</p> <ul>\n<li>Fatal: When the unexpected happen – for example, the error that was thrown was not mapped to a known error response (i.e. Unexpected Server Error should not be returned)</li> <li>Error: When an error happens that should not happen, i.e. try catch worked to recover the situation…. but…</li> <li>Warning: When the error was caused by customer input. The input must be recorded into the log (less passwords). This typically indicates a defect in UI, training or child applications</li> <li>Info: everything else, i.e. counts</li> <li>Debug: what ever</li> </ul>\n<p>We also implemented the ability to change the log4net settings on the fly – so we could, in production, get every message for a short period of time (massive logs)</p> <h1>Load Stress with Concurrency</h1> <p>Correct load testing is very challenging and requires significant design and statistics to do and validate the results.</p> <p> </p> <p>One of the simplest implementation is to have a week old copy of the database, capture all of the web request traffic in the last week and do a play back in a reduced time period. With new functionality extending existing APIs then we are reasonably good – except we need to make sure that we reach six-sigma level – i.e.  was there at least 250,000 calls???  This can be further complicated if the existing system has a 0.1% error rate. A 0.1% error rate means 250 errors are expected on average, unfortunately this means that detecting a 1 error in 250,000 calls difference is impossible from a single run (or even a dozen runs). Often the first stage is to drive error rates down to near zero on the existing code base. I have personally (over several months) a 50K/day exception logging rate to less than 10. It can be done – just a lot of systematic slow work (and fighting to get these <em>not business significant bug fixes </em>into production). IMHO, they are business significant: they reduce triage time, false leads, bug reports, and thus customer experience with the application.</p> <p> </p> <p>One of the issues is whether the 250,000 calls applies to the system as a whole – or just the method being added or modified? For true six-sigma, it needs to be the method modified – sorry! And if there are 250,000 different users (or other objects) to be tested, then random selection of test data is required.</p> <p> </p> <p>I advocate the use of PNUnit (Parallel Nunit) on multiple machines with a slight twist. In the above UserFactory.Get() described above, we randomly select the user, but  for stress testing, we could use the seconds (long) and modular it with the number of candidate users and then execute the tests. This approach intentionally creates a situation where concurrent activity will generated, potentially creating blocks, deadlocks and inconsistencies.</p> <p> </p> <p>There is a nasty problem with using integration tests mirroring the production distribution of calls. Marking tests appropriately may help, the test runner can them select the tests to simulate the actual production call distribution and rates. Of course, this means that there is data on the call rates and error rates from the production system.</p> <p> </p> <h3>Make sure that you are giving statistically correct reports!</h3> <p> </p> <p>The easy question to answer is “Does the new code make the error rate statistically worst?” Taking our example above of 0.1% error we had 250 errors being expected. If we want to have 95% confidence then we would need to see 325 errors to deem it to be worst. You must stop and think about this, because of the our stated goal was less than 1 error in 250,000 – and we ignore 75 more errors as not being significant!!! This is a very weak criteria. It also makes clear that driving down the back ground error rate is essential. You cannot get strong results with a high background error rate, you may only be able to demonstrate 1 sigma defect rate.</p> <p> </p> <p>In short, you can rarely have a better sigma rate than your current rate unless you fix the current code base to have a lower sigma rate.</p>\n",
            "content": "\n<p>Often my career has faced dealing with an arbitrary environment to test in. This environment preceded my arrival, and often was still there at my departure with many developers became fatalistic towards this arbitrary environment.  This is not good.</p> <p> </p> <h1>The Rhetorical Goal Recomposed</h1> <p>“We use our test environment to verify that our code changes will work as expected”</p> <p>While this assures upper management, it lacks specifics to evaluate if the test environment is appropriate or complete. A more objective measurement would be:</p> <ul>\n<li>The code changes perform as specified at the six-sigma level of certainty.</li> </ul>\n<p>This then logically cascades into sub-measurements:</p> <ul>\n<li>A1: The code changes perform as specified at the highest projected peak load for the next N year (typically 1-2) at the six-sigma level of certainty.</li> <li>A2: The code changes perform as specified on a fresh created (perfect) environment  at the six-sigma level of certainty.</li> <li>A3: The code changes perform as specified on a copy of production environment with random data at the six-sigma level of certainty.</li> </ul>\n<p>The last one is actually the most critical because too often there is bad data from <em>bad prior released code</em> (which may have be rolled back – but the corrupted data remained!) . There is a corollary:</p> <ul>\n<li>C1: The code changes do not need to perform as specified when the environment have had its data corrupted by arbitrary code and data changes that have not made it to production. In other words, ignore a corrupted test environment</li> </ul>\n<p> </p> <h1>Once thru is not enough!</h1> <p>Today’s systems are often multi-layers with timeouts, blockage under load and other things making the outcome not a certainty but a random event. Above, I cited <a href=\"https://en.wikipedia.org/wiki/Six_Sigma\">six sigma</a> – this is a classic level sought in quality assurance of mechanical processes.</p> <p> </p> <p>“A six sigma process is one in which 99.99966% of all opportunities to produce some feature of a part are statistically expected to be free of defects (3.4 defective features per million opportunities).”</p> <p> </p> <p>To translate this into a single test context – the test must run 1,000,000 times and fail less than4 times. Alternatively, <em><strong><u>250,000 times with no failures</u></strong></em>. </p> <p> </p> <h1>Load testing to reach six-sigma</h1> <p>Load testing will often result in 250,000 calls being made. In some cases, it may mean that the load test may need to run for 24 hours instead of 1 hour. There are some common problem with many load tests: </p> <ul>\n<li>The load test does not run on a full copy of the production environment – violates A3:</li> <li>The same data is used time and again for the tests – thus A3: the use of random data fails. </li> <ul>\n<li>If you have a system that has been running for 5 years, then the data should be selected based on user created data with 1/5 from each year</li> <li>If the system has had N releases, then the data should be selected on user created data with 1/n from each release period</li> </ul>\n</ul>\n<h1>Proposal for a Conforming Pattern</h1> <p><em><u>Preliminary development (PD)</u></em> is done on a virgin system each day. By virgin I mean that databases and other data stores are created from scripts and populated with perfect data. There may be super user data but no common user data.  This should be done by an automated process. I have seen this done in some firms and it has some real benefits:</p> <ul>\n<li>Integration tests must create (instead of borrow) users </li> <ul>\n<li>Integration tests are done immediately after build – the environment is confirmed before any developers arrive at work.</li> <li>Images of this environment could be saved to allow faster restores.</li> </ul>\n<li>Performance is good because the data store is small</li> <li>A test environment is much smaller and can be easily (and cheaply) created on one or more cloud services or even VMs</li> <li>Residue from bad code do not persist (often reducing triage time greatly) – when a developer realized they have accidentally jacked the data then they just blow away the environment and recreate it</li> </ul>\n<p>After the virgin system is built, the developer’s “release folder scripts” are executed – for example, adding new tables, altering stored procedures, adding new data to system tables. Then the integration tests are executed again. <strong><em>Some tests may fail. A simple solution that I have seen is for these tests to call into the data store to get the version number and add an extension to NUnit that indicate that this test applies to before of after this version number. </em></strong>Tests can then be excluded that are expected to fail (and also identified for a new version to be written). </p> <p> </p> <p><u>Integration development(ID) </u>applies to the situation where there may be multiple teams working on stuff that will go out in a single release. Often it is more efficient to keep the teams in complete isolation for preliminary development – if there are complexities and side-effects than only one team suffers. A new environment is created then each teams’ “release folder scripts” are executed and tests are executed. </p> <p>i.e. PD+PD+….+PD = ID</p> <p>This keeps the <em>number of moving code fragments</em> controlled.</p> <p> </p> <h3>Scope of Testing in PD and ID</h3> <p>A2 level is as far as we can do in this environment. We cannot do A1 or A3.</p> <p> </p> <p>SmokeTest development (STD) means that an image of the production data base is made available to the integration team and they can test the code changes using real data. Ideally, they should regress with users  created during each release period so artifact issues can be identified. This may be significant testing, but is not load testing because we do not push up to peak volumes. </p> <p>Tests either creates a new user (in the case of PD and ID) or searches for a random user that was created in release cycle 456 in the case of STD. Of course, code like SELECT TOP 1 *… should not be used, rather all users retrieved and one randomly selected.</p> <p> </p> <p>This gets us close to A3: if we do enough iterations.</p> <p> </p> <h1>Designing Unit Tests for multiple Test Environment</h1> <p>Designing a UserFactory with a signature such as</p> <blockquote> <p>UserFactory.GetUser(UserAttributes[] requiredAttributes)</p> </blockquote> <p>can simplify the development of unit tests that can be used across multiple environments. This UserFactory reads a configuration file which may have  properties such as</p> <ul>\n<li>CreateNewUser=”true”</li> <li>PickExistingUser=”ByCreateDate”</li> <li>PickExistingUser=”ByReleaseDate”</li> <li>PickExistingUser=”ByCreateDateMostInactive”</li> </ul>\n<p>In the first case, a user is created with the desired attributes.  In other cases, the attributes are used to filter the <em>production data to get a list of candidates to randomly pick from</em>. </p> <p> </p> <p>In stressing scenarios when we want to test for side-effects due to concurrent operation by the same user, then we could use the current second to select the same user for all tests starting in the current second.</p> <p> </p> <h1>Developers Hiding Significant Errors – Unintentional</h1> <p>At one firm, we successfully established the following guidance:</p> <ul>\n<li>Fatal: When the unexpected happen – for example, the error that was thrown was not mapped to a known error response (i.e. Unexpected Server Error should not be returned)</li> <li>Error: When an error happens that should not happen, i.e. try catch worked to recover the situation…. but…</li> <li>Warning: When the error was caused by customer input. The input must be recorded into the log (less passwords). This typically indicates a defect in UI, training or child applications</li> <li>Info: everything else, i.e. counts</li> <li>Debug: what ever</li> </ul>\n<p>We also implemented the ability to change the log4net settings on the fly – so we could, in production, get every message for a short period of time (massive logs)</p> <h1>Load Stress with Concurrency</h1> <p>Correct load testing is very challenging and requires significant design and statistics to do and validate the results.</p> <p> </p> <p>One of the simplest implementation is to have a week old copy of the database, capture all of the web request traffic in the last week and do a play back in a reduced time period. With new functionality extending existing APIs then we are reasonably good – except we need to make sure that we reach six-sigma level – i.e.  was there at least 250,000 calls???  This can be further complicated if the existing system has a 0.1% error rate. A 0.1% error rate means 250 errors are expected on average, unfortunately this means that detecting a 1 error in 250,000 calls difference is impossible from a single run (or even a dozen runs). Often the first stage is to drive error rates down to near zero on the existing code base. I have personally (over several months) a 50K/day exception logging rate to less than 10. It can be done – just a lot of systematic slow work (and fighting to get these <em>not business significant bug fixes </em>into production). IMHO, they are business significant: they reduce triage time, false leads, bug reports, and thus customer experience with the application.</p> <p> </p> <p>One of the issues is whether the 250,000 calls applies to the system as a whole – or just the method being added or modified? For true six-sigma, it needs to be the method modified – sorry! And if there are 250,000 different users (or other objects) to be tested, then random selection of test data is required.</p> <p> </p> <p>I advocate the use of PNUnit (Parallel Nunit) on multiple machines with a slight twist. In the above UserFactory.Get() described above, we randomly select the user, but  for stress testing, we could use the seconds (long) and modular it with the number of candidate users and then execute the tests. This approach intentionally creates a situation where concurrent activity will generated, potentially creating blocks, deadlocks and inconsistencies.</p> <p> </p> <p>There is a nasty problem with using integration tests mirroring the production distribution of calls. Marking tests appropriately may help, the test runner can them select the tests to simulate the actual production call distribution and rates. Of course, this means that there is data on the call rates and error rates from the production system.</p> <p> </p> <h3>Make sure that you are giving statistically correct reports!</h3> <p> </p> <p>The easy question to answer is “Does the new code make the error rate statistically worst?” Taking our example above of 0.1% error we had 250 errors being expected. If we want to have 95% confidence then we would need to see 325 errors to deem it to be worst. You must stop and think about this, because of the our stated goal was less than 1 error in 250,000 – and we ignore 75 more errors as not being significant!!! This is a very weak criteria. It also makes clear that driving down the back ground error rate is essential. You cannot get strong results with a high background error rate, you may only be able to demonstrate 1 sigma defect rate.</p> <p> </p> <p>In short, you can rarely have a better sigma rate than your current rate unless you fix the current code base to have a lower sigma rate.</p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "The sad state of evidence based development management patterns",
            "pubDate": "2016-05-12 13:09:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/05/the-sad-state-of-evidence-based.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-4176104235187941862",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>I have been in the development game for many decades. I did my first programs using <a href=\"https://en.wikipedia.org/wiki/APL_(programming_language)\">APL/360</a> and Fortran (<a href=\"https://en.wikipedia.org/wiki/WATFIV\">WatFiv</a>) at the University of Waterloo, and have seen and coded a lot of languages over the years (FORTH, COBOL, Asm, Pascal, B,C, C++, SAS, etc). </p> <p> </p> <p>My academic training was in Operations Research – that is <em>mathematical optimization of business processes</em>. Today, I look at the development processes that I see and it is dominantly “fly by the seats of the pants”, “everybody is doing it” or “academic correctness”. I am not talking about waterfall or agile or scrum. I am not talking about architecture etc. Yet is some ways I am. Some processes assert <a href=\"https://www.scrum.org/Blog/ArtMID/1765/ArticleID/14/%E2%80%98Evidence-Based-Management%E2%80%99-for-Software-Organizations\">Evidence Based Management</a>, yet fails to deliver the evidence of better results. <a href=\"http://blog.agilistic.nl/why-i-dont-believe-in-evidence-based-management-2-of-6-origins-and-ebm-applied-to-software-development/\">Some bloggers</a> detail the problems with EBM.  A few books attempt to summarize the little research that has occurred, such as <a href=\"http://www.amazon.ca/dp/0596808321\">\"Making Software: What Really Works and Why we Believe It\"</a></p> <p> </p> <p>As an Operation Research person, I would define the optimization problem facing a development manager or director or lead as follows:</p> <ul>\n<li>Performance (which often comes at increased man hours to develop and operational costs) </li> <li>Scalability (which often comes at increased man hours to develop and operational costs) </li> <li>Cost to deliver </li> <li>Accuracy of deliverable (Customer satisfaction) </li> <li>Completeness of deliverable </li> <li>Elapsed time to delivery (shorter time often exponentially increase cost to deliver and defect rates) </li> <li>Ongoing operational costs (a bad design may result in huge cloud computing costs) </li> <li>Time for a new developer to become efficient across the entire product </li> <li>Defect rate <ul>\n<li>Number of defects </li> <li>ETA from reporting to fix </li> </ul>\n</li> <li>Developer resources <ul>\n<li>For development </li> <li>For maintenance </li> </ul>\n</li> </ul>\n<p>All of these factors interact. For evidence, there are no studies and I do not expect them to be. Technology is changing too fast, there is huge differences between projects, and any study will be outdated before it is usable. There is some evidence that we can work from. </p> <h2>Lines of Code across a system</h2> <p>Lines of code directly impacts several of the above.</p> <ul>\n<li>Defect rate is a function of the number of lines of code ranging from 200/100K to 1000/100K lines [<a href=\"http://www.coverity.com/library/pdf/open_source_quality_report.pdf\">source</a>] which is scaled by developer skill level. Junior or new developers will have a higher defect rate. </li> <li>Some classic measures defined in the literature, for example, <a href=\"https://en.wikipedia.org/wiki/Cyclomatic_complexity\">cyclomatic complexity</a>. Studies find a positive correlation between cyclomatic complexity and defects: functions and methods that have the highest complexity tend to also contain the most defects. </li> <li>Time to deliver is often a function of the lines of code written. </li> </ul>\n<p><strong><em>There is a mistaken belief that lines of code is an immutable for a project.</em></strong> In the early 2000’s I lead a rewrite of a middle tier and backend tier (with the web front end being left as is), the original C++/SQL server code base was 474,000 lines of code and was the result of 25 man years of coding. With a team of 6 new (to the application) developers sent over from India and 2 intense local developer, we recreated these tiers with 100% api compliance in just 25,000 lines of code in about 8 weeks. 25 man years –&gt; 1 man year. a 20 fold decrease in code base. And the last factor was an increase in concurrent load by 20 fold.  </p> <p> </p> <p>On other projects I have seen massive copy and paste (with some minor change) that result in code bloat. When a bug is discovered it was often only fixed in some of the pastes. <a href=\"http://martinfowler.com/bliki/CannotMeasureProductivity.html\">Martin Fowler</a> describes Lines of Code as a measure of developer productivity as useless; the same applies to lines of code in a project.  A change of programming language can result in a 10 fold drop (or increase) in lines of code. A change of a developer can also result in a similar change – depending on skill sets.</p> <p> </p> <h2>Implementation Design</h2> <p>The use of Object-Relational Mapping (ORM) can often result in increased lines of code, defects, steeper learning curves and greater challenges addressing performance issues. A simple illustration is to move all addresses in Washington State from a master table to a child table. In SQL Server, TSQL – it is a one line statement, calling this from SQL it amounts to 4 lines of C# code. Using an ORM, this can quickly grow to 100-200 lines. ORMs came along because of a shortage of SQL developer skills. As with most things, it carry hidden costs that are omitted in the sales literature!</p> <p> </p> <p>“Correct academic design” does not mean effective (i.e. low cost) development. One of the worst systems (for performance and maintenance) that I have seen was absolutely beautifully designed with a massive array of well defined classes – which unfortunately ignored the database reality.  Many calls of a single method cascaded through these classes and resulted in 12 – 60 individual sql queries being executed against the database.  Most of the methods could be converted to a wrapper on a single stored procedure with a major improvement of performance. The object hierarchy was flattened (or downsized!).</p> <p> </p> <p>I extend the concept of <a href=\"https://en.wikipedia.org/wiki/Cyclomatic_complexity\">cyclomatic complexity</a> to the maximum stack depth in developer written code.  The greater the depth, the longer it takes to debug (because the developer has to walk through the stack) and likely to write. The learning curve goes up. I suggest a maximum depth of 7 (less than cyclomatic complexity), ideally 5. This number comes out of research for short term memory (<a href=\"https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two\">wikipedia</a>). Going beyond seven significantly increases the effort that a developer needs to make to understand the stack. On the one hand, having a deep hierarchy of objects looks nice academically – but it is counterproductive for efficient coding. Seven is a magic number to keep asking “Why do we have more than seven ….”</p> <h2>Developer Skill Sets</h2> <p>Many architects suffer from the delusion that all developers are as skilled as they are, i.e. IQs over 145.  During my high school teaching years, I was assigned both gifted classes and challenged classes – and learn to present appropriately to both. In some cities (for example Stockholm, Sweden) – 20% of the work force is in IT. This means that the IQ of the developers likely range from 100 upwards. When an application is released, the support developers likely will end up with an average IQ around 100. The question must be asked, how <em>simple is the code </em>to understand for future enhancements and maintenance?</p> <p> </p> <p>If a firm has a policy of significant use of off-shore or contractor resources, there are  further challenges:</p> <ul>\n<li>A high percentage of the paid time is in ramp-up mode </li> <li>There is a high level of non- conformity to existing standards and practices. <ul>\n<li>Higher defect rate, greater time for existing staff to come up to speed on the code </li> </ul>\n</li> <li>Size of team and ratio of application-experienced versus new developer can greatly alter delivery scheduled (see <a href=\"https://en.wikipedia.org/wiki/Brooks%E2%80%99_law\">Brook’s law</a>)  </li> </ul>\n<p>Pseudo coding different architecture rarely happens. It has some advantages – if you code up the most complex logic and then ask the question – “ A bug happens and nothing comes back, what are the steps to isolated the issue with certainty?” The architecture with the least diagnostic steps may be the more efficient one.</p> <p> </p> <p>Last, the availability now and in the future of developers with the appropriate skills.  The industry is full of technology that was hot and promised the moon and then were disrupted by a new technology (think of <a href=\"https://en.wikipedia.org/wiki/Delphi_(programming_language)\">Borland Delphi</a> and <a href=\"https://en.wikipedia.org/wiki/Pascal_(programming_language)\">Pascal</a>!). I often do a weighted value composed of years since launch, popularity at the moment and trend to refine choices (and in some cases to say No to a developer or architect that want to play with the latest and greatest!). Some sites are <a href=\"http://db-engines.com/en/ranking\">DB-Engine Ranking</a> and <a href=\"http://pypl.github.io/PYPL.html\">PYPL</a>.  After short listing, then it’s a matter of coding up some complex examples in each and counting lines of code needed. </p> <h2>Specification Completeness And Stability</h2> <p>On one side, I have worked with a few PMs that deliver wonderful specifications (200-500 pages) that had no change-orders between the first line of code being written and final delivery a year later. What was originally handed to developers was not changed. Work was done in sprints. The behavior and content of every web page was detailed. There was a clean and well-reviewed dictionary of terms and meanings. Needless to say, delivery was prompt, on schedule, etc.</p> <p> </p> <p>On the other side, I have had minor change-requests which mutated constantly. The number of lines of code written over all of these changes were 20x the number of lines of code finally delivered. </p> <h2>Concurrent Development</h2> <p>Concurrent development means that two or more set of changes were happening to the same code base. At one firm we had several git-hub forks: Master,Develop, Sprint, Epic and Saga. The title indicate when the changes were expected to be propagated to master. It worked reasonably, but often I ended up spending two days resolving conflicts and debugging bugs that were introduced whenever I attempted to get forks in sync. Concurrent development increases overhead exponentially according to the number of independent forks are active. Almost everything in development has exponential cost with size, <em>there is no economy of scale in development</em>.</p> <p> </p> <p>On the flip side, at Amazon using the microservices model, there were no interaction between feature requests. Each API was self contained and would evolve independently. If an API needed another API changed, then the independent API would be changed, tested and released. The dependent API then was developed against the released independent API. There was no code-juggling act. Each code base API was single development and self-contained. Dependencies were by API not libraries and code bases.</p> <p> </p> <h3>Bottom Line</h3> <p>Controlling costs and improving delivery depends greatly on the preparation work IMHO -- namely:</p> <ul>\n<li>Specification stability and completeness </li> <li>Architectural / Design being well crafted for the developer population </li> <li>Minimum noise (i.e. no concurrent development, change orders, change of priorities) </li> <li>Methodology (Scrum, Agile, Waterfall, Plan Driven) is of low significance IMHO – except for those selling it and ‘true believers’. </li> </ul>\n<p>On the flip side, often the business will demand delivery schedules that add technical debt and significantly increase ongoing costs.</p> <p> </p> <p>A common problem that I have seen is solving this multiple dimension problem by looking at just one (and rarely two) dimensions and discovering the consequences of that decision down stream.  I will continue to add additional dimensions as I recall them from past experience.</p>\n",
            "content": "\n<p>I have been in the development game for many decades. I did my first programs using <a href=\"https://en.wikipedia.org/wiki/APL_(programming_language)\">APL/360</a> and Fortran (<a href=\"https://en.wikipedia.org/wiki/WATFIV\">WatFiv</a>) at the University of Waterloo, and have seen and coded a lot of languages over the years (FORTH, COBOL, Asm, Pascal, B,C, C++, SAS, etc). </p> <p> </p> <p>My academic training was in Operations Research – that is <em>mathematical optimization of business processes</em>. Today, I look at the development processes that I see and it is dominantly “fly by the seats of the pants”, “everybody is doing it” or “academic correctness”. I am not talking about waterfall or agile or scrum. I am not talking about architecture etc. Yet is some ways I am. Some processes assert <a href=\"https://www.scrum.org/Blog/ArtMID/1765/ArticleID/14/%E2%80%98Evidence-Based-Management%E2%80%99-for-Software-Organizations\">Evidence Based Management</a>, yet fails to deliver the evidence of better results. <a href=\"http://blog.agilistic.nl/why-i-dont-believe-in-evidence-based-management-2-of-6-origins-and-ebm-applied-to-software-development/\">Some bloggers</a> detail the problems with EBM.  A few books attempt to summarize the little research that has occurred, such as <a href=\"http://www.amazon.ca/dp/0596808321\">\"Making Software: What Really Works and Why we Believe It\"</a></p> <p> </p> <p>As an Operation Research person, I would define the optimization problem facing a development manager or director or lead as follows:</p> <ul>\n<li>Performance (which often comes at increased man hours to develop and operational costs) </li> <li>Scalability (which often comes at increased man hours to develop and operational costs) </li> <li>Cost to deliver </li> <li>Accuracy of deliverable (Customer satisfaction) </li> <li>Completeness of deliverable </li> <li>Elapsed time to delivery (shorter time often exponentially increase cost to deliver and defect rates) </li> <li>Ongoing operational costs (a bad design may result in huge cloud computing costs) </li> <li>Time for a new developer to become efficient across the entire product </li> <li>Defect rate <ul>\n<li>Number of defects </li> <li>ETA from reporting to fix </li> </ul>\n</li> <li>Developer resources <ul>\n<li>For development </li> <li>For maintenance </li> </ul>\n</li> </ul>\n<p>All of these factors interact. For evidence, there are no studies and I do not expect them to be. Technology is changing too fast, there is huge differences between projects, and any study will be outdated before it is usable. There is some evidence that we can work from. </p> <h2>Lines of Code across a system</h2> <p>Lines of code directly impacts several of the above.</p> <ul>\n<li>Defect rate is a function of the number of lines of code ranging from 200/100K to 1000/100K lines [<a href=\"http://www.coverity.com/library/pdf/open_source_quality_report.pdf\">source</a>] which is scaled by developer skill level. Junior or new developers will have a higher defect rate. </li> <li>Some classic measures defined in the literature, for example, <a href=\"https://en.wikipedia.org/wiki/Cyclomatic_complexity\">cyclomatic complexity</a>. Studies find a positive correlation between cyclomatic complexity and defects: functions and methods that have the highest complexity tend to also contain the most defects. </li> <li>Time to deliver is often a function of the lines of code written. </li> </ul>\n<p><strong><em>There is a mistaken belief that lines of code is an immutable for a project.</em></strong> In the early 2000’s I lead a rewrite of a middle tier and backend tier (with the web front end being left as is), the original C++/SQL server code base was 474,000 lines of code and was the result of 25 man years of coding. With a team of 6 new (to the application) developers sent over from India and 2 intense local developer, we recreated these tiers with 100% api compliance in just 25,000 lines of code in about 8 weeks. 25 man years –&gt; 1 man year. a 20 fold decrease in code base. And the last factor was an increase in concurrent load by 20 fold.  </p> <p> </p> <p>On other projects I have seen massive copy and paste (with some minor change) that result in code bloat. When a bug is discovered it was often only fixed in some of the pastes. <a href=\"http://martinfowler.com/bliki/CannotMeasureProductivity.html\">Martin Fowler</a> describes Lines of Code as a measure of developer productivity as useless; the same applies to lines of code in a project.  A change of programming language can result in a 10 fold drop (or increase) in lines of code. A change of a developer can also result in a similar change – depending on skill sets.</p> <p> </p> <h2>Implementation Design</h2> <p>The use of Object-Relational Mapping (ORM) can often result in increased lines of code, defects, steeper learning curves and greater challenges addressing performance issues. A simple illustration is to move all addresses in Washington State from a master table to a child table. In SQL Server, TSQL – it is a one line statement, calling this from SQL it amounts to 4 lines of C# code. Using an ORM, this can quickly grow to 100-200 lines. ORMs came along because of a shortage of SQL developer skills. As with most things, it carry hidden costs that are omitted in the sales literature!</p> <p> </p> <p>“Correct academic design” does not mean effective (i.e. low cost) development. One of the worst systems (for performance and maintenance) that I have seen was absolutely beautifully designed with a massive array of well defined classes – which unfortunately ignored the database reality.  Many calls of a single method cascaded through these classes and resulted in 12 – 60 individual sql queries being executed against the database.  Most of the methods could be converted to a wrapper on a single stored procedure with a major improvement of performance. The object hierarchy was flattened (or downsized!).</p> <p> </p> <p>I extend the concept of <a href=\"https://en.wikipedia.org/wiki/Cyclomatic_complexity\">cyclomatic complexity</a> to the maximum stack depth in developer written code.  The greater the depth, the longer it takes to debug (because the developer has to walk through the stack) and likely to write. The learning curve goes up. I suggest a maximum depth of 7 (less than cyclomatic complexity), ideally 5. This number comes out of research for short term memory (<a href=\"https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two\">wikipedia</a>). Going beyond seven significantly increases the effort that a developer needs to make to understand the stack. On the one hand, having a deep hierarchy of objects looks nice academically – but it is counterproductive for efficient coding. Seven is a magic number to keep asking “Why do we have more than seven ….”</p> <h2>Developer Skill Sets</h2> <p>Many architects suffer from the delusion that all developers are as skilled as they are, i.e. IQs over 145.  During my high school teaching years, I was assigned both gifted classes and challenged classes – and learn to present appropriately to both. In some cities (for example Stockholm, Sweden) – 20% of the work force is in IT. This means that the IQ of the developers likely range from 100 upwards. When an application is released, the support developers likely will end up with an average IQ around 100. The question must be asked, how <em>simple is the code </em>to understand for future enhancements and maintenance?</p> <p> </p> <p>If a firm has a policy of significant use of off-shore or contractor resources, there are  further challenges:</p> <ul>\n<li>A high percentage of the paid time is in ramp-up mode </li> <li>There is a high level of non- conformity to existing standards and practices. <ul>\n<li>Higher defect rate, greater time for existing staff to come up to speed on the code </li> </ul>\n</li> <li>Size of team and ratio of application-experienced versus new developer can greatly alter delivery scheduled (see <a href=\"https://en.wikipedia.org/wiki/Brooks%E2%80%99_law\">Brook’s law</a>)  </li> </ul>\n<p>Pseudo coding different architecture rarely happens. It has some advantages – if you code up the most complex logic and then ask the question – “ A bug happens and nothing comes back, what are the steps to isolated the issue with certainty?” The architecture with the least diagnostic steps may be the more efficient one.</p> <p> </p> <p>Last, the availability now and in the future of developers with the appropriate skills.  The industry is full of technology that was hot and promised the moon and then were disrupted by a new technology (think of <a href=\"https://en.wikipedia.org/wiki/Delphi_(programming_language)\">Borland Delphi</a> and <a href=\"https://en.wikipedia.org/wiki/Pascal_(programming_language)\">Pascal</a>!). I often do a weighted value composed of years since launch, popularity at the moment and trend to refine choices (and in some cases to say No to a developer or architect that want to play with the latest and greatest!). Some sites are <a href=\"http://db-engines.com/en/ranking\">DB-Engine Ranking</a> and <a href=\"http://pypl.github.io/PYPL.html\">PYPL</a>.  After short listing, then it’s a matter of coding up some complex examples in each and counting lines of code needed. </p> <h2>Specification Completeness And Stability</h2> <p>On one side, I have worked with a few PMs that deliver wonderful specifications (200-500 pages) that had no change-orders between the first line of code being written and final delivery a year later. What was originally handed to developers was not changed. Work was done in sprints. The behavior and content of every web page was detailed. There was a clean and well-reviewed dictionary of terms and meanings. Needless to say, delivery was prompt, on schedule, etc.</p> <p> </p> <p>On the other side, I have had minor change-requests which mutated constantly. The number of lines of code written over all of these changes were 20x the number of lines of code finally delivered. </p> <h2>Concurrent Development</h2> <p>Concurrent development means that two or more set of changes were happening to the same code base. At one firm we had several git-hub forks: Master,Develop, Sprint, Epic and Saga. The title indicate when the changes were expected to be propagated to master. It worked reasonably, but often I ended up spending two days resolving conflicts and debugging bugs that were introduced whenever I attempted to get forks in sync. Concurrent development increases overhead exponentially according to the number of independent forks are active. Almost everything in development has exponential cost with size, <em>there is no economy of scale in development</em>.</p> <p> </p> <p>On the flip side, at Amazon using the microservices model, there were no interaction between feature requests. Each API was self contained and would evolve independently. If an API needed another API changed, then the independent API would be changed, tested and released. The dependent API then was developed against the released independent API. There was no code-juggling act. Each code base API was single development and self-contained. Dependencies were by API not libraries and code bases.</p> <p> </p> <h3>Bottom Line</h3> <p>Controlling costs and improving delivery depends greatly on the preparation work IMHO -- namely:</p> <ul>\n<li>Specification stability and completeness </li> <li>Architectural / Design being well crafted for the developer population </li> <li>Minimum noise (i.e. no concurrent development, change orders, change of priorities) </li> <li>Methodology (Scrum, Agile, Waterfall, Plan Driven) is of low significance IMHO – except for those selling it and ‘true believers’. </li> </ul>\n<p>On the flip side, often the business will demand delivery schedules that add technical debt and significantly increase ongoing costs.</p> <p> </p> <p>A common problem that I have seen is solving this multiple dimension problem by looking at just one (and rarely two) dimensions and discovering the consequences of that decision down stream.  I will continue to add additional dimensions as I recall them from past experience.</p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "Mining PubMed via Neo4J Graph Database–Getting the data",
            "pubDate": "2016-05-11 00:35:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/05/mining-pubmed-via-neo4j-graph.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-6695918311124531605",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>I have <a href=\"https://cfsremission.wordpress.com/\">a blog dealing with various complex autoimmune diseases</a> and spend a lot of time walking <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/\">links at PubMed.com</a>. Often readers send me an article that I missed.  </p> <p> </p> <p>I thought that a series of post on how to do it will help other people (including MDs, grad students and citizen scientists) better research medical issues.</p> <p> </p> <h1>Getting the data from Pub Med</h1> <p>I implemented a simple logic to obtain a collection of relevant articles:</p> <ul>\n<li> <h4>Query for 10,000 articles on a subject or key word</h4> </li> <li>Retrieve each of these articles <em>and any articles they referenced (i.e. the knowledge graph).</em>\n</li> <li><em>Keep repeating until you have enough articles or you run out of them!!</em></li> </ul>\n<h2><em>Getting the bootstrapping list of articles</em></h2> <p>A console application that reads the command line arguments and retrieves the list. For example,</p> <p>downloader.exe Crohn’s Disease</p> <p>which produces this URI</p> <p><a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=Crohn's+disease\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=Crohn's+disease</a> </p> <p></p> <p>This results in an XML file being sent</p> <p> </p> <p>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; <br>&lt;!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"<a href=\"http://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd%22\">http://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\"</a>&gt; <br>&lt;eSearchResult&gt;&lt;Count&gt;44880&lt;/Count&gt;&lt;RetMax&gt;10000&lt;/RetMax&gt;&lt;RetStart&gt;0&lt;/RetStart&gt;&lt;QueryKey&gt;1&lt;/QueryKey&gt;&lt;WebEnv&gt;NCID_1_84230330_130.14.22.215_9001_1462926138_46088356_0MetA0_S_MegaStore_F_1&lt;/WebEnv&gt;&lt;IdList&gt; <br>&lt;Id&gt;27159423&lt;/Id&gt; <br>&lt;Id&gt;27158773&lt;/Id&gt; <br>&lt;Id&gt;27158547&lt;/Id&gt; <br>&lt;Id&gt;27158537&lt;/Id&gt; <br>&lt;Id&gt;27158536&lt;/Id&gt; <br>&lt;Id&gt;27158345&lt;/Id&gt; <br>&lt;Id&gt;27158125&lt;/Id&gt; <br>&lt;Id&gt;27157449&lt;/Id&gt; <br>&lt;Id&gt;27156530&lt;/Id&gt; <br>&lt;Id&gt;27154890&lt;/Id&gt; <br>&lt;Id&gt;27154001&lt;/Id&gt; <br>&lt;Id&gt;27153721&lt;/Id&gt; <br>&lt;Id&gt;27152873&lt;/Id&gt; <br>&lt;Id&gt;27152872&lt;/Id&gt; <br>&lt;Id&gt;27152547&lt;/Id&gt;</p> <p></p> <p>So let us look at the code</p> <p></p> <p>class Program <br>    { <br>        static Downloader downloader = new Downloader(); <br>        static void Main(string[] args) <br>        { <br>            if (args.Length &gt; 0) <br>            { <br>                var search = new StringBuilder(); <br>                foreach (var arg in args) <br>                { <br>                    search.AppendFormat(\"{0} \", arg); <br>                } <br>                downloader.TermSearch(search.ToString()); <br>                downloader.ProcessAll(); <br>            } <br>            downloader.Save(); <br>        } <br>      }</p> <p>The Downloader class tracks articles already downloaded and those to do next. It simply starts downloading and saving each article summary to an Xml file using the unique article Id as the file name. I wanted to keep the summaries on my disk to speed reprocessing if my Neo4J model changes.</p> <p> <br>using System; <br>using System.Collections.Generic;       <br>using System.Collections.Concurrent; <br>using System.Net;                  </p> <p>using System.Linq; <br>using System.Threading.Tasks;  <br>using System.Xml;                     </p> <p>using System.Text;     <br>using System.Configuration; <br>using System.IO; <br>namespace PubMed <br>{ <br>    public class Downloader <br>    { <br>        // Entrez E-utilities at the US National Center for Biotechnology Information: <br>        static readonly String server = \"<a href=\"http://www.ncbi.nlm.nih.gov/entrez/eutils/%22;\">http://www.ncbi.nlm.nih.gov/entrez/eutils/\";</a> <br>        string dataFolder = \"C:\\\\PubMed\"; <br>        string logFile; <br>        public System.Collections.Concurrent.ConcurrentBag&lt;string&gt; index = new ConcurrentBag&lt;string&gt;(); <br>        public System.Collections.Concurrent.ConcurrentQueue&lt;string&gt; todo = new ConcurrentQueue&lt;string&gt;(); <br>        public Downloader() <br>        { <br>            logFile = Path.Combine(dataFolder, \"article.log\"); <br>            if (File.Exists(logFile)) <br>            { <br>                var lines = File.ReadAllLines(logFile); <br>                foreach (var line in lines) <br>                { <br>                    if (!string.IsNullOrWhiteSpace(line)) <br>                        index.Add(line); <br>                } <br>            } <br>        } <br>        public void Save() <br>        { <br>            File.WriteAllLines(logFile, index.ToArray()); <br>        }</p> <p>         public void ProcessAll() <br>        {</p> <p>            var nextId = string.Empty; <br>            while (todo.Count &gt; 0) <br>            { <br>                if (todo.Count &gt; 12) <br>                { <br>                    var tasks = new List&lt;Task&gt;(); <br>                    int t = 0; <br>                    for (t = 0; t &lt; 10; t++) <br>                    { <br>                        if (todo.TryDequeue(out nextId)) <br>                        {</p> <p>                            tasks.Add(Task.Factory.StartNew(() =&gt; NcbiPubmedArticle(nextId))); <br>                        } <br>                    } <br>                    Task.WaitAll(tasks.ToArray()); <br>                    Save(); <br>                } <br>                else <br>                { <br>                    if (todo.TryDequeue(out nextId)) <br>                    {</p> <p>                        NcbiPubmedArticle(nextId); <br>                    } <br>                } <br>            } <br>        }</p> <p>        public void TermSearch(String term) <br>        { <br>            var search = string.Format(\"<a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=%7B0%7D%22\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term={0}\"</a>, term.Replace(\" \", \"+\")); <br>            new WebClient().DownloadFile(new Uri(search), \"temp.log\"); <br>            var xml = new XmlDocument(); <br>            xml.Load(\"temp.Log\"); <br>            foreach (XmlNode node in xml.DocumentElement.SelectNodes(\"//Id\")) <br>            { <br>                var id = node.InnerText; <br>                if (!index.Contains(id) &amp;&amp; !todo.Contains(id)) <br>                { <br>                    todo.Enqueue(id); <br>                } <br>            } <br>        }</p> <p></p> <p>        public void NcbiPubmedArticle(String term) <br>        {</p> <p>            if (!index.Contains(term)) <br>            { <br>                try <br>                { <br>                    var fileLocation = Path.Combine(dataFolder, string.Format(\"{0}.xml\", term)); <br>                    if (File.Exists(fileLocation)) return; <br>                    var search = string.Format(\"<a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=%7B0%7D&amp;retmode=xml%22\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id={0}&amp;retmode=xml\"</a>, term); <br>                    new WebClient().DownloadFile(new Uri(search), fileLocation); <br>                    index.Add(term); <br>                    GetChildren(fileLocation); <br>                    Console.WriteLine(term); <br>                } <br>                catch <br>                {</p> <p>                } <br>            } <br>        } <br>        private void GetChildren(string fileName) <br>        { <br>            try <br>            { <br>                var dom = new XmlDocument(); <br>                dom.Load(fileName); <br>                foreach (XmlNode node in dom.DocumentElement.SelectNodes(\"//PMID\")) <br>                { <br>                    var id = node.InnerText; <br>                    if (!index.Contains(id) &amp;&amp; !todo.Contains(id)) <br>                    { <br>                        todo.Enqueue(id); <br>                    } <br>                } <br>            } <br>            catch (Exception exc) <br>            { <br>                Console.WriteLine(exc.Message); <br>            } <br>        } <br>    } <br>}</p> <p></p> <h1>Next Importing into Neo4J</h1> <p>An example of the structured data to load is shown below. Try defining your own model while you wait for the next post. </p> <p> </p> <p>&lt;?xml version=\"1.0\"?&gt; <br>&lt;!DOCTYPE PubmedArticleSet PUBLIC \"-//NLM//DTD PubMedArticle, 1st January 2016//EN\" \"<a href=\"http://www.ncbi.nlm.nih.gov/corehtml/query/DTD/pubmed_160101.dtd%22\">http://www.ncbi.nlm.nih.gov/corehtml/query/DTD/pubmed_160101.dtd\"</a>&gt; <br>&lt;PubmedArticleSet&gt; <br>&lt;PubmedArticle&gt; <br>    &lt;MedlineCitation Owner=\"NLM\" Status=\"MEDLINE\"&gt; <br>        &lt;PMID Version=\"1\"&gt;10022306&lt;/PMID&gt; <br>        &lt;DateCreated&gt; <br>            &lt;Year&gt;1999&lt;/Year&gt; <br>            &lt;Month&gt;02&lt;/Month&gt; <br>            &lt;Day&gt;25&lt;/Day&gt; <br>        &lt;/DateCreated&gt; <br>        &lt;DateCompleted&gt; <br>            &lt;Year&gt;1999&lt;/Year&gt; <br>            &lt;Month&gt;02&lt;/Month&gt; <br>            &lt;Day&gt;25&lt;/Day&gt; <br>        &lt;/DateCompleted&gt; <br>        &lt;DateRevised&gt; <br>            &lt;Year&gt;2006&lt;/Year&gt; <br>            &lt;Month&gt;11&lt;/Month&gt; <br>            &lt;Day&gt;15&lt;/Day&gt; <br>        &lt;/DateRevised&gt; <br>        &lt;Article PubModel=\"Print\"&gt; <br>            &lt;Journal&gt; <br>                &lt;ISSN IssnType=\"Print\"&gt;0378-4274&lt;/ISSN&gt; <br>                &lt;JournalIssue CitedMedium=\"Print\"&gt; <br>                    &lt;Volume&gt;102-103&lt;/Volume&gt; <br>                    &lt;PubDate&gt; <br>                        &lt;Year&gt;1998&lt;/Year&gt; <br>                        &lt;Month&gt;Dec&lt;/Month&gt; <br>                        &lt;Day&gt;28&lt;/Day&gt; <br>                    &lt;/PubDate&gt; <br>                &lt;/JournalIssue&gt; <br>                &lt;Title&gt;Toxicology letters&lt;/Title&gt; <br>                &lt;ISOAbbreviation&gt;Toxicol. Lett.&lt;/ISOAbbreviation&gt; <br>            &lt;/Journal&gt; <br>            &lt;ArticleTitle&gt;Epidemiological association in US veterans between Gulf War illness and exposures to anticholinesterases.&lt;/ArticleTitle&gt; <br>            &lt;Pagination&gt; <br>                &lt;MedlinePgn&gt;523-6&lt;/MedlinePgn&gt; <br>            &lt;/Pagination&gt; <br>            &lt;Abstract&gt; <br>                &lt;AbstractText&gt;To investigate complaints of Gulf War veterans, epidemiologic, case-control and animal modeling studies were performed. Looking for OPIDP variants, our epidemiologic project studied 249 Naval Reserve construction battalion (CB24) men. Extensive surveys were drawn for symptoms and exposures. An existing test (PAI) was used for neuropsychologic. Using FACTOR, LOGISTIC and FREQ in 6.07 SAS, symptom clusters were sought with high eigenvalues from orthogonally rotated two-stage factor analysis. After factor loadings and Kaiser measure for sampling adequacy (0.82), three major and three minor symptom clusters were identified. Internally consistent by Cronbach's coefficient, these were labeled syndromes: (1) impaired cognition; (2) confusion-ataxia; (3) arthro-myo-neuropathy; (4) phobia-apraxia; (5) fever-adenopathy; and (6) weakness-incontinence. Syndrome variants identified 63 patients (63/249, 25%) with 91 syndromes. With pyridostigmine bromide as the drug in these drug-chemical exposures, syndrome chemicals were: (1) pesticide-containing flea and tick collars (P &amp;lt; 0.001); (2) alarms from chemical weapons attacks (P &amp;lt; 0.001), being in a sector later found to have nerve agent exposure (P &amp;lt; 0.04); and (3) insect repellent (DEET) (P &amp;lt; 0.001). From CB24, 23 cases, 10 deployed and 10 non-deployed controls were studied. Auditory evoked potentials showed dysfunction (P &amp;lt; 0.02), nystagmic velocity on rotation testing, asymmetry on saccadic velocity (P &amp;lt; 0.04), somatosensory evoked potentials both sides (right P &amp;lt; 0.03, left P &amp;lt; 0.005) and synstagmic velocity after caloric stimulation bilaterally (P-range, 0.02-0.04). Brain dysfunction was shown on the Halstead Impairment Index (P &amp;lt; 0.01), General Neuropsychological Deficit Scale (P &amp;lt; 0.03) and Trail Making part B (P &amp;lt; 0.03). Butylcholinesterase phenotypes did not trend for inherent abnormalities. Parallel hen studies at Duke University established similar drug-chemical delayed neurotoxicity. These investigations lend credibility that sublethal exposures to drug-chemical combinations caused delayed-onset neurotoxic variants.&lt;/AbstractText&gt; <br>            &lt;/Abstract&gt; <br>            &lt;AuthorList CompleteYN=\"Y\"&gt; <br>                &lt;Author ValidYN=\"Y\"&gt; <br>                    &lt;LastName&gt;Kurt&lt;/LastName&gt; <br>                    &lt;ForeName&gt;T L&lt;/ForeName&gt; <br>                    &lt;Initials&gt;TL&lt;/Initials&gt; <br>                    &lt;AffiliationInfo&gt; <br>                        &lt;Affiliation&gt;Department of Internal Medicine, University of Texas Southwestern Medical School, Dallas 75235, USA.&lt;/Affiliation&gt; <br>                    &lt;/AffiliationInfo&gt; <br>                &lt;/Author&gt; <br>            &lt;/AuthorList&gt; <br>            &lt;Language&gt;eng&lt;/Language&gt; <br>            &lt;PublicationTypeList&gt; <br>                &lt;PublicationType UI=\"D016428\"&gt;Journal Article&lt;/PublicationType&gt; <br>                &lt;PublicationType UI=\"D013485\"&gt;Research Support, Non-U.S. Gov't&lt;/PublicationType&gt; <br>            &lt;/PublicationTypeList&gt; <br>        &lt;/Article&gt; <br>        &lt;MedlineJournalInfo&gt; <br>            &lt;Country&gt;NETHERLANDS&lt;/Country&gt; <br>            &lt;MedlineTA&gt;Toxicol Lett&lt;/MedlineTA&gt; <br>            &lt;NlmUniqueID&gt;7709027&lt;/NlmUniqueID&gt; <br>            &lt;ISSNLinking&gt;0378-4274&lt;/ISSNLinking&gt; <br>        &lt;/MedlineJournalInfo&gt; <br>        &lt;ChemicalList&gt; <br>            &lt;Chemical&gt; <br>                &lt;RegistryNumber&gt;0&lt;/RegistryNumber&gt; <br>                &lt;NameOfSubstance UI=\"D002800\"&gt;Cholinesterase Inhibitors&lt;/NameOfSubstance&gt; <br>            &lt;/Chemical&gt; <br>        &lt;/ChemicalList&gt; <br>        &lt;CitationSubset&gt;IM&lt;/CitationSubset&gt; <br>        &lt;MeshHeadingList&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D016022\"&gt;Case-Control Studies&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D002800\"&gt;Cholinesterase Inhibitors&lt;/DescriptorName&gt; <br>                &lt;QualifierName MajorTopicYN=\"Y\" UI=\"Q000633\"&gt;toxicity&lt;/QualifierName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D006801\"&gt;Humans&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D008297\"&gt;Male&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D018923\"&gt;Persian Gulf Syndrome&lt;/DescriptorName&gt; <br>                &lt;QualifierName MajorTopicYN=\"Y\" UI=\"Q000209\"&gt;etiology&lt;/QualifierName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"Y\" UI=\"D014728\"&gt;Veterans&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>        &lt;/MeshHeadingList&gt; <br>    &lt;/MedlineCitation&gt; <br>    &lt;PubmedData&gt; <br>        &lt;History&gt; <br>            &lt;PubMedPubDate PubStatus=\"pubmed\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>            &lt;/PubMedPubDate&gt; <br>            &lt;PubMedPubDate PubStatus=\"medline\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>                &lt;Hour&gt;0&lt;/Hour&gt; <br>                &lt;Minute&gt;1&lt;/Minute&gt; <br>            &lt;/PubMedPubDate&gt; <br>            &lt;PubMedPubDate PubStatus=\"entrez\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>                &lt;Hour&gt;0&lt;/Hour&gt; <br>                &lt;Minute&gt;0&lt;/Minute&gt; <br>            &lt;/PubMedPubDate&gt; <br>        &lt;/History&gt; <br>        &lt;PublicationStatus&gt;ppublish&lt;/PublicationStatus&gt; <br>        &lt;ArticleIdList&gt; <br>            &lt;ArticleId IdType=\"pubmed\"&gt;10022306&lt;/ArticleId&gt; <br>        &lt;/ArticleIdList&gt; <br>    &lt;/PubmedData&gt; <br>&lt;/PubmedArticle&gt;</p> <p>&lt;/PubmedArticleSet&gt; <br></p>\n",
            "content": "\n<p>I have <a href=\"https://cfsremission.wordpress.com/\">a blog dealing with various complex autoimmune diseases</a> and spend a lot of time walking <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/\">links at PubMed.com</a>. Often readers send me an article that I missed.  </p> <p> </p> <p>I thought that a series of post on how to do it will help other people (including MDs, grad students and citizen scientists) better research medical issues.</p> <p> </p> <h1>Getting the data from Pub Med</h1> <p>I implemented a simple logic to obtain a collection of relevant articles:</p> <ul>\n<li> <h4>Query for 10,000 articles on a subject or key word</h4> </li> <li>Retrieve each of these articles <em>and any articles they referenced (i.e. the knowledge graph).</em>\n</li> <li><em>Keep repeating until you have enough articles or you run out of them!!</em></li> </ul>\n<h2><em>Getting the bootstrapping list of articles</em></h2> <p>A console application that reads the command line arguments and retrieves the list. For example,</p> <p>downloader.exe Crohn’s Disease</p> <p>which produces this URI</p> <p><a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=Crohn's+disease\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=Crohn's+disease</a> </p> <p></p> <p>This results in an XML file being sent</p> <p> </p> <p>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; <br>&lt;!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"<a href=\"http://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd%22\">http://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\"</a>&gt; <br>&lt;eSearchResult&gt;&lt;Count&gt;44880&lt;/Count&gt;&lt;RetMax&gt;10000&lt;/RetMax&gt;&lt;RetStart&gt;0&lt;/RetStart&gt;&lt;QueryKey&gt;1&lt;/QueryKey&gt;&lt;WebEnv&gt;NCID_1_84230330_130.14.22.215_9001_1462926138_46088356_0MetA0_S_MegaStore_F_1&lt;/WebEnv&gt;&lt;IdList&gt; <br>&lt;Id&gt;27159423&lt;/Id&gt; <br>&lt;Id&gt;27158773&lt;/Id&gt; <br>&lt;Id&gt;27158547&lt;/Id&gt; <br>&lt;Id&gt;27158537&lt;/Id&gt; <br>&lt;Id&gt;27158536&lt;/Id&gt; <br>&lt;Id&gt;27158345&lt;/Id&gt; <br>&lt;Id&gt;27158125&lt;/Id&gt; <br>&lt;Id&gt;27157449&lt;/Id&gt; <br>&lt;Id&gt;27156530&lt;/Id&gt; <br>&lt;Id&gt;27154890&lt;/Id&gt; <br>&lt;Id&gt;27154001&lt;/Id&gt; <br>&lt;Id&gt;27153721&lt;/Id&gt; <br>&lt;Id&gt;27152873&lt;/Id&gt; <br>&lt;Id&gt;27152872&lt;/Id&gt; <br>&lt;Id&gt;27152547&lt;/Id&gt;</p> <p></p> <p>So let us look at the code</p> <p></p> <p>class Program <br>    { <br>        static Downloader downloader = new Downloader(); <br>        static void Main(string[] args) <br>        { <br>            if (args.Length &gt; 0) <br>            { <br>                var search = new StringBuilder(); <br>                foreach (var arg in args) <br>                { <br>                    search.AppendFormat(\"{0} \", arg); <br>                } <br>                downloader.TermSearch(search.ToString()); <br>                downloader.ProcessAll(); <br>            } <br>            downloader.Save(); <br>        } <br>      }</p> <p>The Downloader class tracks articles already downloaded and those to do next. It simply starts downloading and saving each article summary to an Xml file using the unique article Id as the file name. I wanted to keep the summaries on my disk to speed reprocessing if my Neo4J model changes.</p> <p> <br>using System; <br>using System.Collections.Generic;       <br>using System.Collections.Concurrent; <br>using System.Net;                  </p> <p>using System.Linq; <br>using System.Threading.Tasks;  <br>using System.Xml;                     </p> <p>using System.Text;     <br>using System.Configuration; <br>using System.IO; <br>namespace PubMed <br>{ <br>    public class Downloader <br>    { <br>        // Entrez E-utilities at the US National Center for Biotechnology Information: <br>        static readonly String server = \"<a href=\"http://www.ncbi.nlm.nih.gov/entrez/eutils/%22;\">http://www.ncbi.nlm.nih.gov/entrez/eutils/\";</a> <br>        string dataFolder = \"C:\\\\PubMed\"; <br>        string logFile; <br>        public System.Collections.Concurrent.ConcurrentBag&lt;string&gt; index = new ConcurrentBag&lt;string&gt;(); <br>        public System.Collections.Concurrent.ConcurrentQueue&lt;string&gt; todo = new ConcurrentQueue&lt;string&gt;(); <br>        public Downloader() <br>        { <br>            logFile = Path.Combine(dataFolder, \"article.log\"); <br>            if (File.Exists(logFile)) <br>            { <br>                var lines = File.ReadAllLines(logFile); <br>                foreach (var line in lines) <br>                { <br>                    if (!string.IsNullOrWhiteSpace(line)) <br>                        index.Add(line); <br>                } <br>            } <br>        } <br>        public void Save() <br>        { <br>            File.WriteAllLines(logFile, index.ToArray()); <br>        }</p> <p>         public void ProcessAll() <br>        {</p> <p>            var nextId = string.Empty; <br>            while (todo.Count &gt; 0) <br>            { <br>                if (todo.Count &gt; 12) <br>                { <br>                    var tasks = new List&lt;Task&gt;(); <br>                    int t = 0; <br>                    for (t = 0; t &lt; 10; t++) <br>                    { <br>                        if (todo.TryDequeue(out nextId)) <br>                        {</p> <p>                            tasks.Add(Task.Factory.StartNew(() =&gt; NcbiPubmedArticle(nextId))); <br>                        } <br>                    } <br>                    Task.WaitAll(tasks.ToArray()); <br>                    Save(); <br>                } <br>                else <br>                { <br>                    if (todo.TryDequeue(out nextId)) <br>                    {</p> <p>                        NcbiPubmedArticle(nextId); <br>                    } <br>                } <br>            } <br>        }</p> <p>        public void TermSearch(String term) <br>        { <br>            var search = string.Format(\"<a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term=%7B0%7D%22\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=Pubmed&amp;retmax=1000&amp;usehistory=y&amp;term={0}\"</a>, term.Replace(\" \", \"+\")); <br>            new WebClient().DownloadFile(new Uri(search), \"temp.log\"); <br>            var xml = new XmlDocument(); <br>            xml.Load(\"temp.Log\"); <br>            foreach (XmlNode node in xml.DocumentElement.SelectNodes(\"//Id\")) <br>            { <br>                var id = node.InnerText; <br>                if (!index.Contains(id) &amp;&amp; !todo.Contains(id)) <br>                { <br>                    todo.Enqueue(id); <br>                } <br>            } <br>        }</p> <p></p> <p>        public void NcbiPubmedArticle(String term) <br>        {</p> <p>            if (!index.Contains(term)) <br>            { <br>                try <br>                { <br>                    var fileLocation = Path.Combine(dataFolder, string.Format(\"{0}.xml\", term)); <br>                    if (File.Exists(fileLocation)) return; <br>                    var search = string.Format(\"<a href=\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=%7B0%7D&amp;retmode=xml%22\">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id={0}&amp;retmode=xml\"</a>, term); <br>                    new WebClient().DownloadFile(new Uri(search), fileLocation); <br>                    index.Add(term); <br>                    GetChildren(fileLocation); <br>                    Console.WriteLine(term); <br>                } <br>                catch <br>                {</p> <p>                } <br>            } <br>        } <br>        private void GetChildren(string fileName) <br>        { <br>            try <br>            { <br>                var dom = new XmlDocument(); <br>                dom.Load(fileName); <br>                foreach (XmlNode node in dom.DocumentElement.SelectNodes(\"//PMID\")) <br>                { <br>                    var id = node.InnerText; <br>                    if (!index.Contains(id) &amp;&amp; !todo.Contains(id)) <br>                    { <br>                        todo.Enqueue(id); <br>                    } <br>                } <br>            } <br>            catch (Exception exc) <br>            { <br>                Console.WriteLine(exc.Message); <br>            } <br>        } <br>    } <br>}</p> <p></p> <h1>Next Importing into Neo4J</h1> <p>An example of the structured data to load is shown below. Try defining your own model while you wait for the next post. </p> <p> </p> <p>&lt;?xml version=\"1.0\"?&gt; <br>&lt;!DOCTYPE PubmedArticleSet PUBLIC \"-//NLM//DTD PubMedArticle, 1st January 2016//EN\" \"<a href=\"http://www.ncbi.nlm.nih.gov/corehtml/query/DTD/pubmed_160101.dtd%22\">http://www.ncbi.nlm.nih.gov/corehtml/query/DTD/pubmed_160101.dtd\"</a>&gt; <br>&lt;PubmedArticleSet&gt; <br>&lt;PubmedArticle&gt; <br>    &lt;MedlineCitation Owner=\"NLM\" Status=\"MEDLINE\"&gt; <br>        &lt;PMID Version=\"1\"&gt;10022306&lt;/PMID&gt; <br>        &lt;DateCreated&gt; <br>            &lt;Year&gt;1999&lt;/Year&gt; <br>            &lt;Month&gt;02&lt;/Month&gt; <br>            &lt;Day&gt;25&lt;/Day&gt; <br>        &lt;/DateCreated&gt; <br>        &lt;DateCompleted&gt; <br>            &lt;Year&gt;1999&lt;/Year&gt; <br>            &lt;Month&gt;02&lt;/Month&gt; <br>            &lt;Day&gt;25&lt;/Day&gt; <br>        &lt;/DateCompleted&gt; <br>        &lt;DateRevised&gt; <br>            &lt;Year&gt;2006&lt;/Year&gt; <br>            &lt;Month&gt;11&lt;/Month&gt; <br>            &lt;Day&gt;15&lt;/Day&gt; <br>        &lt;/DateRevised&gt; <br>        &lt;Article PubModel=\"Print\"&gt; <br>            &lt;Journal&gt; <br>                &lt;ISSN IssnType=\"Print\"&gt;0378-4274&lt;/ISSN&gt; <br>                &lt;JournalIssue CitedMedium=\"Print\"&gt; <br>                    &lt;Volume&gt;102-103&lt;/Volume&gt; <br>                    &lt;PubDate&gt; <br>                        &lt;Year&gt;1998&lt;/Year&gt; <br>                        &lt;Month&gt;Dec&lt;/Month&gt; <br>                        &lt;Day&gt;28&lt;/Day&gt; <br>                    &lt;/PubDate&gt; <br>                &lt;/JournalIssue&gt; <br>                &lt;Title&gt;Toxicology letters&lt;/Title&gt; <br>                &lt;ISOAbbreviation&gt;Toxicol. Lett.&lt;/ISOAbbreviation&gt; <br>            &lt;/Journal&gt; <br>            &lt;ArticleTitle&gt;Epidemiological association in US veterans between Gulf War illness and exposures to anticholinesterases.&lt;/ArticleTitle&gt; <br>            &lt;Pagination&gt; <br>                &lt;MedlinePgn&gt;523-6&lt;/MedlinePgn&gt; <br>            &lt;/Pagination&gt; <br>            &lt;Abstract&gt; <br>                &lt;AbstractText&gt;To investigate complaints of Gulf War veterans, epidemiologic, case-control and animal modeling studies were performed. Looking for OPIDP variants, our epidemiologic project studied 249 Naval Reserve construction battalion (CB24) men. Extensive surveys were drawn for symptoms and exposures. An existing test (PAI) was used for neuropsychologic. Using FACTOR, LOGISTIC and FREQ in 6.07 SAS, symptom clusters were sought with high eigenvalues from orthogonally rotated two-stage factor analysis. After factor loadings and Kaiser measure for sampling adequacy (0.82), three major and three minor symptom clusters were identified. Internally consistent by Cronbach's coefficient, these were labeled syndromes: (1) impaired cognition; (2) confusion-ataxia; (3) arthro-myo-neuropathy; (4) phobia-apraxia; (5) fever-adenopathy; and (6) weakness-incontinence. Syndrome variants identified 63 patients (63/249, 25%) with 91 syndromes. With pyridostigmine bromide as the drug in these drug-chemical exposures, syndrome chemicals were: (1) pesticide-containing flea and tick collars (P &amp;lt; 0.001); (2) alarms from chemical weapons attacks (P &amp;lt; 0.001), being in a sector later found to have nerve agent exposure (P &amp;lt; 0.04); and (3) insect repellent (DEET) (P &amp;lt; 0.001). From CB24, 23 cases, 10 deployed and 10 non-deployed controls were studied. Auditory evoked potentials showed dysfunction (P &amp;lt; 0.02), nystagmic velocity on rotation testing, asymmetry on saccadic velocity (P &amp;lt; 0.04), somatosensory evoked potentials both sides (right P &amp;lt; 0.03, left P &amp;lt; 0.005) and synstagmic velocity after caloric stimulation bilaterally (P-range, 0.02-0.04). Brain dysfunction was shown on the Halstead Impairment Index (P &amp;lt; 0.01), General Neuropsychological Deficit Scale (P &amp;lt; 0.03) and Trail Making part B (P &amp;lt; 0.03). Butylcholinesterase phenotypes did not trend for inherent abnormalities. Parallel hen studies at Duke University established similar drug-chemical delayed neurotoxicity. These investigations lend credibility that sublethal exposures to drug-chemical combinations caused delayed-onset neurotoxic variants.&lt;/AbstractText&gt; <br>            &lt;/Abstract&gt; <br>            &lt;AuthorList CompleteYN=\"Y\"&gt; <br>                &lt;Author ValidYN=\"Y\"&gt; <br>                    &lt;LastName&gt;Kurt&lt;/LastName&gt; <br>                    &lt;ForeName&gt;T L&lt;/ForeName&gt; <br>                    &lt;Initials&gt;TL&lt;/Initials&gt; <br>                    &lt;AffiliationInfo&gt; <br>                        &lt;Affiliation&gt;Department of Internal Medicine, University of Texas Southwestern Medical School, Dallas 75235, USA.&lt;/Affiliation&gt; <br>                    &lt;/AffiliationInfo&gt; <br>                &lt;/Author&gt; <br>            &lt;/AuthorList&gt; <br>            &lt;Language&gt;eng&lt;/Language&gt; <br>            &lt;PublicationTypeList&gt; <br>                &lt;PublicationType UI=\"D016428\"&gt;Journal Article&lt;/PublicationType&gt; <br>                &lt;PublicationType UI=\"D013485\"&gt;Research Support, Non-U.S. Gov't&lt;/PublicationType&gt; <br>            &lt;/PublicationTypeList&gt; <br>        &lt;/Article&gt; <br>        &lt;MedlineJournalInfo&gt; <br>            &lt;Country&gt;NETHERLANDS&lt;/Country&gt; <br>            &lt;MedlineTA&gt;Toxicol Lett&lt;/MedlineTA&gt; <br>            &lt;NlmUniqueID&gt;7709027&lt;/NlmUniqueID&gt; <br>            &lt;ISSNLinking&gt;0378-4274&lt;/ISSNLinking&gt; <br>        &lt;/MedlineJournalInfo&gt; <br>        &lt;ChemicalList&gt; <br>            &lt;Chemical&gt; <br>                &lt;RegistryNumber&gt;0&lt;/RegistryNumber&gt; <br>                &lt;NameOfSubstance UI=\"D002800\"&gt;Cholinesterase Inhibitors&lt;/NameOfSubstance&gt; <br>            &lt;/Chemical&gt; <br>        &lt;/ChemicalList&gt; <br>        &lt;CitationSubset&gt;IM&lt;/CitationSubset&gt; <br>        &lt;MeshHeadingList&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D016022\"&gt;Case-Control Studies&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D002800\"&gt;Cholinesterase Inhibitors&lt;/DescriptorName&gt; <br>                &lt;QualifierName MajorTopicYN=\"Y\" UI=\"Q000633\"&gt;toxicity&lt;/QualifierName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D006801\"&gt;Humans&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D008297\"&gt;Male&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"N\" UI=\"D018923\"&gt;Persian Gulf Syndrome&lt;/DescriptorName&gt; <br>                &lt;QualifierName MajorTopicYN=\"Y\" UI=\"Q000209\"&gt;etiology&lt;/QualifierName&gt; <br>            &lt;/MeshHeading&gt; <br>            &lt;MeshHeading&gt; <br>                &lt;DescriptorName MajorTopicYN=\"Y\" UI=\"D014728\"&gt;Veterans&lt;/DescriptorName&gt; <br>            &lt;/MeshHeading&gt; <br>        &lt;/MeshHeadingList&gt; <br>    &lt;/MedlineCitation&gt; <br>    &lt;PubmedData&gt; <br>        &lt;History&gt; <br>            &lt;PubMedPubDate PubStatus=\"pubmed\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>            &lt;/PubMedPubDate&gt; <br>            &lt;PubMedPubDate PubStatus=\"medline\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>                &lt;Hour&gt;0&lt;/Hour&gt; <br>                &lt;Minute&gt;1&lt;/Minute&gt; <br>            &lt;/PubMedPubDate&gt; <br>            &lt;PubMedPubDate PubStatus=\"entrez\"&gt; <br>                &lt;Year&gt;1999&lt;/Year&gt; <br>                &lt;Month&gt;2&lt;/Month&gt; <br>                &lt;Day&gt;18&lt;/Day&gt; <br>                &lt;Hour&gt;0&lt;/Hour&gt; <br>                &lt;Minute&gt;0&lt;/Minute&gt; <br>            &lt;/PubMedPubDate&gt; <br>        &lt;/History&gt; <br>        &lt;PublicationStatus&gt;ppublish&lt;/PublicationStatus&gt; <br>        &lt;ArticleIdList&gt; <br>            &lt;ArticleId IdType=\"pubmed\"&gt;10022306&lt;/ArticleId&gt; <br>        &lt;/ArticleIdList&gt; <br>    &lt;/PubmedData&gt; <br>&lt;/PubmedArticle&gt;</p> <p>&lt;/PubmedArticleSet&gt; <br></p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "Microservices–Do it right!",
            "pubDate": "2016-05-07 19:52:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/05/microservicesdo-it-right.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-8631168033956420012",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "\n<p>In my earlier post, <a href=\"http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/an-financially-frugal-architectural.html\">A Financially Frugal Architectural Pattern for the Cloud</a>,  I advocated the use of microservices. Microservices are similar to REST, a concept or pattern or architectural standard, unlike  SOAP which is standards based. The modern IT industry trend towards “good enough”,  “lip-service” and “we’ll fix it in the next release”.  A contemporary application may use relational database software (SQL Server, Oracle, MySql) and thus the developers (and their management) would assert that their is a relational database system. If I move a magnetic tape based system into tables (one table for each type of tape) using relational database software – would that make it a relational database system? <strong>My opinion is no – never!!!</strong></p> <p> </p> <p><strong>Then what makes it one? </strong>The data has been fully normalized in the logical model. Often the database has <em>never been</em> reviewed for normalization  despite such information being ancient (see William Kent, <a href=\"http://www.bkent.net/Doc/simple5.htm\">A Simple Guide to Five Normal Forms in Relational Database Theory</a>, 1982), older than many developers. The implementation may be de-normalized in the physical model (if you have just a ‘database model’ and not separate physical and logical, then you are likely heading to trouble – in time (usually after the original developers have left!). For NoSql database, there is a lot of ancient literature out there dealing with both <strong>hierarchical databases</strong> and <strong>network databases </strong>which should also be used with MongoDB and Neo4j – but likely not.</p> <p> </p> <p>My academic training is in mathematics and thus axioms and deriving theorems from them though rigorous logic.  The normalization of databases is immediately attractive to me. Knowing the literature (especially <a href=\"https://en.wikipedia.org/wiki/Christopher_J._Date\">Christopher J.Date</a>’s early writings from the 1970’s) is essential since “\"Those who do not learn history are doomed to repeat it.”</p> <p> </p> <h1>Microservices Normalization Rules</h1> <p>Below are my attempt to define equivalent rules for microservices. They will likely be revised over time. They are very mathematical in definition by intent. Martin Fowler’s <a href=\"http://martinfowler.com/articles/microservices.html\">article</a> is also a good read. Much of the discussion on the web is at a high level (the hand waving level), such as <a href=\"http://blog.ness-ses.com/microservices-architecture-and-design-principles\">Microservices Architecture and Design Principles</a>,  <a href=\"https://techietweak.wordpress.com/2015/07/05/mdp/\">Microservices Design Principles</a>, with some echoing some of the issues cited below <a href=\"https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/\">Adopting Microservices at Netflix: Lessons for Architectural Design</a></p> <p> </p> <p>A public REST API consumed over the internet is probably not a microservice. It may use many microservices and other composite APIs.</p> <p> </p> <ul>\n<li>\n<strong>Composite API</strong>: A service that consumes other composite APIs and/or microservices but do not qualify below </li> <li>\n<strong>Independent Microservice</strong>: A service that does not call any other microservices </li> <li>\n<strong>Dependent Microservice</strong>: A service that calls Independent Microservices <em>in parallel</em>\n</li> </ul>\n<h2><strong>An Independent Microservice</strong></h2> <p> An independent microservice is the <em><strong>exclusive owner</strong></em> of a data store. </p> <ul>\n<li>No other service or system may access the data store. </li> <li>A microservice may change the software used to create the datastore with no consequences on any other system. </li> </ul>\n<ul>\n<li>A microservice does not make calls to other services <ul>\n<li>An corollary of this is that microservices rarely use any libraries that are not generic across the industry <ul>\n<li>Exception: libraries of static functions that are explicit to a firm, for example, encryption of  keys (i.e. Identity Integers –&gt; strings) </li> </ul>\n</li> </ul>\n</li> <li>A microservice may contain publishers <ul>\n<li>The datastore that it controls may need to be pushed to reporting and other systems </li> </ul>\n</li> <li>A microservice may create log records that are directly consumed by other systems. <ul>\n<li>Non-blocking outputs from a microservice are fine </li> </ul>\n</li> <li>A microservice may make periodic calls. <ul>\n<li>A microservice may pull things off a queue, or push things to a queue <ul>\n<li>The nature of this data is transient. The queue services interaction must match the model of an API call and response. The call comes from one queue and written to another queue. <ul>\n<li>Ideally there will be no references to queues inside the microservice. </li> <li>A call to the microservice would start reading a queue at a regular interval (the queue is in the call parameters) </li> <li>The data on the queue would specify where the results should be sent </li> </ul>\n</li> </ul>\n</li> <li>A microservice should not pull data from (non-microservice) data store. <ul>\n<li>Exception: a configurationless implementation such as described for queues above is fine. </li> </ul>\n</li> </ul>\n</li> <li>A microservice configuration should never reference anything outside of it’s own world. <ul>\n<li>Configuration Injection is allowed. Microservice is deployed and loaded, then a call is done to supply it’s configuration. </li> </ul>\n</li> </ul>\n<h2>A Dependent Microservice</h2> <ul>\n<li>Has exclusive ownership of it’s data store just like an independent microservice.</li> <li>Calls dependent microservice to <em>obtain data only (no update, delete or create)</em>\n</li> <ul>\n<li>Create Update Delete calls must always go to the independent microservice that owns it, no relaying should occur.</li> <li>Calls are in parallel, never sequential</li> </ul>\n<li>Note: Some types of queries may be inefficient, those should be directed at a reporting microservice (which independent microservices may publish to) or a composite service.</li> </ul>\n<p> </p> <h1>Control of Microservices</h1> <p>The best model that I have seen is one that some groups at Amazon used.</p> <ul>\n<li>All calls to the microservice must have an identifier (cookie?) that identifies the caller. <ul>\n<li>The microservice determines if it is an authorized caller based on the identifier and possibly the IP address </li> </ul>\n</li> <li>The consumer of the microservice must be authorized by the owner of the microservice based on: a contract containing at least <ul>\n<li>Daily load estimates </li> <li>Peak load estimates </li> <li>Availability </li> </ul>\n</li> <li>The microservice may disable any consumer that exceeds the contracted load. </li> <li>The consumer should be given a number from 1-100 indicating business importance. <ul>\n<li>If the microservice is stressed, then those services with lower values will be disabled. </li> </ul>\n</li> <li>There should always be SLA in place </li> </ul>\n<p>Microservices should be executed on isolated VMs behind  a load distributor. The datastore should be also on a dedicated set of VMs, for example a set of VMs supporting a Casandra implementation.</p> <p> </p> <h1>More suggestions?</h1> <p>To quote Martin Fowler, “If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that's less explicit and harder to control.” I have seen this happen – with the appearance of microservices (because there are tons of REST APis on different servers) but behind this layer, there are shared DLL’s accessing multiple databases causing endless pains with keeping DLL current as features are added or bugs fixed. A bug in a single library may require the fix to be propagated to a dozen REST APIs’. If it must be propagated it is not a microservice.</p> <p> </p> <p>My goal with this post is to define a set of <em><strong>objective check items</strong></em> that can be clearly determined by inspection. The ideal implementation would have all of them passing. </p> <p> </p> <p>One of the side-effects is that the rules can often be <em><strong>inconvenient</strong></em> for quick designs. A rethinking of what you are trying to do often results – similar to what I have seen happen when you push for full normalization in a logical model.  </p> <p> </p> <p>Do you have further suggestions?</p>\n",
            "content": "\n<p>In my earlier post, <a href=\"http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/an-financially-frugal-architectural.html\">A Financially Frugal Architectural Pattern for the Cloud</a>,  I advocated the use of microservices. Microservices are similar to REST, a concept or pattern or architectural standard, unlike  SOAP which is standards based. The modern IT industry trend towards “good enough”,  “lip-service” and “we’ll fix it in the next release”.  A contemporary application may use relational database software (SQL Server, Oracle, MySql) and thus the developers (and their management) would assert that their is a relational database system. If I move a magnetic tape based system into tables (one table for each type of tape) using relational database software – would that make it a relational database system? <strong>My opinion is no – never!!!</strong></p> <p> </p> <p><strong>Then what makes it one? </strong>The data has been fully normalized in the logical model. Often the database has <em>never been</em> reviewed for normalization  despite such information being ancient (see William Kent, <a href=\"http://www.bkent.net/Doc/simple5.htm\">A Simple Guide to Five Normal Forms in Relational Database Theory</a>, 1982), older than many developers. The implementation may be de-normalized in the physical model (if you have just a ‘database model’ and not separate physical and logical, then you are likely heading to trouble – in time (usually after the original developers have left!). For NoSql database, there is a lot of ancient literature out there dealing with both <strong>hierarchical databases</strong> and <strong>network databases </strong>which should also be used with MongoDB and Neo4j – but likely not.</p> <p> </p> <p>My academic training is in mathematics and thus axioms and deriving theorems from them though rigorous logic.  The normalization of databases is immediately attractive to me. Knowing the literature (especially <a href=\"https://en.wikipedia.org/wiki/Christopher_J._Date\">Christopher J.Date</a>’s early writings from the 1970’s) is essential since “\"Those who do not learn history are doomed to repeat it.”</p> <p> </p> <h1>Microservices Normalization Rules</h1> <p>Below are my attempt to define equivalent rules for microservices. They will likely be revised over time. They are very mathematical in definition by intent. Martin Fowler’s <a href=\"http://martinfowler.com/articles/microservices.html\">article</a> is also a good read. Much of the discussion on the web is at a high level (the hand waving level), such as <a href=\"http://blog.ness-ses.com/microservices-architecture-and-design-principles\">Microservices Architecture and Design Principles</a>,  <a href=\"https://techietweak.wordpress.com/2015/07/05/mdp/\">Microservices Design Principles</a>, with some echoing some of the issues cited below <a href=\"https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/\">Adopting Microservices at Netflix: Lessons for Architectural Design</a></p> <p> </p> <p>A public REST API consumed over the internet is probably not a microservice. It may use many microservices and other composite APIs.</p> <p> </p> <ul>\n<li>\n<strong>Composite API</strong>: A service that consumes other composite APIs and/or microservices but do not qualify below </li> <li>\n<strong>Independent Microservice</strong>: A service that does not call any other microservices </li> <li>\n<strong>Dependent Microservice</strong>: A service that calls Independent Microservices <em>in parallel</em>\n</li> </ul>\n<h2><strong>An Independent Microservice</strong></h2> <p> An independent microservice is the <em><strong>exclusive owner</strong></em> of a data store. </p> <ul>\n<li>No other service or system may access the data store. </li> <li>A microservice may change the software used to create the datastore with no consequences on any other system. </li> </ul>\n<ul>\n<li>A microservice does not make calls to other services <ul>\n<li>An corollary of this is that microservices rarely use any libraries that are not generic across the industry <ul>\n<li>Exception: libraries of static functions that are explicit to a firm, for example, encryption of  keys (i.e. Identity Integers –&gt; strings) </li> </ul>\n</li> </ul>\n</li> <li>A microservice may contain publishers <ul>\n<li>The datastore that it controls may need to be pushed to reporting and other systems </li> </ul>\n</li> <li>A microservice may create log records that are directly consumed by other systems. <ul>\n<li>Non-blocking outputs from a microservice are fine </li> </ul>\n</li> <li>A microservice may make periodic calls. <ul>\n<li>A microservice may pull things off a queue, or push things to a queue <ul>\n<li>The nature of this data is transient. The queue services interaction must match the model of an API call and response. The call comes from one queue and written to another queue. <ul>\n<li>Ideally there will be no references to queues inside the microservice. </li> <li>A call to the microservice would start reading a queue at a regular interval (the queue is in the call parameters) </li> <li>The data on the queue would specify where the results should be sent </li> </ul>\n</li> </ul>\n</li> <li>A microservice should not pull data from (non-microservice) data store. <ul>\n<li>Exception: a configurationless implementation such as described for queues above is fine. </li> </ul>\n</li> </ul>\n</li> <li>A microservice configuration should never reference anything outside of it’s own world. <ul>\n<li>Configuration Injection is allowed. Microservice is deployed and loaded, then a call is done to supply it’s configuration. </li> </ul>\n</li> </ul>\n<h2>A Dependent Microservice</h2> <ul>\n<li>Has exclusive ownership of it’s data store just like an independent microservice.</li> <li>Calls dependent microservice to <em>obtain data only (no update, delete or create)</em>\n</li> <ul>\n<li>Create Update Delete calls must always go to the independent microservice that owns it, no relaying should occur.</li> <li>Calls are in parallel, never sequential</li> </ul>\n<li>Note: Some types of queries may be inefficient, those should be directed at a reporting microservice (which independent microservices may publish to) or a composite service.</li> </ul>\n<p> </p> <h1>Control of Microservices</h1> <p>The best model that I have seen is one that some groups at Amazon used.</p> <ul>\n<li>All calls to the microservice must have an identifier (cookie?) that identifies the caller. <ul>\n<li>The microservice determines if it is an authorized caller based on the identifier and possibly the IP address </li> </ul>\n</li> <li>The consumer of the microservice must be authorized by the owner of the microservice based on: a contract containing at least <ul>\n<li>Daily load estimates </li> <li>Peak load estimates </li> <li>Availability </li> </ul>\n</li> <li>The microservice may disable any consumer that exceeds the contracted load. </li> <li>The consumer should be given a number from 1-100 indicating business importance. <ul>\n<li>If the microservice is stressed, then those services with lower values will be disabled. </li> </ul>\n</li> <li>There should always be SLA in place </li> </ul>\n<p>Microservices should be executed on isolated VMs behind  a load distributor. The datastore should be also on a dedicated set of VMs, for example a set of VMs supporting a Casandra implementation.</p> <p> </p> <h1>More suggestions?</h1> <p>To quote Martin Fowler, “If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that's less explicit and harder to control.” I have seen this happen – with the appearance of microservices (because there are tons of REST APis on different servers) but behind this layer, there are shared DLL’s accessing multiple databases causing endless pains with keeping DLL current as features are added or bugs fixed. A bug in a single library may require the fix to be propagated to a dozen REST APIs’. If it must be propagated it is not a microservice.</p> <p> </p> <p>My goal with this post is to define a set of <em><strong>objective check items</strong></em> that can be clearly determined by inspection. The ideal implementation would have all of them passing. </p> <p> </p> <p>One of the side-effects is that the rules can often be <em><strong>inconvenient</strong></em> for quick designs. A rethinking of what you are trying to do often results – similar to what I have seen happen when you push for full normalization in a logical model.  </p> <p> </p> <p>Do you have further suggestions?</p>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "Testing Angular Directives with Karma, Mocha, and Phantom",
            "pubDate": "2016-04-26 20:44:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/04/testing-angular-directives-with-karma.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-7399443093584438037",
            "author": "Dina Berry",
            "thumbnail": "",
            "description": "\n<h1>\n</h1>\n<h3>\nAll Code on Github</h3>\nThe entire code from this article is on <a href=\"https://github.com/dfberry/AngularDirectiveKarma\">github</a>. <br><h3>\nIntroduction</h3>\nAngular Directives are part of the web part/components group of client-side  tools that allow quick additions to web pages. The idea is that with a very simple and short addition to an html page, complex functionality and UX are available. <br><br>\nImagine a user login form with the traditional validation contained in a html template and Angular controller. The main, calling web page’s html could just include <strong>&lt;login&gt;&lt;/login&gt;</strong> to bring in that rich validation and display. Directives are wonderful for encapsulating the complexity away from the containing HTML.<br><br>\nTesting the directive is trickier. Granted the controller isn’t difficult to test. But the compiled html and scope are not so easy. <br><br>\nPerhaps it is enough to test the controller, and depend on the browser and the Angular library to manage the rest.  You could definitely make that case. Or perhaps you leave the compiled directive testing to the end to end (e2e) tests. That is also a fair. If either of these solutions doesn’t work for you, this article will explain 3 ways to test the compiled Angular directive. <br><br><strong>Template </strong><br>\nWhen you build the directive, you can choose static (hopefully short) html in the definition of the directive.  Notice that {{data}} will be replaced with the $scope.data value from the directive.<br><blockquote>\n<span>exports.user = function() { <br>  return { <br>    controller: 'userController', <br>    template: '&lt;div class=\"user\"&gt;{{data}}&lt;/div&gt;' <br>    }; <br>};</span>\n</blockquote>\n<strong>TemplateUrl</strong><br>\nIf you have more html or want to separate the html from the Angular directive, templateUrl is the way to go. <br><blockquote>\n<pre><span>exports.menu = function() {\n return {\n controller: 'menuController',\n templateUrl: '/templates/menu.html'\n };\n};</span></pre>\n</blockquote>\n<pre><span>The html contained in menu.html is below:</span></pre>\n<blockquote>\n<pre><span>&lt;div class=\"menu\"&gt;\n {{data}}\n&lt;/div&gt;</span></pre>\n</blockquote>\n<h3>\nCompilation</h3>\nAngular compiles the controller and the template in order to replace the html tag. This compilation is necessary to test the directive. <br><br><h3>\nPrerequisites</h3>\nThis article assumes you have some understanding of javascript, Angular, and unit testing. I’ll focus on how the test was setup and compiled the directive so that it could be tested. You need node and npm to install packages and run scripts. The other dependencies are listed in the package.json files. <br><h3>\n<strong></strong>Karma, Mocha, Chai, Phantom Test System</h3>\nSince Angular is a client-side framework, Karma acts as the web server and manages the web browser for html and javascript. Instead of running a real browser, I chose Phantom so everything can run from the command line. Mocha and chai are the test framework and assert library.<br><br>\nWhile the Angular code is browserified, that doesn’t have any impact on how the directive is tested. <br><br><h3>\nThe Source Code and Test</h3>\nWith very little difference, each of the 3 examples is just about the same: a very simple controller that has $scope.data, an html template that uses {{data}} from the scope, and a test that compiles the directive and validates that the {{data}} syntax was replaced with the $scope.data value. <br><br>\nEach project has the same directory layout:<br><br><table border=\"0\" cellpadding=\"2\" cellspacing=\"0\"><tbody>\n<tr>\n<td valign=\"top\" width=\"200\">/client</td>\n\n <td valign=\"top\" width=\"200\">angular files to be browserified into /public/app.js</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">/public</td>\n\n <td valign=\"top\" width=\"200\">index.html, app.js, html templates</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">/test</td>\n\n <td valign=\"top\" width=\"200\">mocha/chai test file</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">karma.conf.js</td>\n\n <td valign=\"top\" width=\"200\">karma configuration file</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">gulpfile.js</td>\n\n <td valign=\"top\" width=\"200\">browserify configuration</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">package.json</td>\n\n <td valign=\"top\" width=\"200\">list of dependencies and script to build and run test</td>\n </tr>\n</tbody></table>\n<br><h3>\nTesting the static template</h3>\n<strong></strong><br>\nThe first example tests a template using static html.  <br><br>\nThe controller:<br><blockquote>\n<pre><span>exports.userController = function ($scope) {\n $scope.data = \"user\";\n};</span></pre>\n</blockquote>\n<pre><span>The directive: </span></pre>\n<blockquote>\n<pre><span>exports.user = function() {\n return {\n controller: 'userController',\n template: '&lt;div class=\"user\"&gt;{{data}}&lt;/div&gt;'\n };\n};</span></pre>\n</blockquote>\n<pre><span>The calling html page</span></pre>\n<blockquote>\n<pre><span>&lt;html ng-app=\"app\"&gt;\n &lt;head&gt;\n &lt;script type=\"text/javascript\"\n src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular.js\"&gt;\n &lt;/script&gt;\n &lt;script type=\"text/javascript\"\n src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular-route.js\"&gt;\n &lt;/script&gt;\n &lt;script type=\"text/javascript\" src=\"/app.js\"&gt;&lt;/script&gt;\n &lt;/head&gt;\n &lt;body&gt;\n &lt;div&gt;\n <strong> &lt;user&gt;&lt;/user&gt;</strong>\n &lt;/div&gt; \n &lt;/body&gt;\n&lt;/html&gt;</span></pre>\n</blockquote>\n<span>The final html page will replace &lt;user&gt;&lt;/user&gt; with the compiled html from the template with the string “user” replacing {{data}} in the static template html.</span>\n<span>The mocha test</span>\n<blockquote>\n<pre><span><span>describe('userDirective', function() {\n var scope;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile) {\n </span><span><strong>element = angular.element('&lt;user&gt;&lt;/user&gt;');\n compiledElement = $compile(element)(scope);\n \n scope.$digest();</strong>\n\n var dataFromHtml = compiledElement.find('.user').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></span></pre>\n</blockquote>\n<span>As part of the test setup, in the beforeEach functions, the test brings in the angular app, brings in the mock library, and sets the scope. Inside the test (the ‘it’ function), the element function defines the directive’s html element, and compiles the scope and the element. The compiled html text value is tested against the scope value – which we expect to be the same. </span>\n<span>The overall test concept is the same for each of the examples. Get the controller and template, compile it, check it. The details differ in exactly how this is done in the 2 remaining examples. </span>\n<h3>\nThe 2 TemplateUrl Examples</h3>\n<span>The first example above relied on the directive definition to load the template html. The next 2 examples use the TemplateUrl property which has the html stored in a separate file so that method won’t work. Both of the next 2 examples use the templateCache to load the template, but each does it in a different way.</span>\n<span>The first example loads the template and tests the template as though the templateCache isn’t used. This is a good example for apps that don’t generally use the templateCache and developers that don’t want to change the app code in order to test the directive. The templateCache is only used as a convenience for testing. </span>\n<span>The second example alters the app code by loading the template in the templateCache and browserifying the app with the ‘templates’ dependency code. This is a good way to learn about the templateCache and test it.</span>\n<h3>\nTesting TemplateUrl – least intrusive method</h3>\n<span>The second example stores the html in a separate file instead of in the directive function. As a result, the test (and the karma config file) needs to bring the separate file in before compiling the element with the scope. </span>\n<span>The controller:</span>\n<blockquote>\n<pre><span>exports.menuController = function ($scope) {\n $scope.data = \"menu\";\n};</span></pre>\n</blockquote>\n<span>The directive:</span>\n<blockquote>\n<pre><span>exports.menu = function() {\n return {\n controller: 'menuController',\n templateUrl: '/templates/menu.html'\n };\n};</span></pre>\n</blockquote>\n<span>The template: </span>\n<blockquote>\n<pre><span>&lt;div class=\"menu\"&gt;\n {{data}}\n&lt;/div&gt;</span></pre>\n</blockquote>\n<span>The calling html page only different in that the html for the directive is </span>\n<blockquote>\n<pre><span>&lt;menu&gt;&lt;/menu&gt;</span></pre>\n</blockquote>\n<pre><span>karma-utils.js</span></pre>\n<blockquote>\n<pre><span>function httpGetSync(filePath) {\n var xhr = new XMLHttpRequest();\n \n var finalPath = filePath;\n \n //console.log(\"finalPath=\" + finalPath);\n \n <strong>xhr.open(\"GET\", finalPath, false);</strong>\n xhr.send();\n return xhr.responseText;\n}\n\nfunction preloadTemplate(path) {\n return inject(function ($templateCache) {\n var response = httpGetSync(path);\n //console.log(response);\n <strong> $templateCache.put(path, response);</strong>\n });\n}</span></pre>\n</blockquote>\n<span>The file along with the template file are brought in via the karma.conf.js file in the files property:</span>\n<blockquote>\n<span>// list of files / patterns to load in the browser \n <br>files: [ \n\n <br>    'http://code.jquery.com/jquery-1.11.3.js', \n\n <br>    'https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular.js', \n\n <br>    <br>    // For ngMockE2E \n\n <br>    'https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular-mocks.js', \n\n <br>    'test/karma-utils.js', \n\n <br>    'test/test.directive.*.js', \n\n <br>    'public/app.js', \n\n <br>    'public/templates/*.html' \n\n <br>],</span>\n</blockquote>\n<span>The mocha test</span>\n<blockquote>\n<pre><span><span>describe('menuDirective', function() {\n var mockScope;\n var compileService;\n var template;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(preloadTemplate('/templates/menu.html'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile) {\n</span><span><strong> element = angular.element('&lt;menu&gt;&lt;/menu&gt;');\n compiledElement = $compile(element)(scope);\n \n scope.$digest();</strong>\n\n var dataFromHtml = compiledElement.find('.menu').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></span></pre>\n</blockquote>\n<span></span><span>As part of the test setup, in the beforeEach functions, the test brings in the angular app, brings in the mock library, brings in the html template, and sets the scope. The template is brought in via a help function in another file in the test folder called preloadTemplate. </span><span>Inside the test (the ‘it’ function), the element function defines the directive’s html element, and compiles the scope and the element. The compiled html text value is tested against the scope value – which we expect to be the same.</span>\n<span>This is the line of the test file that deals with the templateCache:</span>\n<blockquote>\n<pre><span>beforeEach(preloadTemplate('/templates/menu.html'));</span> </pre>\n</blockquote>\n<span>The app and test code do not use the templateCache in any other way. The test itself is almost identical to the first test that uses the static html in the directive. </span>\n<h3>\nTesting TemplateUrl – most intrusive method</h3>\n<span>In this last example, the app.js code in the /client directory is altered to pull in a new ‘templates’ dependency module. The templates module is built in the gulpfile.js by grabbing all the html files in the /public/templates directory and wrapping it in javascript that adds the templates to the templateCache. The test explicitly pulls the template from the templateCache before compilation. </span>\n<span>gulpfile.js – to build templates dependency into /client/templates.js</span>\n<blockquote>\n<pre><span>var gulp = require('gulp');\nvar browserify = require('gulp-browserify');\n\nvar angularTemplateCache = require('gulp-angular-templatecache');\n\nvar concat = require('gulp-concat');\nvar addStream = require('add-stream');\n\ngulp.task('browserify', function() {\n return gulp.\n src('./client/app.js').\n //pipe(addStream('./client/templates.js')).\n //pipe(concat('app.js')).\n pipe(browserify()).\n pipe(gulp.dest('./public'));\n});\n\ngulp.task('templates', function () {\n return gulp.src('./public/templates/*.html')\n .pipe(angularTemplateCache({module:'templates', root: '/templates/'}))\n .pipe(gulp.dest('./client'));\n});\n\ngulp.task('build',['templates', 'browserify']);</span></pre>\n</blockquote>\n<span>templates.js – built by gulpfile.js</span><br><blockquote>\n<span>angular.module(\"templates\").run([\n <br>    \"$templateCache\",  function($templateCache) {\n\n <br>     $templateCache.put(\"/templates/system.html\",\"&lt;div class=\\\"menu\\\"&gt;\\n    {{data}}\\n&lt;/div&gt;\");\n\n <br>    }\n\n <br>]);</span>\n</blockquote>\n<span>app.js – defines template module and adds it to app list of dependencies</span>\n<blockquote>\n<span>var controllers = require('./controllers'); \n <br>var directives = require('./directives'); \n\n <br>var _ = require('underscore'); </span><br><span>// this never changes \n <br><strong>angular.module('templates', []);</strong> </span><br><span>// templates added as dependency \n <br>var components = angular.module('app', ['ng','templates']); </span><br><span>_.each(controllers, function(controller, name) \n <br>{ components.controller(name, controller); }); </span><br><span>_.each(directives, function(directive, name) \n <br>{ components.directive(name, directive); }); </span><br><strong><span>require('./templates')</span></strong>\n</blockquote>\nThe mocha test<br><blockquote>\n<pre><span>describe('menuDirective', function() {\n var mockScope;\n var compileService;\n var template;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile, $templateCache) {\n element = angular.element('&lt;system&gt;&lt;/system&gt;');\n compiledElement = $compile(element)(scope); \n\n // APP - app.js used templates dependency which loads template\n // into templateCache\n // TEST - this test pulls template from templateCache \n<strong> template = $templateCache.get('/templates/system.html');</strong> \n \n scope.$digest();\n\n var dataFromHtml = compiledElement.find('.menu').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></pre>\n</blockquote>\n<h3>\n<span>The T</span><span>est Results</span>\n</h3>\n<span>The test results for each of the 3 projects is checked in to the <a href=\"https://github.com/dfberry/AngularDirectiveKarma\">github</a> project. Karma ran with debug turned on so that the http and file requests could be validated. When you look at the testResult.log files (1 in each of the 3 subdirectories), you want to make sure that the http and file requests that karma made were actually successful. The lines with ‘Fetching’, ‘Requesting’, and ‘(cached)’ are the important lines before the test runs. </span><br><br><span>36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mFetching /home/dina/repos/AngularDirectiveKarma/directiveTemplate/test/test.directive.template.js </span><br><span>[36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mRequesting /base/public/app.js?6da99f7db89b4401f7fc5df6e04644e14cbed1f7 / </span><br><span>[36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mFetching /home/dina/repos/AngularDirectiveKarma/directiveTemplate/public/app.js </span><br><span>[36m26 04 2016 11:42:09.319:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/node_modules/mocha/mocha.js </span><br><span>[36m26 04 2016 11:42:09.329:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/node_modules/karma-mocha/lib/adapter.js [36m26 04 2016 11:42:09.331:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/test/test.directive.template.js </span><br><span>[36m26 04 2016 11:42:09.333:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/public/app.js</span><br><br>\nDuring the test, you should see that the $scope.data value (different in each of the 3 tests) is equated to the compiled html as expected, for example, “user = user”:<br><blockquote>\n<span>LOG: 'user == user'</span>\n</blockquote>\nAnd finally, that the test passed:<br><blockquote>\n<span>Executed 1 of 1 SUCCESS</span>\n</blockquote>\n",
            "content": "\n<h1>\n</h1>\n<h3>\nAll Code on Github</h3>\nThe entire code from this article is on <a href=\"https://github.com/dfberry/AngularDirectiveKarma\">github</a>. <br><h3>\nIntroduction</h3>\nAngular Directives are part of the web part/components group of client-side  tools that allow quick additions to web pages. The idea is that with a very simple and short addition to an html page, complex functionality and UX are available. <br><br>\nImagine a user login form with the traditional validation contained in a html template and Angular controller. The main, calling web page’s html could just include <strong>&lt;login&gt;&lt;/login&gt;</strong> to bring in that rich validation and display. Directives are wonderful for encapsulating the complexity away from the containing HTML.<br><br>\nTesting the directive is trickier. Granted the controller isn’t difficult to test. But the compiled html and scope are not so easy. <br><br>\nPerhaps it is enough to test the controller, and depend on the browser and the Angular library to manage the rest.  You could definitely make that case. Or perhaps you leave the compiled directive testing to the end to end (e2e) tests. That is also a fair. If either of these solutions doesn’t work for you, this article will explain 3 ways to test the compiled Angular directive. <br><br><strong>Template </strong><br>\nWhen you build the directive, you can choose static (hopefully short) html in the definition of the directive.  Notice that {{data}} will be replaced with the $scope.data value from the directive.<br><blockquote>\n<span>exports.user = function() { <br>  return { <br>    controller: 'userController', <br>    template: '&lt;div class=\"user\"&gt;{{data}}&lt;/div&gt;' <br>    }; <br>};</span>\n</blockquote>\n<strong>TemplateUrl</strong><br>\nIf you have more html or want to separate the html from the Angular directive, templateUrl is the way to go. <br><blockquote>\n<pre><span>exports.menu = function() {\n return {\n controller: 'menuController',\n templateUrl: '/templates/menu.html'\n };\n};</span></pre>\n</blockquote>\n<pre><span>The html contained in menu.html is below:</span></pre>\n<blockquote>\n<pre><span>&lt;div class=\"menu\"&gt;\n {{data}}\n&lt;/div&gt;</span></pre>\n</blockquote>\n<h3>\nCompilation</h3>\nAngular compiles the controller and the template in order to replace the html tag. This compilation is necessary to test the directive. <br><br><h3>\nPrerequisites</h3>\nThis article assumes you have some understanding of javascript, Angular, and unit testing. I’ll focus on how the test was setup and compiled the directive so that it could be tested. You need node and npm to install packages and run scripts. The other dependencies are listed in the package.json files. <br><h3>\n<strong></strong>Karma, Mocha, Chai, Phantom Test System</h3>\nSince Angular is a client-side framework, Karma acts as the web server and manages the web browser for html and javascript. Instead of running a real browser, I chose Phantom so everything can run from the command line. Mocha and chai are the test framework and assert library.<br><br>\nWhile the Angular code is browserified, that doesn’t have any impact on how the directive is tested. <br><br><h3>\nThe Source Code and Test</h3>\nWith very little difference, each of the 3 examples is just about the same: a very simple controller that has $scope.data, an html template that uses {{data}} from the scope, and a test that compiles the directive and validates that the {{data}} syntax was replaced with the $scope.data value. <br><br>\nEach project has the same directory layout:<br><br><table border=\"0\" cellpadding=\"2\" cellspacing=\"0\"><tbody>\n<tr>\n<td valign=\"top\" width=\"200\">/client</td>\n\n <td valign=\"top\" width=\"200\">angular files to be browserified into /public/app.js</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">/public</td>\n\n <td valign=\"top\" width=\"200\">index.html, app.js, html templates</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">/test</td>\n\n <td valign=\"top\" width=\"200\">mocha/chai test file</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">karma.conf.js</td>\n\n <td valign=\"top\" width=\"200\">karma configuration file</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">gulpfile.js</td>\n\n <td valign=\"top\" width=\"200\">browserify configuration</td>\n </tr>\n<tr>\n<td valign=\"top\" width=\"200\">package.json</td>\n\n <td valign=\"top\" width=\"200\">list of dependencies and script to build and run test</td>\n </tr>\n</tbody></table>\n<br><h3>\nTesting the static template</h3>\n<strong></strong><br>\nThe first example tests a template using static html.  <br><br>\nThe controller:<br><blockquote>\n<pre><span>exports.userController = function ($scope) {\n $scope.data = \"user\";\n};</span></pre>\n</blockquote>\n<pre><span>The directive: </span></pre>\n<blockquote>\n<pre><span>exports.user = function() {\n return {\n controller: 'userController',\n template: '&lt;div class=\"user\"&gt;{{data}}&lt;/div&gt;'\n };\n};</span></pre>\n</blockquote>\n<pre><span>The calling html page</span></pre>\n<blockquote>\n<pre><span>&lt;html ng-app=\"app\"&gt;\n &lt;head&gt;\n &lt;script type=\"text/javascript\"\n src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular.js\"&gt;\n &lt;/script&gt;\n &lt;script type=\"text/javascript\"\n src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular-route.js\"&gt;\n &lt;/script&gt;\n &lt;script type=\"text/javascript\" src=\"/app.js\"&gt;&lt;/script&gt;\n &lt;/head&gt;\n &lt;body&gt;\n &lt;div&gt;\n <strong> &lt;user&gt;&lt;/user&gt;</strong>\n &lt;/div&gt; \n &lt;/body&gt;\n&lt;/html&gt;</span></pre>\n</blockquote>\n<span>The final html page will replace &lt;user&gt;&lt;/user&gt; with the compiled html from the template with the string “user” replacing {{data}} in the static template html.</span>\n<span>The mocha test</span>\n<blockquote>\n<pre><span><span>describe('userDirective', function() {\n var scope;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile) {\n </span><span><strong>element = angular.element('&lt;user&gt;&lt;/user&gt;');\n compiledElement = $compile(element)(scope);\n \n scope.$digest();</strong>\n\n var dataFromHtml = compiledElement.find('.user').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></span></pre>\n</blockquote>\n<span>As part of the test setup, in the beforeEach functions, the test brings in the angular app, brings in the mock library, and sets the scope. Inside the test (the ‘it’ function), the element function defines the directive’s html element, and compiles the scope and the element. The compiled html text value is tested against the scope value – which we expect to be the same. </span>\n<span>The overall test concept is the same for each of the examples. Get the controller and template, compile it, check it. The details differ in exactly how this is done in the 2 remaining examples. </span>\n<h3>\nThe 2 TemplateUrl Examples</h3>\n<span>The first example above relied on the directive definition to load the template html. The next 2 examples use the TemplateUrl property which has the html stored in a separate file so that method won’t work. Both of the next 2 examples use the templateCache to load the template, but each does it in a different way.</span>\n<span>The first example loads the template and tests the template as though the templateCache isn’t used. This is a good example for apps that don’t generally use the templateCache and developers that don’t want to change the app code in order to test the directive. The templateCache is only used as a convenience for testing. </span>\n<span>The second example alters the app code by loading the template in the templateCache and browserifying the app with the ‘templates’ dependency code. This is a good way to learn about the templateCache and test it.</span>\n<h3>\nTesting TemplateUrl – least intrusive method</h3>\n<span>The second example stores the html in a separate file instead of in the directive function. As a result, the test (and the karma config file) needs to bring the separate file in before compiling the element with the scope. </span>\n<span>The controller:</span>\n<blockquote>\n<pre><span>exports.menuController = function ($scope) {\n $scope.data = \"menu\";\n};</span></pre>\n</blockquote>\n<span>The directive:</span>\n<blockquote>\n<pre><span>exports.menu = function() {\n return {\n controller: 'menuController',\n templateUrl: '/templates/menu.html'\n };\n};</span></pre>\n</blockquote>\n<span>The template: </span>\n<blockquote>\n<pre><span>&lt;div class=\"menu\"&gt;\n {{data}}\n&lt;/div&gt;</span></pre>\n</blockquote>\n<span>The calling html page only different in that the html for the directive is </span>\n<blockquote>\n<pre><span>&lt;menu&gt;&lt;/menu&gt;</span></pre>\n</blockquote>\n<pre><span>karma-utils.js</span></pre>\n<blockquote>\n<pre><span>function httpGetSync(filePath) {\n var xhr = new XMLHttpRequest();\n \n var finalPath = filePath;\n \n //console.log(\"finalPath=\" + finalPath);\n \n <strong>xhr.open(\"GET\", finalPath, false);</strong>\n xhr.send();\n return xhr.responseText;\n}\n\nfunction preloadTemplate(path) {\n return inject(function ($templateCache) {\n var response = httpGetSync(path);\n //console.log(response);\n <strong> $templateCache.put(path, response);</strong>\n });\n}</span></pre>\n</blockquote>\n<span>The file along with the template file are brought in via the karma.conf.js file in the files property:</span>\n<blockquote>\n<span>// list of files / patterns to load in the browser \n <br>files: [ \n\n <br>    'http://code.jquery.com/jquery-1.11.3.js', \n\n <br>    'https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular.js', \n\n <br>    <br>    // For ngMockE2E \n\n <br>    'https://ajax.googleapis.com/ajax/libs/angularjs/1.4.0/angular-mocks.js', \n\n <br>    'test/karma-utils.js', \n\n <br>    'test/test.directive.*.js', \n\n <br>    'public/app.js', \n\n <br>    'public/templates/*.html' \n\n <br>],</span>\n</blockquote>\n<span>The mocha test</span>\n<blockquote>\n<pre><span><span>describe('menuDirective', function() {\n var mockScope;\n var compileService;\n var template;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(preloadTemplate('/templates/menu.html'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile) {\n</span><span><strong> element = angular.element('&lt;menu&gt;&lt;/menu&gt;');\n compiledElement = $compile(element)(scope);\n \n scope.$digest();</strong>\n\n var dataFromHtml = compiledElement.find('.menu').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></span></pre>\n</blockquote>\n<span></span><span>As part of the test setup, in the beforeEach functions, the test brings in the angular app, brings in the mock library, brings in the html template, and sets the scope. The template is brought in via a help function in another file in the test folder called preloadTemplate. </span><span>Inside the test (the ‘it’ function), the element function defines the directive’s html element, and compiles the scope and the element. The compiled html text value is tested against the scope value – which we expect to be the same.</span>\n<span>This is the line of the test file that deals with the templateCache:</span>\n<blockquote>\n<pre><span>beforeEach(preloadTemplate('/templates/menu.html'));</span> </pre>\n</blockquote>\n<span>The app and test code do not use the templateCache in any other way. The test itself is almost identical to the first test that uses the static html in the directive. </span>\n<h3>\nTesting TemplateUrl – most intrusive method</h3>\n<span>In this last example, the app.js code in the /client directory is altered to pull in a new ‘templates’ dependency module. The templates module is built in the gulpfile.js by grabbing all the html files in the /public/templates directory and wrapping it in javascript that adds the templates to the templateCache. The test explicitly pulls the template from the templateCache before compilation. </span>\n<span>gulpfile.js – to build templates dependency into /client/templates.js</span>\n<blockquote>\n<pre><span>var gulp = require('gulp');\nvar browserify = require('gulp-browserify');\n\nvar angularTemplateCache = require('gulp-angular-templatecache');\n\nvar concat = require('gulp-concat');\nvar addStream = require('add-stream');\n\ngulp.task('browserify', function() {\n return gulp.\n src('./client/app.js').\n //pipe(addStream('./client/templates.js')).\n //pipe(concat('app.js')).\n pipe(browserify()).\n pipe(gulp.dest('./public'));\n});\n\ngulp.task('templates', function () {\n return gulp.src('./public/templates/*.html')\n .pipe(angularTemplateCache({module:'templates', root: '/templates/'}))\n .pipe(gulp.dest('./client'));\n});\n\ngulp.task('build',['templates', 'browserify']);</span></pre>\n</blockquote>\n<span>templates.js – built by gulpfile.js</span><br><blockquote>\n<span>angular.module(\"templates\").run([\n <br>    \"$templateCache\",  function($templateCache) {\n\n <br>     $templateCache.put(\"/templates/system.html\",\"&lt;div class=\\\"menu\\\"&gt;\\n    {{data}}\\n&lt;/div&gt;\");\n\n <br>    }\n\n <br>]);</span>\n</blockquote>\n<span>app.js – defines template module and adds it to app list of dependencies</span>\n<blockquote>\n<span>var controllers = require('./controllers'); \n <br>var directives = require('./directives'); \n\n <br>var _ = require('underscore'); </span><br><span>// this never changes \n <br><strong>angular.module('templates', []);</strong> </span><br><span>// templates added as dependency \n <br>var components = angular.module('app', ['ng','templates']); </span><br><span>_.each(controllers, function(controller, name) \n <br>{ components.controller(name, controller); }); </span><br><span>_.each(directives, function(directive, name) \n <br>{ components.directive(name, directive); }); </span><br><strong><span>require('./templates')</span></strong>\n</blockquote>\nThe mocha test<br><blockquote>\n<pre><span>describe('menuDirective', function() {\n var mockScope;\n var compileService;\n var template;\n\n beforeEach(angular.mock.module(\"app\"));\n \n beforeEach(angular.mock.module('ngMockE2E'));\n\n beforeEach(inject(function ($rootScope) {\n scope = $rootScope.$new();\n }));\n \n it('should exist', inject(function ($compile, $templateCache) {\n element = angular.element('&lt;system&gt;&lt;/system&gt;');\n compiledElement = $compile(element)(scope); \n\n // APP - app.js used templates dependency which loads template\n // into templateCache\n // TEST - this test pulls template from templateCache \n<strong> template = $templateCache.get('/templates/system.html');</strong> \n \n scope.$digest();\n\n var dataFromHtml = compiledElement.find('.menu').text().trim();\n var dataFromScope = scope.data;\n \n console.log(dataFromHtml + \" == \" + dataFromScope);\n\n expect(dataFromHtml).to.equal(dataFromScope);\n }));\n});</span></pre>\n</blockquote>\n<h3>\n<span>The T</span><span>est Results</span>\n</h3>\n<span>The test results for each of the 3 projects is checked in to the <a href=\"https://github.com/dfberry/AngularDirectiveKarma\">github</a> project. Karma ran with debug turned on so that the http and file requests could be validated. When you look at the testResult.log files (1 in each of the 3 subdirectories), you want to make sure that the http and file requests that karma made were actually successful. The lines with ‘Fetching’, ‘Requesting’, and ‘(cached)’ are the important lines before the test runs. </span><br><br><span>36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mFetching /home/dina/repos/AngularDirectiveKarma/directiveTemplate/test/test.directive.template.js </span><br><span>[36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mRequesting /base/public/app.js?6da99f7db89b4401f7fc5df6e04644e14cbed1f7 / </span><br><span>[36m26 04 2016 11:42:09.318:DEBUG [middleware:source-files]: [39mFetching /home/dina/repos/AngularDirectiveKarma/directiveTemplate/public/app.js </span><br><span>[36m26 04 2016 11:42:09.319:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/node_modules/mocha/mocha.js </span><br><span>[36m26 04 2016 11:42:09.329:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/node_modules/karma-mocha/lib/adapter.js [36m26 04 2016 11:42:09.331:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/test/test.directive.template.js </span><br><span>[36m26 04 2016 11:42:09.333:DEBUG [web-server]: [39mserving (cached): /home/dina/repos/AngularDirectiveKarma/directiveTemplate/public/app.js</span><br><br>\nDuring the test, you should see that the $scope.data value (different in each of the 3 tests) is equated to the compiled html as expected, for example, “user = user”:<br><blockquote>\n<span>LOG: 'user == user'</span>\n</blockquote>\nAnd finally, that the test passed:<br><blockquote>\n<span>Executed 1 of 1 SUCCESS</span>\n</blockquote>\n",
            "enclosure": [],
            "categories": [
                "angular",
                "karma",
                "Dina",
                "Node",
                "directive",
                "Mocha",
                "PhantomJS",
                "unit-test"
            ]
        },
        {
            "title": "Capturing a Stripe Credit Card Charge",
            "pubDate": "2016-03-04 18:50:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/03/capturing-stripe-credit-card-charge_7.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-7841066101395387910",
            "author": "Dina Berry",
            "thumbnail": "https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s72-c/stripe_popup.jpg",
            "description": "In this article, I'll show you the JSON objects and Angular/Node code to capture a credit card with the <a href=\"https://stripe.com/\">Stripe service</a>. The code for this example project is on <a href=\"https://github.com/dfberry/stripe-angular-express\">GitHub</a>. It is a working project so the code may not be exactly like this article by the time you find it. <br><br><b>Introduction</b> <br>\nCapturing credit card information for products, services, and subscriptions is easy with many tools provided by credit card processing companies. Stripe provides an <a href=\"https://stripe.com/docs/checkout/tutorial\">html button that pops up a form to collect credit card information</a>. It is incredibly simple and straightforward.<br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s1600/stripe_popup.jpg\" imageanchor=\"1\"><img border=\"0\" height=\"312\" src=\"https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s400/stripe_popup.jpg\" width=\"400\"></a>\n</div>\n<br><br>\nYou can control the form to collect more data. You don't need to know how to program it, and it works. Yeah! This article doesn't cover the easy route of button and pop up because Stripe did a great job of that <a href=\"https://stripe.com/docs/checkout/tutorial\">on their website</a>. <br><br>\nIf you would prefer to control the credit card experience from beginning to end, you may choose to build your own form and process the data you collect to meet your own business needs. If that is the case read on.<br><br><strong>The Credit Card Token/Charge Experience</strong><br>\nA credit card number should never make it to your server – where you are liable for fraud or theft. Instead, the client-side code sends the credit card information to the processor, in this case Stripe, and Stripe sends back a token to the client-side code.<br><br>\nThe token isn't a charge to the card but more a promise to charge the card in the future. For the remainder of the customer's experience, you do not use the credit card information, only the token. The token travels with the rest of the customer information (name, address, items purchased) to the server. The server will make the actual charge and receive the resulting information from Stripe.<br><br>\nWith stripe, all you really need is a way to make an https request to their Api server, passing JSON, then receive the JSON request. That is true for both the token request and the charge request. Stripe responses with JSON.<br><br><strong>JSON is independent of the Technology</strong><br>\nThe rest of this article is broken into two different sections. The first section will review the JSON objects which have some requirements and a lot of optional fields. You can use any technology you want to make these, including curl. The second section will cover a specific code base of Angular on the client and Node/Express on the server. The code is very simple so there is little or no styling, validation, or error management.<br><br><strong>The Stripe Key</strong><br>\nStripe needs your Publishable key to create the token. You can find this on the Stripe dashboard in your account settings.<br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-3xQR2HhxtxM/Vtn09R3GlMI/AAAAAAAAAj8/PmxeqN6pi0o/s1600/Snip20160304_4.jpg\" imageanchor=\"1\"><img border=\"0\" height=\"457\" src=\"https://2.bp.blogspot.com/-3xQR2HhxtxM/Vtn09R3GlMI/AAAAAAAAAj8/PmxeqN6pi0o/s640/Snip20160304_4.jpg\" width=\"640\"></a>\n</div>\n<br><br>\nIf you are using curl, you can pass the key along with the token request. If you are passing it from code, it will need to be set before the token request. The curl example is:<br><div>\n<blockquote>\n<pre><code>\ncurl https://api.stripe.com/v1/tokens \\  -u sk_test_WqKypPUzUwdHSgZLT3zWZmBq: \\  -d card[number]=4242424242424242 \\  -d card[exp_month]=12 \\  -d card[exp_year]=2017 \\  -d card[cvc]=123\n</code></pre>\n</blockquote>\n</div>\n<br>\nThe key is the value after the –u param: <b>sk_test_…</b><br><br>\nNotice that only the key and card information is passed in the above curl. You can and should capture and pass the billing address. This allows you to see the address in the Stripe Dashboard when the card is finally made.<br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-tQ7sUHrGwp8/Vtn18CJIfwI/AAAAAAAAAkI/2OLdPU3ov5Q/s1600/Snip20160304_5.png\" imageanchor=\"1\"><img border=\"0\" height=\"228\" src=\"https://3.bp.blogspot.com/-tQ7sUHrGwp8/Vtn18CJIfwI/AAAAAAAAAkI/2OLdPU3ov5Q/s640/Snip20160304_5.png\" width=\"640\"></a>\n</div>\n<br><br>\nNotice that you only see the last 4 digits of the credit card. <br><br><strong>JSON to create a Stripe Token</strong><br>\nThe JSON object to request a token contains the credit card information. It should also contain the billing information for the customer.  A full list of card key/value pairs is listed in the <a href=\"https://stripe.com/docs/api/java#card_object\">Stripe docs</a>.<br><div>\n<blockquote>\n<pre><code>client stripe token request = \n{ \n \"number\":\"4242424242424242\", \n \"cvc\":\"123\", \n \"exp_month\":\"11\", \n \"exp_year\":\"2016\", \n \"name\":\"Barbara Jones\", \n \"address_city\":\"Seattle\", \n \"address_line1\":\"5678 Nine Street\", \n \"address_line2\":\"Box 3\", \n \"address_country\":\"USA\", \n \"address_state\":\"WA\", \n \"address_zip\":\"98105\" \n}</code></pre>\n</blockquote>\n</div>\n<br>\nThe response will be a status code, success is 200 with a json object of data including the token.<br><br><div>\n<blockquote>\n<pre><code>client stripe token response.response = { \n \"id\":\"tok_17jylQJklCPSOV9aLkiy5879\", \n \"object\":\"token\", \n \"card\":{ \n \"id\":\"card_17jylQJklCPSOV9ahtXhPVB8\", \n \"object\":\"card\", \n \"address_city\":\"Seattle\", \n \"address_country\":\"USA\", \n \"address_line1\":\"5678 Nine Street\", \n \"address_line1_check\":\"unchecked\", \n \"address_line2\":\"Box 3\", \n \"address_state\":\"WA\", \n \"address_zip\":\"98105\", \n \"address_zip_check\":\"unchecked\", \n \"brand\":\"Visa\", \n \"country\":\"US\", \n \"cvc_check\":\"unchecked\", \n \"dynamic_last4\":null, \n \"exp_month\":11, \n \"exp_year\":2016, \n \"funding\":\"credit\", \n \"last4\":\"4242\", \n \"metadata\":{ \n }, \n \"name\":\"Barbara Jones\", \n \"tokenization_method\":null \n }, \n \"client_ip\":\"73.11.000.147\", \n \"created\":1456782072, \n \"livemode\":false, \n \"type\":\"card\", \n \"used\":false \n}</code></pre>\n</blockquote>\n</div>\n<br>\nFor the rest of the credit card transaction, use the token only. You will need to pass it when you charge the customer's credit card. <br><br><strong>JSON for a successful Stripe Charge</strong><br>\nNow that you have the token, you can create a JSON object to represent the credit card charge.<br><br>\nclient stripe token response.status = 200 \n <br><div>\n<pre><code>server stripe charge request object = {    \n   \"amount\":1000,  \n   \"currency\":\"usd\",  \n   \"source\":\"tok_17jylQJklCPSOV9aLkiy5879\",  \n   \"description\":\"Donation for XYZ\",  \n   \"metadata\":{    \n      \"ShipTo\":\"Bob Smith\",  \n      \"BillTo\":\"Barbara Jones\"  \n   },  \n   \"receipt_email\":\"bob@company.com\",  \n   \"statement_descriptor\":\"MyStripeStore\",  \n   \"shipping\":{    \n      \"address\":{    \n         \"city\":\"Seattle\",  \n         \"country\":\"USA\",  \n         \"line1\":\"1234 Five Lane\",  \n         \"line2\":\"Floor 2\",  \n         \"postal_code\":\"98101\",  \n         \"state\":\"WA\"  \n      },  \n      \"name\":\"Bob Smith\",  \n      \"phone\":\"\"  \n   }  \n} </code></pre>\n</div>\n<br>\nMake sure the statement_descriptor has meaningful information to figure out the charge was valid– it shows up on the customer's bill. If you have information important to the transaction that Stripe doesn't collect, put those values in the metadata key. You can retrieve this information from Stripe to reconcile or fulfill the transaction on your end. Think of it as a backup – if your system goes down, Stripe still has enough information for you to rebuild the transaction.  The amount includes dollars and cents but no decimal. So \"1000\" is ten dollars, $10.00. <br><br>\nIf you are new to Stripe, you may not be using their advanced features but if you collect the data now, converting to and seeding some of the advanced feature objects will be easy.<br><br>\nA successful charge response returns an null error object and a result object. <br><br><div>\n<pre><code>server stripe charge response.charge = {    \n   \"id\":\"ch_17jylQJklCPSOV9aHyof0XV8\",  \n   \"object\":\"charge\",  \n   \"amount\":1000,  \n   \"amount_refunded\":0,  \n   \"application_fee\":null,  \n   \"balance_transaction\":\"txn_17jylQJklCPSOV9afi1bpfz5\",  \n   \"captured\":true,  \n   \"created\":1456782072,  \n   \"currency\":\"usd\",  \n   \"customer\":null,  \n   \"description\":\"Donation for XYZ\",  \n   \"destination\":null,  \n   \"dispute\":null,  \n   \"failure_code\":null,  \n   \"failure_message\":null,  \n   \"fraud_details\":{    \n   },  \n   \"invoice\":null,  \n   \"livemode\":false,  \n   \"metadata\":{    \n      \"ShipTo\":\"Bob Smith\",  \n      \"BillTo\":\"Barbara Jones\"  \n   },  \n   \"order\":null,  \n   \"paid\":true,  \n   \"receipt_email\":\"bob@company.com\",  \n   \"receipt_number\":null,  \n   \"refunded\":false,  \n   \"refunds\":{    \n      \"object\":\"list\",  \n      \"data\":[    \n      ],  \n      \"has_more\":false,  \n      \"total_count\":0,  \n      \"url\":\"/v1/charges/ch_17jylQJklCPSOV9aHyof0XV8/refunds\"  \n   },  \n   \"shipping\":{    \n      \"address\":{    \n         \"city\":\"Seattle\",  \n         \"country\":\"USA\",  \n         \"line1\":\"1234 Five Lane\",  \n         \"line2\":\"Floor 2\",  \n         \"postal_code\":\"98101\",  \n         \"state\":\"WA\"  \n      },  \n      \"carrier\":null,  \n      \"name\":\"Bob Smith\",  \n      \"phone\":\"\",  \n      \"tracking_number\":null  \n   },  \n   \"source\":{    \n      \"id\":\"card_17jylQJklCPSOV9ahtXhPVB8\",  \n      \"object\":\"card\",  \n      \"address_city\":\"Seattle\",  \n      \"address_country\":\"USA\",  \n      \"address_line1\":\"5678 Nine Street\",  \n      \"address_line1_check\":\"pass\",  \n      \"address_line2\":\"Box 3\",  \n      \"address_state\":\"WA\",  \n      \"address_zip\":\"98105\",  \n      \"address_zip_check\":\"pass\",  \n      \"brand\":\"Visa\",  \n      \"country\":\"US\",  \n      \"customer\":null,  \n      \"cvc_check\":\"pass\",  \n      \"dynamic_last4\":null,  \n      \"exp_month\":11,  \n      \"exp_year\":2016,  \n      \"fingerprint\":\"uOlT1SgxEykd9grd\",  \n      \"funding\":\"credit\",  \n      \"last4\":\"4242\",  \n      \"metadata\":{    \n      },  \n      \"name\":\"Barbara Jones\",  \n      \"tokenization_method\":null  \n   },  \n   \"source_transfer\":null,  \n   \"statement_descriptor\":\"MyStripeStore\",  \n   \"status\":\"succeeded\"  \n} </code></pre>\n</div>\n<br>\nThe two items you want to look for is status and paid. <br><br>\nStripe keeps a log of your transactions as JSON objects including the request and response of both the token and the charge. You may want to store all the information on your server, but if you don't, you can get it from Stripe when you need to. <br><br><strong>JSON for a failed Stripe Charge</strong><br>\nIf you customer's charge fails, the JSON for the charge is a bit different. You will want to present the customer with the meaningful error message to help them correct the problem on their end and complete the transaction successfully.<br><br>\nThe charge object will be null and the status object will have the error message.<br><br><div>\n<pre><code>server stripe charge response.status = {    \n   \"type\":\"StripeCardError\",  \n   \"stack\":\"Error: Your card was declined.\\n    at Error._Error (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/Error.js:12:17)\\n    at Error.Constructor (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/utils.js:105:13)\\n    at Error.Constructor (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/utils.js:105:13)\\n    at Function.StripeError.generate (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/Error.js:54:14)\\n    at IncomingMessage.&lt;anonymous&gt; (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/StripeResource.js:138:39)\\n    at emitNone (events.js:72:20)\\n    at IncomingMessage.emit (events.js:166:7)\\n    at endReadableNT (_stream_readable.js:905:12)\\n    at nextTickCallbackWith2Args (node.js:455:9)\\n    at process._tickCallback (node.js:369:17)\",  \n   \"rawType\":\"card_error\",  \n   \"code\":\"card_declined\",  \n   \"message\":\"Your card was declined.\",  \n   \"raw\":{    \n      \"message\":\"Your card was declined.\",  \n      \"type\":\"card_error\",  \n      \"code\":\"card_declined\",  \n      \"charge\":\"ch_17jz4rJklCPSOV9aDPh2eooP\",  \n      \"statusCode\":402,  \n      \"requestId\":\"req_7zz28B9jlOpH1x\"  \n   },  \n   \"requestId\":\"req_7zz28B9jlOpH1x\",  \n   \"statusCode\":402  \n} </code></pre>\n</div>\n<br>\nThe message contains the information to display to the customer. While the transaction didn't complete, the log in the Stripe Dashboard will contain the same information. For a complete list of issues, look at the <a href=\"https://stripe.com/docs/api#errors\">Stripe </a><a href=\"https://stripe.com/docs/api#errors\">api</a><a href=\"https://stripe.com/docs/api#errors\"> for errors</a>.<br><br><strong>The Angular and Node/Express application</strong><br>\nIn order to use Stripe on the client, you need to pull in the Stripe javascript library.<br><br>\nYou can get the Stripe library for the client from stripe<br><br><div>\n<blockquote>\n<pre><code>&lt;script type=\"text/javascript\" src=\"https://js.stripe.com/v2/\"&gt;&lt;/script&gt;</code></pre>\n</blockquote>\n</div>\n<br>\nIn order to use Stripe on the server, you need to install Stripe from NPM<br><br><div>\n<blockquote>\n<pre><code>npm install stripe –save-dev</code></pre>\n</blockquote>\n</div>\n<br><strong>The Donation Form</strong><br>\nThe example is a donation form. It collects the customer information and allows the customer to choose the donation amount. A shipping address is also collected, for a thank you card back to the customer.<br><br><div class=\"separator\">\n<a href=\"https://4.bp.blogspot.com/-GvNk19u1eQM/Vtn5nIKpK3I/AAAAAAAAAkk/0UXycIfyySE/s1600/Snip20160229_4.png\" imageanchor=\"1\"><img border=\"0\" height=\"640\" src=\"https://4.bp.blogspot.com/-GvNk19u1eQM/Vtn5nIKpK3I/AAAAAAAAAkk/0UXycIfyySE/s640/Snip20160229_4.png\" width=\"342\"></a>\n</div>\n<br><br>\nThe web page has very little styling, no validation, and only a single error message if the credit card charge is denied.<br><br><strong>Angular Client Code</strong><br>\nThe Angular code includes a directive to display the html, a controller to collect the customer's information, and a service to post to the server. The token creation and charge are both handled in the service one call right after the other. This is definitely not ideal for a real-world situation. <br><br>\nThe card, card, and customer information are kept in JSON objects in the models.js file. The Stripe publishable key is in config.js along with the cart item name. When the token and charge are made, the JSON object that Stripe expects for each of these is created. <br><br>\nAs all of the Stripe work is in the service, that is the Angular code to review. The complete charge as 3 separate objects, cart, card, and customer.<br><br><div>\n<pre><code>exports.$myservice = function($http,$myappconfig){  \n    var commit = function (completeCharge, callback){  \n        var result = {};  \n        // my stripe test key  \n        Stripe.setPublishableKey($myappconfig.stripePublishableKey);  \n        // credit card info passed   \n        // billing address passed as part of charge.card  \n        Stripe.card.createToken(completeCharge.card, function(status, response) {  \n             if (status.error) {  \n                console.log(\"stripe token not created\");  \n                result.error = status.error;  \n                callback(result);  \n            }   \n            var chargeRequestObject = {   \n                    stripeToken: response.id,   \n                    cart: completeCharge.cart ,   \n                    customer: completeCharge.customer  \n            };  \n            // token (not credit card) passed  \n            // shipping address passed in charge.customer  \n            $http.  \n                post('/api/v1/checkout', chargeRequestObject).  \n                then(function(data) { //success  \n                    callback(null, data);  \n                },  \n                function(response){ //failure  \n                    callback(response.data.error, response);  \n                });  \n            });  \n        }  \n    return {  \n      commit: commit  \n    };      \n}</code></pre>\n</div>\n<br><strong>Node Server Code</strong><br>\nThe server code is a node app using Express. It only serves the html page above and has an api to process a charge -- very lean in order to drop it into any other web site.  Morgan is used for logging and Wagner is used for dependency injection.  You can start the app with npm start or node server/index.js.<br><br><b>Index.js</b>\n\n<br><div>\n<pre><code>var express = require('express');  \nvar wagner = require('wagner-core');  \nvar path = require('path');  \nrequire('./dependencies')(wagner);  \nvar app = express();  \napp.use(require('morgan')());  \napp.get(['/'], function (req, res) {res.sendFile(path.join(__dirname + '/../public/default.html'));});  \napp.use('/api/v1', require('./api')(wagner));  \napp.use('/public', express.static(__dirname + '/../public', { maxAge: 4 * 60 * 60 * 1000 /* 2hrs */}));  \napp.listen(3000);  \nconsole.log('Listening on port 3000!'); </code></pre>\n</div>\n<br>\nThe dependencies file creates the wagner dependency injection for the config and stripe objects.<br><br><strong>Dependencies.js</strong><br><div>\n<pre><code>var fs = require('fs');  \nvar Stripe = require('stripe');  \nvar configFile = require('./config.json');  \nmodule.exports = function(wagner) {  \n  wagner.factory('Stripe', function(Config) {  \n    return Stripe(Config.stripeKey);  \n  });  \n  wagner.factory('Config', function() {  \n    return configFile;  \n  });  \n};</code></pre>\n</div>\n<br>\nThe<strong> config.json</strong> is simple the configuration json object: \n\n <br><div>\n<pre><code>\n{  \n  \"stripeKey\": \"sk_test_WqKypPUzUXXXXXXXT3zWZmBq\",  \n  \"stripePublishableClientKey\": \"pk_test_ArJPMXXXXlF2Ml4m4e8ILmiP\",  \n  \"Characters22_StoreName\": \"MyStripeStore\"  \n}</code></pre>\n</div>\n<br>\nThe main Stripe code of the application is in the<strong> api.js</strong> file to process the Stripe charge. <br><div>\n<pre><code>module.exports = function(wagner) {  \n  var api = express.Router();  \n  api.use(bodyparser.json());  \n  /* Stripe Checkout API */  \n  api.post('/checkout', wagner.invoke(function(Stripe) {  \n    return function(req, res) {  \n        // https://stripe.com/docs/api#capture_charge  \n        // shipping name is in the metadata so that it is easily found on stripe's website   \n        // statement_descriptor &amp; description will show on credit card bill  \n        // receipt_email is sent but stripe isn't sending receipt - you still have to do that  \n        // shipping is sent only so you can pull information from stripe   \n        // metadata: 20 keys, with key names up to 40 characters long and values up to 500 characters long  \n        var stripeCharge = {  \n            amount: req.body.cart.totalprice,  \n            currency: 'usd',  \n            source: req.body.stripeToken,  \n            description: req.body.cart.name,  \n            metadata: {'ShipTo': req.body.customer.shipping.name, 'BillTo': req.body.customer.billing.name},  \n            receipt_email: req.body.customer.email,  \n            statement_descriptor: config.Characters22_StoreName,  \n            shipping: req.body.customer.shipping   \n        };  \n        console.log(\"server stripe charge request object = \" + JSON.stringify(stripeCharge)+ \"\\n\");  \n        // Charge the card NOW  \n        Stripe.charges.create(stripeCharge,function(err, charge) {  \n            console.log(\"server stripe charge response.err = \" + JSON.stringify(err) + \"\\n\");        \n            console.log(\"server stripe charge response.charge = \" + JSON.stringify(charge) + \"\\n\");   \n            if (err) {  \n                return res.  \n                status(status.INTERNAL_SERVER_ERROR).  \n                json({ error: err.toString(), charge: err.raw.charge, request: err.requestId, type : err.type});  \n            }  \n            return res.json(charge);  \n        });   \n     };  \n  }));  \n  return api;  \n}; </code></pre>\n</div>\n<br><strong>Summary</strong><br>\nCaptures credit cards with Stripe is simple. You can use their button and form or create your own system. If you create your own client and server, understanding the JSON objects for the token request and the charge request is important. You can pass just the minimum or all the data you have. Create the token on the client and pass the token and charge details to the server to complete the transaction.",
            "content": "In this article, I'll show you the JSON objects and Angular/Node code to capture a credit card with the <a href=\"https://stripe.com/\">Stripe service</a>. The code for this example project is on <a href=\"https://github.com/dfberry/stripe-angular-express\">GitHub</a>. It is a working project so the code may not be exactly like this article by the time you find it. <br><br><b>Introduction</b> <br>\nCapturing credit card information for products, services, and subscriptions is easy with many tools provided by credit card processing companies. Stripe provides an <a href=\"https://stripe.com/docs/checkout/tutorial\">html button that pops up a form to collect credit card information</a>. It is incredibly simple and straightforward.<br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s1600/stripe_popup.jpg\" imageanchor=\"1\"><img border=\"0\" height=\"312\" src=\"https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s400/stripe_popup.jpg\" width=\"400\"></a>\n</div>\n<br><br>\nYou can control the form to collect more data. You don't need to know how to program it, and it works. Yeah! This article doesn't cover the easy route of button and pop up because Stripe did a great job of that <a href=\"https://stripe.com/docs/checkout/tutorial\">on their website</a>. <br><br>\nIf you would prefer to control the credit card experience from beginning to end, you may choose to build your own form and process the data you collect to meet your own business needs. If that is the case read on.<br><br><strong>The Credit Card Token/Charge Experience</strong><br>\nA credit card number should never make it to your server – where you are liable for fraud or theft. Instead, the client-side code sends the credit card information to the processor, in this case Stripe, and Stripe sends back a token to the client-side code.<br><br>\nThe token isn't a charge to the card but more a promise to charge the card in the future. For the remainder of the customer's experience, you do not use the credit card information, only the token. The token travels with the rest of the customer information (name, address, items purchased) to the server. The server will make the actual charge and receive the resulting information from Stripe.<br><br>\nWith stripe, all you really need is a way to make an https request to their Api server, passing JSON, then receive the JSON request. That is true for both the token request and the charge request. Stripe responses with JSON.<br><br><strong>JSON is independent of the Technology</strong><br>\nThe rest of this article is broken into two different sections. The first section will review the JSON objects which have some requirements and a lot of optional fields. You can use any technology you want to make these, including curl. The second section will cover a specific code base of Angular on the client and Node/Express on the server. The code is very simple so there is little or no styling, validation, or error management.<br><br><strong>The Stripe Key</strong><br>\nStripe needs your Publishable key to create the token. You can find this on the Stripe dashboard in your account settings.<br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-3xQR2HhxtxM/Vtn09R3GlMI/AAAAAAAAAj8/PmxeqN6pi0o/s1600/Snip20160304_4.jpg\" imageanchor=\"1\"><img border=\"0\" height=\"457\" src=\"https://2.bp.blogspot.com/-3xQR2HhxtxM/Vtn09R3GlMI/AAAAAAAAAj8/PmxeqN6pi0o/s640/Snip20160304_4.jpg\" width=\"640\"></a>\n</div>\n<br><br>\nIf you are using curl, you can pass the key along with the token request. If you are passing it from code, it will need to be set before the token request. The curl example is:<br><div>\n<blockquote>\n<pre><code>\ncurl https://api.stripe.com/v1/tokens \\  -u sk_test_WqKypPUzUwdHSgZLT3zWZmBq: \\  -d card[number]=4242424242424242 \\  -d card[exp_month]=12 \\  -d card[exp_year]=2017 \\  -d card[cvc]=123\n</code></pre>\n</blockquote>\n</div>\n<br>\nThe key is the value after the –u param: <b>sk_test_…</b><br><br>\nNotice that only the key and card information is passed in the above curl. You can and should capture and pass the billing address. This allows you to see the address in the Stripe Dashboard when the card is finally made.<br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-tQ7sUHrGwp8/Vtn18CJIfwI/AAAAAAAAAkI/2OLdPU3ov5Q/s1600/Snip20160304_5.png\" imageanchor=\"1\"><img border=\"0\" height=\"228\" src=\"https://3.bp.blogspot.com/-tQ7sUHrGwp8/Vtn18CJIfwI/AAAAAAAAAkI/2OLdPU3ov5Q/s640/Snip20160304_5.png\" width=\"640\"></a>\n</div>\n<br><br>\nNotice that you only see the last 4 digits of the credit card. <br><br><strong>JSON to create a Stripe Token</strong><br>\nThe JSON object to request a token contains the credit card information. It should also contain the billing information for the customer.  A full list of card key/value pairs is listed in the <a href=\"https://stripe.com/docs/api/java#card_object\">Stripe docs</a>.<br><div>\n<blockquote>\n<pre><code>client stripe token request = \n{ \n \"number\":\"4242424242424242\", \n \"cvc\":\"123\", \n \"exp_month\":\"11\", \n \"exp_year\":\"2016\", \n \"name\":\"Barbara Jones\", \n \"address_city\":\"Seattle\", \n \"address_line1\":\"5678 Nine Street\", \n \"address_line2\":\"Box 3\", \n \"address_country\":\"USA\", \n \"address_state\":\"WA\", \n \"address_zip\":\"98105\" \n}</code></pre>\n</blockquote>\n</div>\n<br>\nThe response will be a status code, success is 200 with a json object of data including the token.<br><br><div>\n<blockquote>\n<pre><code>client stripe token response.response = { \n \"id\":\"tok_17jylQJklCPSOV9aLkiy5879\", \n \"object\":\"token\", \n \"card\":{ \n \"id\":\"card_17jylQJklCPSOV9ahtXhPVB8\", \n \"object\":\"card\", \n \"address_city\":\"Seattle\", \n \"address_country\":\"USA\", \n \"address_line1\":\"5678 Nine Street\", \n \"address_line1_check\":\"unchecked\", \n \"address_line2\":\"Box 3\", \n \"address_state\":\"WA\", \n \"address_zip\":\"98105\", \n \"address_zip_check\":\"unchecked\", \n \"brand\":\"Visa\", \n \"country\":\"US\", \n \"cvc_check\":\"unchecked\", \n \"dynamic_last4\":null, \n \"exp_month\":11, \n \"exp_year\":2016, \n \"funding\":\"credit\", \n \"last4\":\"4242\", \n \"metadata\":{ \n }, \n \"name\":\"Barbara Jones\", \n \"tokenization_method\":null \n }, \n \"client_ip\":\"73.11.000.147\", \n \"created\":1456782072, \n \"livemode\":false, \n \"type\":\"card\", \n \"used\":false \n}</code></pre>\n</blockquote>\n</div>\n<br>\nFor the rest of the credit card transaction, use the token only. You will need to pass it when you charge the customer's credit card. <br><br><strong>JSON for a successful Stripe Charge</strong><br>\nNow that you have the token, you can create a JSON object to represent the credit card charge.<br><br>\nclient stripe token response.status = 200 \n <br><div>\n<pre><code>server stripe charge request object = {    \n   \"amount\":1000,  \n   \"currency\":\"usd\",  \n   \"source\":\"tok_17jylQJklCPSOV9aLkiy5879\",  \n   \"description\":\"Donation for XYZ\",  \n   \"metadata\":{    \n      \"ShipTo\":\"Bob Smith\",  \n      \"BillTo\":\"Barbara Jones\"  \n   },  \n   \"receipt_email\":\"bob@company.com\",  \n   \"statement_descriptor\":\"MyStripeStore\",  \n   \"shipping\":{    \n      \"address\":{    \n         \"city\":\"Seattle\",  \n         \"country\":\"USA\",  \n         \"line1\":\"1234 Five Lane\",  \n         \"line2\":\"Floor 2\",  \n         \"postal_code\":\"98101\",  \n         \"state\":\"WA\"  \n      },  \n      \"name\":\"Bob Smith\",  \n      \"phone\":\"\"  \n   }  \n} </code></pre>\n</div>\n<br>\nMake sure the statement_descriptor has meaningful information to figure out the charge was valid– it shows up on the customer's bill. If you have information important to the transaction that Stripe doesn't collect, put those values in the metadata key. You can retrieve this information from Stripe to reconcile or fulfill the transaction on your end. Think of it as a backup – if your system goes down, Stripe still has enough information for you to rebuild the transaction.  The amount includes dollars and cents but no decimal. So \"1000\" is ten dollars, $10.00. <br><br>\nIf you are new to Stripe, you may not be using their advanced features but if you collect the data now, converting to and seeding some of the advanced feature objects will be easy.<br><br>\nA successful charge response returns an null error object and a result object. <br><br><div>\n<pre><code>server stripe charge response.charge = {    \n   \"id\":\"ch_17jylQJklCPSOV9aHyof0XV8\",  \n   \"object\":\"charge\",  \n   \"amount\":1000,  \n   \"amount_refunded\":0,  \n   \"application_fee\":null,  \n   \"balance_transaction\":\"txn_17jylQJklCPSOV9afi1bpfz5\",  \n   \"captured\":true,  \n   \"created\":1456782072,  \n   \"currency\":\"usd\",  \n   \"customer\":null,  \n   \"description\":\"Donation for XYZ\",  \n   \"destination\":null,  \n   \"dispute\":null,  \n   \"failure_code\":null,  \n   \"failure_message\":null,  \n   \"fraud_details\":{    \n   },  \n   \"invoice\":null,  \n   \"livemode\":false,  \n   \"metadata\":{    \n      \"ShipTo\":\"Bob Smith\",  \n      \"BillTo\":\"Barbara Jones\"  \n   },  \n   \"order\":null,  \n   \"paid\":true,  \n   \"receipt_email\":\"bob@company.com\",  \n   \"receipt_number\":null,  \n   \"refunded\":false,  \n   \"refunds\":{    \n      \"object\":\"list\",  \n      \"data\":[    \n      ],  \n      \"has_more\":false,  \n      \"total_count\":0,  \n      \"url\":\"/v1/charges/ch_17jylQJklCPSOV9aHyof0XV8/refunds\"  \n   },  \n   \"shipping\":{    \n      \"address\":{    \n         \"city\":\"Seattle\",  \n         \"country\":\"USA\",  \n         \"line1\":\"1234 Five Lane\",  \n         \"line2\":\"Floor 2\",  \n         \"postal_code\":\"98101\",  \n         \"state\":\"WA\"  \n      },  \n      \"carrier\":null,  \n      \"name\":\"Bob Smith\",  \n      \"phone\":\"\",  \n      \"tracking_number\":null  \n   },  \n   \"source\":{    \n      \"id\":\"card_17jylQJklCPSOV9ahtXhPVB8\",  \n      \"object\":\"card\",  \n      \"address_city\":\"Seattle\",  \n      \"address_country\":\"USA\",  \n      \"address_line1\":\"5678 Nine Street\",  \n      \"address_line1_check\":\"pass\",  \n      \"address_line2\":\"Box 3\",  \n      \"address_state\":\"WA\",  \n      \"address_zip\":\"98105\",  \n      \"address_zip_check\":\"pass\",  \n      \"brand\":\"Visa\",  \n      \"country\":\"US\",  \n      \"customer\":null,  \n      \"cvc_check\":\"pass\",  \n      \"dynamic_last4\":null,  \n      \"exp_month\":11,  \n      \"exp_year\":2016,  \n      \"fingerprint\":\"uOlT1SgxEykd9grd\",  \n      \"funding\":\"credit\",  \n      \"last4\":\"4242\",  \n      \"metadata\":{    \n      },  \n      \"name\":\"Barbara Jones\",  \n      \"tokenization_method\":null  \n   },  \n   \"source_transfer\":null,  \n   \"statement_descriptor\":\"MyStripeStore\",  \n   \"status\":\"succeeded\"  \n} </code></pre>\n</div>\n<br>\nThe two items you want to look for is status and paid. <br><br>\nStripe keeps a log of your transactions as JSON objects including the request and response of both the token and the charge. You may want to store all the information on your server, but if you don't, you can get it from Stripe when you need to. <br><br><strong>JSON for a failed Stripe Charge</strong><br>\nIf you customer's charge fails, the JSON for the charge is a bit different. You will want to present the customer with the meaningful error message to help them correct the problem on their end and complete the transaction successfully.<br><br>\nThe charge object will be null and the status object will have the error message.<br><br><div>\n<pre><code>server stripe charge response.status = {    \n   \"type\":\"StripeCardError\",  \n   \"stack\":\"Error: Your card was declined.\\n    at Error._Error (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/Error.js:12:17)\\n    at Error.Constructor (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/utils.js:105:13)\\n    at Error.Constructor (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/utils.js:105:13)\\n    at Function.StripeError.generate (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/Error.js:54:14)\\n    at IncomingMessage.&lt;anonymous&gt; (/Users/dfberry/repos/stripe-express-angular/node_modules/stripe/lib/StripeResource.js:138:39)\\n    at emitNone (events.js:72:20)\\n    at IncomingMessage.emit (events.js:166:7)\\n    at endReadableNT (_stream_readable.js:905:12)\\n    at nextTickCallbackWith2Args (node.js:455:9)\\n    at process._tickCallback (node.js:369:17)\",  \n   \"rawType\":\"card_error\",  \n   \"code\":\"card_declined\",  \n   \"message\":\"Your card was declined.\",  \n   \"raw\":{    \n      \"message\":\"Your card was declined.\",  \n      \"type\":\"card_error\",  \n      \"code\":\"card_declined\",  \n      \"charge\":\"ch_17jz4rJklCPSOV9aDPh2eooP\",  \n      \"statusCode\":402,  \n      \"requestId\":\"req_7zz28B9jlOpH1x\"  \n   },  \n   \"requestId\":\"req_7zz28B9jlOpH1x\",  \n   \"statusCode\":402  \n} </code></pre>\n</div>\n<br>\nThe message contains the information to display to the customer. While the transaction didn't complete, the log in the Stripe Dashboard will contain the same information. For a complete list of issues, look at the <a href=\"https://stripe.com/docs/api#errors\">Stripe </a><a href=\"https://stripe.com/docs/api#errors\">api</a><a href=\"https://stripe.com/docs/api#errors\"> for errors</a>.<br><br><strong>The Angular and Node/Express application</strong><br>\nIn order to use Stripe on the client, you need to pull in the Stripe javascript library.<br><br>\nYou can get the Stripe library for the client from stripe<br><br><div>\n<blockquote>\n<pre><code>&lt;script type=\"text/javascript\" src=\"https://js.stripe.com/v2/\"&gt;&lt;/script&gt;</code></pre>\n</blockquote>\n</div>\n<br>\nIn order to use Stripe on the server, you need to install Stripe from NPM<br><br><div>\n<blockquote>\n<pre><code>npm install stripe –save-dev</code></pre>\n</blockquote>\n</div>\n<br><strong>The Donation Form</strong><br>\nThe example is a donation form. It collects the customer information and allows the customer to choose the donation amount. A shipping address is also collected, for a thank you card back to the customer.<br><br><div class=\"separator\">\n<a href=\"https://4.bp.blogspot.com/-GvNk19u1eQM/Vtn5nIKpK3I/AAAAAAAAAkk/0UXycIfyySE/s1600/Snip20160229_4.png\" imageanchor=\"1\"><img border=\"0\" height=\"640\" src=\"https://4.bp.blogspot.com/-GvNk19u1eQM/Vtn5nIKpK3I/AAAAAAAAAkk/0UXycIfyySE/s640/Snip20160229_4.png\" width=\"342\"></a>\n</div>\n<br><br>\nThe web page has very little styling, no validation, and only a single error message if the credit card charge is denied.<br><br><strong>Angular Client Code</strong><br>\nThe Angular code includes a directive to display the html, a controller to collect the customer's information, and a service to post to the server. The token creation and charge are both handled in the service one call right after the other. This is definitely not ideal for a real-world situation. <br><br>\nThe card, card, and customer information are kept in JSON objects in the models.js file. The Stripe publishable key is in config.js along with the cart item name. When the token and charge are made, the JSON object that Stripe expects for each of these is created. <br><br>\nAs all of the Stripe work is in the service, that is the Angular code to review. The complete charge as 3 separate objects, cart, card, and customer.<br><br><div>\n<pre><code>exports.$myservice = function($http,$myappconfig){  \n    var commit = function (completeCharge, callback){  \n        var result = {};  \n        // my stripe test key  \n        Stripe.setPublishableKey($myappconfig.stripePublishableKey);  \n        // credit card info passed   \n        // billing address passed as part of charge.card  \n        Stripe.card.createToken(completeCharge.card, function(status, response) {  \n             if (status.error) {  \n                console.log(\"stripe token not created\");  \n                result.error = status.error;  \n                callback(result);  \n            }   \n            var chargeRequestObject = {   \n                    stripeToken: response.id,   \n                    cart: completeCharge.cart ,   \n                    customer: completeCharge.customer  \n            };  \n            // token (not credit card) passed  \n            // shipping address passed in charge.customer  \n            $http.  \n                post('/api/v1/checkout', chargeRequestObject).  \n                then(function(data) { //success  \n                    callback(null, data);  \n                },  \n                function(response){ //failure  \n                    callback(response.data.error, response);  \n                });  \n            });  \n        }  \n    return {  \n      commit: commit  \n    };      \n}</code></pre>\n</div>\n<br><strong>Node Server Code</strong><br>\nThe server code is a node app using Express. It only serves the html page above and has an api to process a charge -- very lean in order to drop it into any other web site.  Morgan is used for logging and Wagner is used for dependency injection.  You can start the app with npm start or node server/index.js.<br><br><b>Index.js</b>\n\n<br><div>\n<pre><code>var express = require('express');  \nvar wagner = require('wagner-core');  \nvar path = require('path');  \nrequire('./dependencies')(wagner);  \nvar app = express();  \napp.use(require('morgan')());  \napp.get(['/'], function (req, res) {res.sendFile(path.join(__dirname + '/../public/default.html'));});  \napp.use('/api/v1', require('./api')(wagner));  \napp.use('/public', express.static(__dirname + '/../public', { maxAge: 4 * 60 * 60 * 1000 /* 2hrs */}));  \napp.listen(3000);  \nconsole.log('Listening on port 3000!'); </code></pre>\n</div>\n<br>\nThe dependencies file creates the wagner dependency injection for the config and stripe objects.<br><br><strong>Dependencies.js</strong><br><div>\n<pre><code>var fs = require('fs');  \nvar Stripe = require('stripe');  \nvar configFile = require('./config.json');  \nmodule.exports = function(wagner) {  \n  wagner.factory('Stripe', function(Config) {  \n    return Stripe(Config.stripeKey);  \n  });  \n  wagner.factory('Config', function() {  \n    return configFile;  \n  });  \n};</code></pre>\n</div>\n<br>\nThe<strong> config.json</strong> is simple the configuration json object: \n\n <br><div>\n<pre><code>\n{  \n  \"stripeKey\": \"sk_test_WqKypPUzUXXXXXXXT3zWZmBq\",  \n  \"stripePublishableClientKey\": \"pk_test_ArJPMXXXXlF2Ml4m4e8ILmiP\",  \n  \"Characters22_StoreName\": \"MyStripeStore\"  \n}</code></pre>\n</div>\n<br>\nThe main Stripe code of the application is in the<strong> api.js</strong> file to process the Stripe charge. <br><div>\n<pre><code>module.exports = function(wagner) {  \n  var api = express.Router();  \n  api.use(bodyparser.json());  \n  /* Stripe Checkout API */  \n  api.post('/checkout', wagner.invoke(function(Stripe) {  \n    return function(req, res) {  \n        // https://stripe.com/docs/api#capture_charge  \n        // shipping name is in the metadata so that it is easily found on stripe's website   \n        // statement_descriptor &amp; description will show on credit card bill  \n        // receipt_email is sent but stripe isn't sending receipt - you still have to do that  \n        // shipping is sent only so you can pull information from stripe   \n        // metadata: 20 keys, with key names up to 40 characters long and values up to 500 characters long  \n        var stripeCharge = {  \n            amount: req.body.cart.totalprice,  \n            currency: 'usd',  \n            source: req.body.stripeToken,  \n            description: req.body.cart.name,  \n            metadata: {'ShipTo': req.body.customer.shipping.name, 'BillTo': req.body.customer.billing.name},  \n            receipt_email: req.body.customer.email,  \n            statement_descriptor: config.Characters22_StoreName,  \n            shipping: req.body.customer.shipping   \n        };  \n        console.log(\"server stripe charge request object = \" + JSON.stringify(stripeCharge)+ \"\\n\");  \n        // Charge the card NOW  \n        Stripe.charges.create(stripeCharge,function(err, charge) {  \n            console.log(\"server stripe charge response.err = \" + JSON.stringify(err) + \"\\n\");        \n            console.log(\"server stripe charge response.charge = \" + JSON.stringify(charge) + \"\\n\");   \n            if (err) {  \n                return res.  \n                status(status.INTERNAL_SERVER_ERROR).  \n                json({ error: err.toString(), charge: err.raw.charge, request: err.requestId, type : err.type});  \n            }  \n            return res.json(charge);  \n        });   \n     };  \n  }));  \n  return api;  \n}; </code></pre>\n</div>\n<br><strong>Summary</strong><br>\nCaptures credit cards with Stripe is simple. You can use their button and form or create your own system. If you create your own client and server, understanding the JSON objects for the token request and the charge request is important. You can pass just the minimum or all the data you have. Create the token on the client and pass the token and charge details to the server to complete the transaction.",
            "enclosure": {
                "thumbnail": "https://3.bp.blogspot.com/-89JUZplgb3g/VtnzjJFE_LI/AAAAAAAAAjw/AsncC9Tw-xo/s72-c/stripe_popup.jpg"
            },
            "categories": [
                "E-Commerce",
                "angular",
                "json",
                "stripe",
                "Dina",
                "Node",
                "CC",
                "token"
            ]
        },
        {
            "title": "Extending a linux web dashboard",
            "pubDate": "2016-02-18 18:11:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/02/extending-linux-web-dashboard.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-880482849541199694",
            "author": "Dina Berry",
            "thumbnail": "https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s72-c/linux-dash-system-status.png",
            "description": "\n<h1>\nAdding pm2 status to linux-dash</h1>\n<a href=\"https://github.com/afaqurk/linux-dash\">linux-dash</a> is a light-weight, open-source web dashboard to monitor your linux machine or virtual machine. You can find this package on <a href=\"https://github.com/afaqurk/linux-dash\">Github</a>. <br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s1600/linux-dash-system-status.png\" imageanchor=\"1\"><img border=\"0\" height=\"448\" src=\"https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s640/linux-dash-system-status.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThe dashboard reports many different aspects of our linux installation via shell scripts (*.sh). This allows the dashboard to be light-weight, and work on most linux machines. The website displays running charts, and tables. The web site can be node, php, or go. For the node webserver, the only dependencies are express and websocket.</div>\n<h2>\nExtending linux-dash</h2>\nYou may have a few extra services or programs running on your linux installation that you would like to display on linux-dash . I use <a href=\"http://pm2.keymetrics.io/\">pm2</a>, a process manager. Adding a table to display the pm2 status information was very easy -- even if you are not familiar with client-side Angular directives or server-side Node.JS or server-side shell scripts. <br><br>\nThe naming convention and templating allows us to focus on the few components we need to build without struggling on the glue between them. <br><h2>\npm2 Dashboard Design</h2>\nThe 'pm2 list' command shows a table with information on the command line. <br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-V474g5YCOdU/VsYHMgvw1II/AAAAAAAAAig/Cn7nhHdy950/s1600/pm2-list.png\" imageanchor=\"1\"><img border=\"0\" height=\"116\" src=\"https://2.bp.blogspot.com/-V474g5YCOdU/VsYHMgvw1II/AAAAAAAAAig/Cn7nhHdy950/s640/pm2-list.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nWe want to show this in the linux-dash website on the applications tab in its own table.</div>\n<br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-KMxcFRkrkCs/VsYHSryQF3I/AAAAAAAAAik/FCYIcG2FZLs/s1600/linux-dash-pm2.png\" imageanchor=\"1\"><img border=\"0\" height=\"532\" src=\"https://3.bp.blogspot.com/-KMxcFRkrkCs/VsYHSryQF3I/AAAAAAAAAik/FCYIcG2FZLs/s640/linux-dash-pm2.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nIn order to do that, we need:</div>\n<ol>\n<li>a new shell script - to capture the results of running \"pm2 list\" and return json</li>\n<li>changes to glue - to find script and display as table</li>\n</ol>\n<h2>\nInstalling linux-dash</h2>\nIf you do not have linux-dash installed, you will need to get it. Clone it from <a href=\"https://github.com/afaqurk/linux-dash\">github</a>. Make sure scripts have execute status and the webserver is started with SUDO privileges. <br><h2>\nWriting the server-side Shell script</h2>\nThis section applies to services with a snapshot or single point in time. <br><br>\nIf you have not written a shell script before, no need to worry. There are plenty of examples of shell scripts at /server/modules/shell_files. The final output of the shell script needs to either be an empty json object such as {} or an array of values such as [{},{},{}]. The values will be key/value pairs (1 key, 1 value) which will diplay as a 2xn grid of information.<br><br><div class=\"separator\">\n<a href=\"https://1.bp.blogspot.com/-H9orTyqpFMQ/VsYHjELyDzI/AAAAAAAAAio/-lakoI-bHFc/s1600/linux-dash-2byN.png\" imageanchor=\"1\"><img border=\"0\" height=\"640\" src=\"https://1.bp.blogspot.com/-H9orTyqpFMQ/VsYHjELyDzI/AAAAAAAAAio/-lakoI-bHFc/s640/linux-dash-2byN.png\" width=\"560\"></a>\n</div>\n<br><br><br>\nThe second choice is a table (array of key/value pairs) with more columns which is what we need.<br><h3>\npm2 list output</h3>\nThe command I usually run at the command line is \"pm2 list\" -- the response shows each process with uptime, status, and other information in a table. <br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-UyFHdGS6eOo/VsYHp6haQOI/AAAAAAAAAis/SAkDR7wkjpU/s1600/pm2-list-numbers.png\" imageanchor=\"1\"><img border=\"0\" height=\"116\" src=\"https://3.bp.blogspot.com/-UyFHdGS6eOo/VsYHp6haQOI/AAAAAAAAAis/SAkDR7wkjpU/s640/pm2-list-numbers.png\" width=\"640\"></a>\n</div>\n<br><br><br>\nWe need to know which lines to ignore (1-3, 6, 7) and which to include (only 4 and 5). <br>\nMake sure each line of your output is accounted for as either ignored or parsed. While I ignored the header and footer, perhaps your data should be included. <br><br>\nThe shell script needs to be able to read each row into a meaningful json object such as:<br><br><div>\n<pre><code>[ \n { \n \"appName\":\"dfberry-8080\",\n \"id\":\"0\",\n \"mode\":\"fork\",\n \"pid\":\"1628\",\n \"status\":\"online\",\n \"restart\":\"0\",\n \"uptime\":\"13D\",\n \"memory\":\"20.043MB\",\n \"watching\":\"disabled\"\n },\n { \n \"appName\":\"linux-dash\",\n \"id\":\"1\",\n \"mode\":\"fork\",\n \"pid\":\"29868\",\n \"status\":\"online\",\n \"restart\":\"21\",\n \"uptime\":\"7D\",\n \"memory\":\"28.293MB\",\n \"watching\":\"disabled\"\n }\n]\n</code></pre>\n</div>\n<h2>\npm2.sh</h2>\nThe script has 3 sections. The first section sets the command to the variable 'command'. The second section executes the command and sets the returned text (the command line table) to the variable 'data'. The third section is in two sections.<br><br>\nThe first section (a) executes if the 'data' variable has any length. The second section (b) returns an empty json object if the 'data' variable is empty.<br><br>\nMost of the work is in section 3.a with the 'awk' command. The first line pipes the 'data' variable through tail, only passing lines 4 or greater to the next pipe which is head. Head takes all the lines except the last 2 and pipes the results to awk.<br><br>\nThe rest of 3.a is working through each column of each row, getting the values $6 means the sixth column. Columns include column break characters of '|' so make sure to include them in the count. <br><br>\nIf you are watching the trailing commas, you may be wondering how the last one is removed. Bash has <a href=\"http://unix.stackexchange.com/questions/144298/delete-the-last-character-of-a-string-using-string-manipulation-in-shell-script\">a couple of different ways</a>, I'm using the older bash version syntax which is ${t%?}. <br><br><div>\n<pre><code>#!/bin/bash\n\n#1: set text of command\ncommand=\"pm2 list\"\n\n#2: execute command\ndata=\"$($command)\"\n\n#3: only process data if variable has a length \n#this should handle cases where pm2 is not installed\nif [ -n \"$data\" ]; then\n\n #a: start processing data on line 4\n #don't process last 2 lines\n json=$( echo \"$data\" | tail -n +4 | head -n +2 \\\n | awk '{print \"{\"}\\\n {print \"\\\"appName\\\":\\\"\" $2 \"\\\",\"} \\\n {print \"\\\"id\\\":\\\"\" $4 \"\\\",\"} \\\n {print \"\\\"mode\\\":\\\"\" $6 \"\\\",\"} \\\n {print \"\\\"pid\\\":\\\"\" $8 \"\\\",\"}\\\n {print \"\\\"status\\\":\\\"\" $10 \"\\\",\"}\\\n {print \"\\\"restart\\\":\\\"\" $12 \"\\\",\"}\\\n {print \"\\\"uptime\\\":\\\"\" $14 \"\\\",\"}\\\n {print \"\\\"memory\\\":\\\"\" $16 $17 \"\\\",\"}\\\n {print \"\\\"watching\\\":\\\"\" $19 \"\\\"\"}\\\n {print \"},\"}')\n #make sure to remove last comma and print in array\n echo \"[\" ${json%?} \"]\"\nelse\n #b: no data found so return empty json object\n echo \"{}\"\nfi\n</code></pre>\n</div>\n<br>\nMake sure the script has execute permissions then try it out on your favorite linux OS. If you have pm2 installed and running, you should get a json object filled in with values.<br><br>\nAt this point, we are done with the server-side code. Isn't that amazing? <br><h3>\nNaming conventions</h3>\nThe client-side piece of the code is connected to the server-side script via the naming convention. I called this script pm2.sh on the server in the server/modules/shell_files directory. For the client-side/Angular files, you need to use the same name or Angular version of the same name. <br><h2>\nClient-side changes for Angular</h2>\nThe Angular directive will be pm2 and used like:<br><br><div>\n<pre><code>&lt;pm2&gt;&lt;/pm2&gt;</code></pre>\n</div>\n<br>\nAdd this to the /templates/sections/applications.html so that the entire file looks like:<br><br><div>\n<pre><code><common-applications></common-applications><pre><code>&lt;common-applications&gt;&lt;/common-applications&gt;\n&lt;memcached&gt;&lt;/memcached&gt;\n&lt;redis&gt;&lt;/redis&gt;\n&lt;pm2&gt;&lt;/pm2&gt;</code></pre>\n<pm2></pm2></code></pre>\n</div>\n<br>\nSince the pm2 directive is at the end, it will display as the last table. Notice I haven't actually built a table in html, css, or any other method.<br><br>\nI just added a directive using a naming convention tied to the server-side script. Pretty cool, huh?<br><h3>\nRouting to the new Angular directive</h3>\nThe last piece is to route the directive 'pm2' to a call to the server for the 'pm2.sh' script. <br>\nIn the /js/modules.js file, the routing for simple tables in controlled by the 'simpleTableModules' variable. Find that section. We need to add a new json object to the array of name/template sections. <br><br><div>\n<pre><code>{\n name: 'pm2',\n template: '&lt;table-data heading=\"P(rocess) M(anager) 2\" module-name=\"pm2\" info=\"pm2 read-out.\"&gt;&lt;/table-data&gt;'\n}, </code></pre>\n</div>\n<br>\nIt doesn't matter where in the array the section is added, just that the naming convention is used. Notice the name is 'pm2' and the template.module-name is set to the same value of 'pm2'. <br><br>\nIf I wanted a simple table of 2 columns instead of 9 columns, the json object would look like:<br><br><div>\n<pre><code>{\n name: 'pm2',\n template: '&lt;key-value-list heading=\"P(rocess) M(anager) 2\" module-name=\"pm2\" info=\"pm2 read-out.\"&gt;&lt;/key-value-list&gt;'\n},</code></pre>\n</div>\n<br>\nThe key-value-list changes the html display to a 2xN column table. <br><h2>\nSummary</h2>\nWith very little code, you can add reports to linux-dash. You need to write a shell script with execute permissions that outputs a json object for the server-side. For the client-side you need to create a directive via adding its syntax to the appropriate section template. Then add a route to the modules.js file. The biggest piece of work is getting the shell script to work. <br><br>\nNow that you know how to create new reporting tables for linux-dash, feel free to add your own code to the project via a pull request.",
            "content": "\n<h1>\nAdding pm2 status to linux-dash</h1>\n<a href=\"https://github.com/afaqurk/linux-dash\">linux-dash</a> is a light-weight, open-source web dashboard to monitor your linux machine or virtual machine. You can find this package on <a href=\"https://github.com/afaqurk/linux-dash\">Github</a>. <br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s1600/linux-dash-system-status.png\" imageanchor=\"1\"><img border=\"0\" height=\"448\" src=\"https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s640/linux-dash-system-status.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThe dashboard reports many different aspects of our linux installation via shell scripts (*.sh). This allows the dashboard to be light-weight, and work on most linux machines. The website displays running charts, and tables. The web site can be node, php, or go. For the node webserver, the only dependencies are express and websocket.</div>\n<h2>\nExtending linux-dash</h2>\nYou may have a few extra services or programs running on your linux installation that you would like to display on linux-dash . I use <a href=\"http://pm2.keymetrics.io/\">pm2</a>, a process manager. Adding a table to display the pm2 status information was very easy -- even if you are not familiar with client-side Angular directives or server-side Node.JS or server-side shell scripts. <br><br>\nThe naming convention and templating allows us to focus on the few components we need to build without struggling on the glue between them. <br><h2>\npm2 Dashboard Design</h2>\nThe 'pm2 list' command shows a table with information on the command line. <br><br><div class=\"separator\">\n<a href=\"https://2.bp.blogspot.com/-V474g5YCOdU/VsYHMgvw1II/AAAAAAAAAig/Cn7nhHdy950/s1600/pm2-list.png\" imageanchor=\"1\"><img border=\"0\" height=\"116\" src=\"https://2.bp.blogspot.com/-V474g5YCOdU/VsYHMgvw1II/AAAAAAAAAig/Cn7nhHdy950/s640/pm2-list.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nWe want to show this in the linux-dash website on the applications tab in its own table.</div>\n<br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-KMxcFRkrkCs/VsYHSryQF3I/AAAAAAAAAik/FCYIcG2FZLs/s1600/linux-dash-pm2.png\" imageanchor=\"1\"><img border=\"0\" height=\"532\" src=\"https://3.bp.blogspot.com/-KMxcFRkrkCs/VsYHSryQF3I/AAAAAAAAAik/FCYIcG2FZLs/s640/linux-dash-pm2.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nIn order to do that, we need:</div>\n<ol>\n<li>a new shell script - to capture the results of running \"pm2 list\" and return json</li>\n<li>changes to glue - to find script and display as table</li>\n</ol>\n<h2>\nInstalling linux-dash</h2>\nIf you do not have linux-dash installed, you will need to get it. Clone it from <a href=\"https://github.com/afaqurk/linux-dash\">github</a>. Make sure scripts have execute status and the webserver is started with SUDO privileges. <br><h2>\nWriting the server-side Shell script</h2>\nThis section applies to services with a snapshot or single point in time. <br><br>\nIf you have not written a shell script before, no need to worry. There are plenty of examples of shell scripts at /server/modules/shell_files. The final output of the shell script needs to either be an empty json object such as {} or an array of values such as [{},{},{}]. The values will be key/value pairs (1 key, 1 value) which will diplay as a 2xn grid of information.<br><br><div class=\"separator\">\n<a href=\"https://1.bp.blogspot.com/-H9orTyqpFMQ/VsYHjELyDzI/AAAAAAAAAio/-lakoI-bHFc/s1600/linux-dash-2byN.png\" imageanchor=\"1\"><img border=\"0\" height=\"640\" src=\"https://1.bp.blogspot.com/-H9orTyqpFMQ/VsYHjELyDzI/AAAAAAAAAio/-lakoI-bHFc/s640/linux-dash-2byN.png\" width=\"560\"></a>\n</div>\n<br><br><br>\nThe second choice is a table (array of key/value pairs) with more columns which is what we need.<br><h3>\npm2 list output</h3>\nThe command I usually run at the command line is \"pm2 list\" -- the response shows each process with uptime, status, and other information in a table. <br><br><div class=\"separator\">\n<a href=\"https://3.bp.blogspot.com/-UyFHdGS6eOo/VsYHp6haQOI/AAAAAAAAAis/SAkDR7wkjpU/s1600/pm2-list-numbers.png\" imageanchor=\"1\"><img border=\"0\" height=\"116\" src=\"https://3.bp.blogspot.com/-UyFHdGS6eOo/VsYHp6haQOI/AAAAAAAAAis/SAkDR7wkjpU/s640/pm2-list-numbers.png\" width=\"640\"></a>\n</div>\n<br><br><br>\nWe need to know which lines to ignore (1-3, 6, 7) and which to include (only 4 and 5). <br>\nMake sure each line of your output is accounted for as either ignored or parsed. While I ignored the header and footer, perhaps your data should be included. <br><br>\nThe shell script needs to be able to read each row into a meaningful json object such as:<br><br><div>\n<pre><code>[ \n { \n \"appName\":\"dfberry-8080\",\n \"id\":\"0\",\n \"mode\":\"fork\",\n \"pid\":\"1628\",\n \"status\":\"online\",\n \"restart\":\"0\",\n \"uptime\":\"13D\",\n \"memory\":\"20.043MB\",\n \"watching\":\"disabled\"\n },\n { \n \"appName\":\"linux-dash\",\n \"id\":\"1\",\n \"mode\":\"fork\",\n \"pid\":\"29868\",\n \"status\":\"online\",\n \"restart\":\"21\",\n \"uptime\":\"7D\",\n \"memory\":\"28.293MB\",\n \"watching\":\"disabled\"\n }\n]\n</code></pre>\n</div>\n<h2>\npm2.sh</h2>\nThe script has 3 sections. The first section sets the command to the variable 'command'. The second section executes the command and sets the returned text (the command line table) to the variable 'data'. The third section is in two sections.<br><br>\nThe first section (a) executes if the 'data' variable has any length. The second section (b) returns an empty json object if the 'data' variable is empty.<br><br>\nMost of the work is in section 3.a with the 'awk' command. The first line pipes the 'data' variable through tail, only passing lines 4 or greater to the next pipe which is head. Head takes all the lines except the last 2 and pipes the results to awk.<br><br>\nThe rest of 3.a is working through each column of each row, getting the values $6 means the sixth column. Columns include column break characters of '|' so make sure to include them in the count. <br><br>\nIf you are watching the trailing commas, you may be wondering how the last one is removed. Bash has <a href=\"http://unix.stackexchange.com/questions/144298/delete-the-last-character-of-a-string-using-string-manipulation-in-shell-script\">a couple of different ways</a>, I'm using the older bash version syntax which is ${t%?}. <br><br><div>\n<pre><code>#!/bin/bash\n\n#1: set text of command\ncommand=\"pm2 list\"\n\n#2: execute command\ndata=\"$($command)\"\n\n#3: only process data if variable has a length \n#this should handle cases where pm2 is not installed\nif [ -n \"$data\" ]; then\n\n #a: start processing data on line 4\n #don't process last 2 lines\n json=$( echo \"$data\" | tail -n +4 | head -n +2 \\\n | awk '{print \"{\"}\\\n {print \"\\\"appName\\\":\\\"\" $2 \"\\\",\"} \\\n {print \"\\\"id\\\":\\\"\" $4 \"\\\",\"} \\\n {print \"\\\"mode\\\":\\\"\" $6 \"\\\",\"} \\\n {print \"\\\"pid\\\":\\\"\" $8 \"\\\",\"}\\\n {print \"\\\"status\\\":\\\"\" $10 \"\\\",\"}\\\n {print \"\\\"restart\\\":\\\"\" $12 \"\\\",\"}\\\n {print \"\\\"uptime\\\":\\\"\" $14 \"\\\",\"}\\\n {print \"\\\"memory\\\":\\\"\" $16 $17 \"\\\",\"}\\\n {print \"\\\"watching\\\":\\\"\" $19 \"\\\"\"}\\\n {print \"},\"}')\n #make sure to remove last comma and print in array\n echo \"[\" ${json%?} \"]\"\nelse\n #b: no data found so return empty json object\n echo \"{}\"\nfi\n</code></pre>\n</div>\n<br>\nMake sure the script has execute permissions then try it out on your favorite linux OS. If you have pm2 installed and running, you should get a json object filled in with values.<br><br>\nAt this point, we are done with the server-side code. Isn't that amazing? <br><h3>\nNaming conventions</h3>\nThe client-side piece of the code is connected to the server-side script via the naming convention. I called this script pm2.sh on the server in the server/modules/shell_files directory. For the client-side/Angular files, you need to use the same name or Angular version of the same name. <br><h2>\nClient-side changes for Angular</h2>\nThe Angular directive will be pm2 and used like:<br><br><div>\n<pre><code>&lt;pm2&gt;&lt;/pm2&gt;</code></pre>\n</div>\n<br>\nAdd this to the /templates/sections/applications.html so that the entire file looks like:<br><br><div>\n<pre><code><common-applications></common-applications><pre><code>&lt;common-applications&gt;&lt;/common-applications&gt;\n&lt;memcached&gt;&lt;/memcached&gt;\n&lt;redis&gt;&lt;/redis&gt;\n&lt;pm2&gt;&lt;/pm2&gt;</code></pre>\n<pm2></pm2></code></pre>\n</div>\n<br>\nSince the pm2 directive is at the end, it will display as the last table. Notice I haven't actually built a table in html, css, or any other method.<br><br>\nI just added a directive using a naming convention tied to the server-side script. Pretty cool, huh?<br><h3>\nRouting to the new Angular directive</h3>\nThe last piece is to route the directive 'pm2' to a call to the server for the 'pm2.sh' script. <br>\nIn the /js/modules.js file, the routing for simple tables in controlled by the 'simpleTableModules' variable. Find that section. We need to add a new json object to the array of name/template sections. <br><br><div>\n<pre><code>{\n name: 'pm2',\n template: '&lt;table-data heading=\"P(rocess) M(anager) 2\" module-name=\"pm2\" info=\"pm2 read-out.\"&gt;&lt;/table-data&gt;'\n}, </code></pre>\n</div>\n<br>\nIt doesn't matter where in the array the section is added, just that the naming convention is used. Notice the name is 'pm2' and the template.module-name is set to the same value of 'pm2'. <br><br>\nIf I wanted a simple table of 2 columns instead of 9 columns, the json object would look like:<br><br><div>\n<pre><code>{\n name: 'pm2',\n template: '&lt;key-value-list heading=\"P(rocess) M(anager) 2\" module-name=\"pm2\" info=\"pm2 read-out.\"&gt;&lt;/key-value-list&gt;'\n},</code></pre>\n</div>\n<br>\nThe key-value-list changes the html display to a 2xN column table. <br><h2>\nSummary</h2>\nWith very little code, you can add reports to linux-dash. You need to write a shell script with execute permissions that outputs a json object for the server-side. For the client-side you need to create a directive via adding its syntax to the appropriate section template. Then add a route to the modules.js file. The biggest piece of work is getting the shell script to work. <br><br>\nNow that you know how to create new reporting tables for linux-dash, feel free to add your own code to the project via a pull request.",
            "enclosure": {
                "thumbnail": "https://3.bp.blogspot.com/-4JUWajW45Iw/VsYG-jAqDFI/AAAAAAAAAic/Q9RsOA7xhvM/s72-c/linux-dash-system-status.png"
            },
            "categories": [
                "web",
                "Shell",
                "angular",
                "Linux",
                "dashboard",
                "bash",
                "Dina",
                "Node",
                "linux-dash",
                "ng",
                "pm2"
            ]
        },
        {
            "title": "Prototyping in MongoDB with the Aggregation Pipeline stage operator $sample",
            "pubDate": "2016-02-05 18:57:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/02/prototyping-in-mongodb-with-aggregation.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-1141737467075056377",
            "author": "Dina Berry",
            "thumbnail": "https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s72-c/emptymap.png",
            "description": "\n<h1>\nPrototyping in MongoDB with the Aggregation Pipeline stage operator $sample</h1>\n<h2>\nThe World Map as a visual example</h2>\nIn order to show how the random sampling works in the mongoDB query, this NodeJS Express website will show the world map and display random latitude/longitude points on the map. Each refresh of the page will produce new random points. Below the map, the docs will display. \n<br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s1600/emptymap.png\" imageanchor=\"1\"><img border=\"0\" height=\"436\" src=\"https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s640/emptymap.png\" width=\"640\"></a>\n</div>\n<br><br>\nOnce the website is up and working with data points, we will play with the query to see how the data points change in response. \n<br><br>\nThe demonstration video is available on <a href=\"https://youtu.be/uyPmDZ8llk4\">YouTube</a>.\n<br><br><a href=\"https://youtu.be/uyPmDZ8llk4\"><img src=\"http://img.youtube.com/vi/uyPmDZ8llk4/0.jpg\" height=\"480\" width=\"640\"></a>\n\n<br><h2>\nSetup steps for the website</h2>\n<h3>\nSetup</h3>\nThis article assumes you have no mongoDB, no website, and no data. It does assume you have an account on <a href=\"http://www.compose.io/\">Compose</a>. Each step is broken out and explained. If there is a step you already have, such as the mongoDB with latitude/longitude data or a website that displays it, skip to the next. \n<br><ol>\n<li>get website running, display map with no data</li>\n<li>setup the mongoDB+ ssl database</li>\n<li>get mock data including latitude and longitude</li>\n<li>insert the mock data into database</li>\n<li>update database data types </li>\n<li>verify world map displays data points</li>\n</ol>\n<h3>\nPlay</h3>\nWhen the website works and the world map displays data points, let's play with it to see how $sample impacts the results.\n<br><ol>\n<li>understand the $sample operator</li>\n<li>change the row count</li>\n<li>change the aggregation pipeline order</li>\n<li>prototype with $sample</li>\n</ol>\n<h3>\nSystem architecture</h3>\nThe <b>data import script</b> is /insert.js. It opens and inserts a json file into a mongoDB collection. It doesn't do any transformation. \n<br><br>\nThe <b>data update script</b> is /update.js. It updates the data to numeric and geojson types.\n<br><br>\nThe <b>server</b> is a nodeJs Express website using the native MongoDB driver. The code uses the filesystem, url, and path libraries. This is a bare-bones express website. The /server/server.js file is the web server, with /server/query.js as the database layer. The server runs at http://127.0.0.1:8080/map/. This address is routed to /public/highmap/world.highmap.html. The data query will be made to http://127.0.0.1:8080/map/data/ from the client file /public/highmap/world.highmap.js. \n<br><br>\nThe <b>client</b> files are in the /public directory. The main web file is /highmap/world.highmap.html. It uses jQuery as the javascript framework, and <a href=\"http://www.highcharts.com/maps/demo\">highmap</a> as the mapping library which plots the points on the world map. The size of the map is controlled by the /public/highmap/world.highmap.css stylesheet for the map id. \n<br><h3>\nStep 1: The NodeJS Express Website</h3>\nIn order to get the website up and going, you need to clone this repository, make sure nodeJS is installed, and install the dependency libraries found in the package.json file. \n<br><br><div>\n<b>Todo</b>: install dependencies</div>\n<br><blockquote class=\"tr_bq\">\nnpm install</blockquote>\n<br>\nOnce the dependencies are installed, you can start the web server.\n<br><br><div>\n<b>Todo</b>: start website</div>\n<br><blockquote class=\"tr_bq\">\nnpm start</blockquote>\n<br><div>\n<b>Todo</b>: Request the website to see the world map. The map should display successfully with no data points.\n</div>\n<br><blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/\">http://127.0.0.1:8080/map/</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s1600/emptymap.png\" imageanchor=\"1\"><img border=\"0\" height=\"436\" src=\"https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s640/emptymap.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 2: Setup the <a href=\"http://compose.io/\">Compose</a> MongoDB+ Deployment and Database</h3>\nYou can move on to the next section, if you have a mongoDB deployment with SSL to use, and have the following items:\n<br><ul>\n<li>deployment public SSL key in the /server/clientcertificate.pem file\n</li>\n<li>connection string for that deployment in /server/config.json \n</li>\n</ul>\n<div>\n<b>Todo</b>: Create a new deployment on Compose for a MongoDB+ database with an SSL connection. </div>\n<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-WQyDeHwDGwM/VrO5XsJe_vI/AAAAAAAAAgM/0itG3a-PeYY/s1600/mongoDB%252BSSL.png\" imageanchor=\"1\"><img border=\"0\" height=\"308\" src=\"https://4.bp.blogspot.com/-WQyDeHwDGwM/VrO5XsJe_vI/AAAAAAAAAgM/0itG3a-PeYY/s640/mongoDB%252BSSL.png\" width=\"640\"></a>\n</div>\n<br><br>\nWhile still on the <a href=\"http://compose.io/\">Compose</a> backoffice, open the new deployment and copy the connection string. \n<br><br><div>\n<b>Todo</b>: Copy connection string </div>\n<br>\nYou will need the <b>entire connection string</b> in order to insert, update, and query the data. The connection string uses a user and password at the beginning and the database name at the end.\n<br><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-QQXZg9b0Dus/VrO5mRtcHzI/AAAAAAAAAgQ/yjNxPVjr7DE/s1600/composeio-ssl.png\" imageanchor=\"1\"><img border=\"0\" height=\"360\" src=\"https://3.bp.blogspot.com/-QQXZg9b0Dus/VrO5mRtcHzI/AAAAAAAAAgQ/yjNxPVjr7DE/s640/composeio-ssl.png\" width=\"640\"></a>\n</div>\n<br><br>\nYou also need to get the SSL Public key from the <a href=\"http://compose.io/\">Compose</a> Deployment Overview page. You will need to login with your <a href=\"http://www.compose.io/\">Compose</a> user password in order for the public key to show. \n<br><br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-BhkZqThglLo/VrO5sUpSQEI/AAAAAAAAAgU/gRc0pOedm-s/s1600/composeiosslpublickey.png\" imageanchor=\"1\"><img border=\"0\" height=\"283\" src=\"https://1.bp.blogspot.com/-BhkZqThglLo/VrO5sUpSQEI/AAAAAAAAAgU/gRc0pOedm-s/s640/composeiosslpublickey.png\" width=\"640\"></a>\n</div>\n<br><br><div>\n<b>Todo</b>: Save the entire <b>SSL Public key</b> to /server/clientcertificate.pem. </div>\n<br>\nIf you save it somewhere else, you need to change the mongodb.certificatefile setting in /server/config.json.\n<br><br>\nYou will also need to create a user in the Deployment's database. \n<br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-lwzNOrvWtSs/VrO55auLHdI/AAAAAAAAAgY/y0bwn6X9OrY/s1600/adduser.png\" imageanchor=\"1\"><img border=\"0\" height=\"243\" src=\"https://2.bp.blogspot.com/-lwzNOrvWtSs/VrO55auLHdI/AAAAAAAAAgY/y0bwn6X9OrY/s640/adduser.png\" width=\"640\"></a>\n</div>\n<br><b><br></b>\n<br><div>\n<b>Todo</b>: Create new database user and password. Once you create the <b>user name</b> and <b>user password</b>, edit the connection string for the user, password, and database name. </div>\n<br><i>connection string format</i><br><pre></pre>\n<blockquote class=\"tr_bq\">\nmongodb://USER:PASSWORD@URL:PORT,URL2:PORT2/DATABASENAME?ssl=true</blockquote>\n<i><br></i>\n<i>connection string example</i><br><pre></pre>\n<blockquote class=\"tr_bq\">\nmongodb://myname:myuser@aws-us-east-1-portal.2.dblayer.com:10907,aws-us-east-1-portal.3.dblayer.com:10962/mydatabase?ssl=true</blockquote>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Change the mongodb.url setting in the /server/config.json file to this new connection string.</div>\n<br><pre>{\n \"mongodb\": {\n \"data\": \"/data/mockdata.json\",\n \"url\": \"mongodb://DBUSER:DBPASSWORD@aws-us-east-1-portal.2.dblayer.com:10907,aws-us-east-1-portal.3.dblayer.com:10962/DATABASE?ssl=true\",\n \"collection\": \"mockdata\",\n \"certificatefile\": \"/clientcertificate.pem\",\n \"sample\": {\n \"on\": true,\n \"size\": 5,\n \"index\": 1\n }\n }\n}\n\n</pre>\n<h3>\nStep 3: The Prototype Data</h3>\nIf you already have latitude and longitude data, or want to use the mock file included at /data/mockdata.json, you can skip this step.\n<br><br>\nUse <a href=\"https://www.mockaroo.com/\">Mockeroo</a> to generate your data. This allows you to get data, including latitude and longitude quickly and easily. Make sure to add the latitude and longitude data in json format.\n<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-seG8Giyx94g/VrO6MEgA2NI/AAAAAAAAAgg/LTjfqncB0pM/s1600/mockaroo.png\" imageanchor=\"1\"><img border=\"0\" height=\"441\" src=\"https://4.bp.blogspot.com/-seG8Giyx94g/VrO6MEgA2NI/AAAAAAAAAgg/LTjfqncB0pM/s640/mockaroo.png\" width=\"640\"></a>\n</div>\n<br><br>\nMake sure you have at least 1000 records for a good show of randomness and save the file as <b>mockdata.json</b> in the data subdirectory.\n<br><br><div>\n<b>Todo</b>: Create mock data and save to /data/mockdata.json.</div>\n<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-GGBZSvkwGHQ/VrO6R1IpKrI/AAAAAAAAAgk/xWbZmkleH6E/s1600/mockdata.png\" imageanchor=\"1\"><img border=\"0\" height=\"353\" src=\"https://3.bp.blogspot.com/-GGBZSvkwGHQ/VrO6R1IpKrI/AAAAAAAAAgk/xWbZmkleH6E/s640/mockdata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 4: Insert the Mock Data into the mockdata Collection</h3>\nThe insert.js file converts the /data/mockdata.json file into the mockdata collection in the mongoDB database.<br><br><b>Note</b>: This script uses the native MongoDB driver and the filesystem node package. The <a href=\"https://www.npmjs.com/package/mongoose\">Mongoose driver</a> can also use the ssl connection and the $sample operator. If you are using any other driver, you will need to check for both ssl and $sample. \n<br><br>\nThe configuration is kept in the /server/config.json file. Make sure it is correct for your mongoDB url, user, password, database name, collection name and mock data file location. The configuration is read in and stored in the privateconfig variable of the insert.js script. \n<br><br>\nThe <b>mongos</b> section of the config variable is for the SSL mongoDB connection. You shouldn't need to change any values.\n<br><br><b>insert.js</b><br><b><br></b>\n<br><pre>var MongoClient = require('mongodb').MongoClient, \n fs = require('fs'),\n path = require('path');\n\nvar privateconfig = require(path.join(__dirname + '/config.json'));\nvar ca = [fs.readFileSync(path.join(__dirname + privateconfig.mongodb.certificatefile))];\nvar data = fs.readFileSync(path.join(__dirname + privateconfig.mongodb.data), 'utf8');\nvar json = JSON.parse(data);\n\nMongoClient.connect(privateconfig.mongodb.url, {\n mongos: {\n ssl: true,\n sslValidate: true,\n sslCA: ca,\n poolSize: 1,\n reconnectTries: 1\n },\n}, function (err, db) {\n if (err) {\n console.log(err);\n } else {\n console.log(\"connected\");\n db.collection(privateconfig.mongodb.collection).insert(json, function (err, collection) {\n if (err) console.log((err));\n db.close();\n console.log('finished');\n }); \n } \n});\n\n</pre>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Run the insert script.</div>\n<pre></pre>\n<blockquote class=\"tr_bq\">\nnode insert.js</blockquote>\nIf you create an SSL database but don't pass the certificate, you won't be able to connect to it. You will get a <b>sockets closed</b> error. \n<br><br>\nOnce you run the script, make sure you can see the documents in the database's <b>mockdata</b> collection.<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-lgyQUtGf5Jw/VrO6nBg3-4I/AAAAAAAAAgo/HDz0dOvjs2M/s1600/insertdata.png\" imageanchor=\"1\"><img border=\"0\" height=\"299\" src=\"https://4.bp.blogspot.com/-lgyQUtGf5Jw/VrO6nBg3-4I/AAAAAAAAAgo/HDz0dOvjs2M/s640/insertdata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 5: Convert latitude and longitude from string to floats</h3>\nThe mock data's latitude and longitude are strings. Use the <b>update.js</b> file to convert the strings to floats as well as create the geojson values. \n<br><br><b>update.js</b><br><pre>var MongoClient = require('mongodb').MongoClient, \n fs = require('fs'),\n path = require('path');\n\nvar privateconfig = require(path.join(__dirname + '/config.json'));\nvar ca = [fs.readFileSync(path.join(__dirname + privateconfig.mongodb.certificatefile))];\n\nMongoClient.connect(privateconfig.mongodb.url, {\n mongos: {\n ssl: true,\n sslValidate: true,\n sslCA: ca,\n poolSize: 1,\n reconnectTries: 1\n },\n}, function (err, db) {\n if (err) console.log(err);\n if (db) console.log(\"connected\");\n \n db.collection(privateconfig.mongodb.collection).find().each(function(err, doc) { \n if (doc){\n \n console.log(doc.latitude + \",\" + doc.longitude);\n \n var numericLat = parseFloat(doc.latitude);\n var numericLon = parseFloat(doc.longitude);\n \n doc.latitude = numericLat;\n doc.longitude = numericLon;\n doc.geojson= { location: { type: 'Point', coordinates : [numericLat, numericLon]}}; // convert field to string\n db.collection(privateconfig.mongodb.collection).save(doc);\n \n } else {\n db.close();\n }\n });\n console.log('finished');\n});\n\n</pre>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Run the insert script</div>\n<blockquote class=\"tr_bq\">\nnode update.js</blockquote>\nOnce you run the script, make sure you can see the documents in the database's <b>mockdata</b> collection with the updated values.\n<br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-3yQ6aXIUXTw/VrPEzLjTcTI/AAAAAAAAAg8/ORaL_gOpl04/s1600/composeiodata.png\" imageanchor=\"1\"><img border=\"0\" height=\"316\" src=\"https://2.bp.blogspot.com/-3yQ6aXIUXTw/VrPEzLjTcTI/AAAAAAAAAg8/ORaL_gOpl04/s640/composeiodata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 6: Verify world map displays points of latitude and longitude</h3>\nRefresh the website several times. This should show different points each time. The variation of randomness should catch your eye. Is it widely random, or not as widely random as you would like?\n<br><b><br></b>\n<br><div>\n<b>Todo</b>: Refresh several times</div>\n<blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=5\">http://127.0.0.1:8080/map/?rows=5</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-mG564Vi_P0s/VrPGrmol5vI/AAAAAAAAAhM/AR5JzfRYonw/s1600/step6-5pts.png\" imageanchor=\"1\"><img border=\"0\" height=\"324\" src=\"https://2.bp.blogspot.com/-mG564Vi_P0s/VrPGrmol5vI/AAAAAAAAAhM/AR5JzfRYonw/s640/step6-5pts.png\" width=\"640\"></a>\n</div>\n<br><br><a href=\"http://127.0.0.1:8080/map/?rows=5\">The warning of the </a><a href=\"https://docs.mongodb.org/manual/reference/operator/aggregation/sample/\">$sample behavior</a> says the data may duplicate within a single query. On this map that would appear as less than the number of requested data points. Did you see that in your tests? \n<br><h2>\nHow $sample impacts the results</h2>\nNow that the website works, let's play with it to see how $sample impacts the results.\n<br><ol>\n<li>understand the $sample code in /server/query.js</li>\n<li>change the row count</li>\n<li>change the aggregation pipeline order</li>\n<li>prototype with $sample</li>\n</ol>\n<h3>\nStep 1: Understand the $sample operator in /server/query.js</h3>\nThe <a href=\"https://docs.mongodb.org/manual/reference/operator/aggregation/sample/#pipe._S_sample\">$sample</a> operator controls random sampling of the query in the <a href=\"https://docs.mongodb.org/manual/core/aggregation-pipeline/\">aggregation pipeline</a>. \n<br>\nThe pipeline used in this article is a series of array elements in the <b>arrangeAggregationPipeline</b> function in the /server/query.js file. The first array element is the $project section which controls what data to return. \n<br><br><b>arrangeAggregationPipeline()</b>\n<br><b><br></b>\n<br><pre> \n \nvar aggregationPipeItems = [\n { $project: \n {\n last: \"$last_name\",\n first: \"$first_name\",\n lat: \"$latitude\",\n lon: \"$longitude\",\n Location: [\"$latitude\", \"$longitude\"],\n _id:0 \n }\n },\n { $sort: {'last': 1}} // sort by last name\n\n</pre>\n<br>\nThe next step in the pipeline is the sorting of the data by last name. If the pipeline runs this way (without $sample), all documents are returned and sorted by last name.\n <br><br>\nThe location of $sample is controlled by the pos value in the url. If pos isn't set, the position defaults to 1. If it is set to 1 of the zero-based array, it will be applied between $project and $sort, at the second position. If the code runs as supplied, the set of data is randomized, documents are selected, then the rows are sorted. This would be meaningful in both that the data is random, and returned sorted. \n<br><br><b>Note</b>: In order for random sampling to work, you must use it in connection with 'rows' in the query string.\n<br><br>\nWe will play with the position in step 3. \n<br><h3>\nStep 2: Change the row count</h3>\nThe count of rows is a parameter in the url to the server, when the data is requested. Change the url to indicate 10 rows returned.\n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<br><blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=10\">http://127.0.0.1:8080/map/?rows=10</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-EtGvDcchn-8/VrPJKS8oHiI/AAAAAAAAAhg/mtpKWQwvVU8/s1600/worldmap10datapoints.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://1.bp.blogspot.com/-EtGvDcchn-8/VrPJKS8oHiI/AAAAAAAAAhg/mtpKWQwvVU8/s640/worldmap10datapoints.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<br><h3>\nStep 3: Change the aggregation pipeline order</h3>\nThe aggregation pipeline order is a parameter in the url to the server. You can control it with the 'pos' name/value pair. The following url is the same as Step 2 but the aggregation pipeline index is explicitly set. \n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=1\">http://127.0.0.1:8080/map/?rows=10&amp;pos=1</a>\n</blockquote>\n<br><b>Note</b>: Only 0, 1, and 2 are valid values<br><br><div class=\"separator\">\n</div>\n<br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-eW1xMCe5Np4/VrPJoR0ednI/AAAAAAAAAho/kgsMwrZQwDo/s1600/row10pos1.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://1.bp.blogspot.com/-eW1xMCe5Np4/VrPJoR0ednI/AAAAAAAAAho/kgsMwrZQwDo/s640/row10pos1.png\" width=\"640\"></a>\n</div>\n<br><br>\nThe results below the map should be sorted. \n<br><br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-uWuw2_ZBFb0/VrPJinb9qMI/AAAAAAAAAhk/eez6hnmTdZQ/s1600/row10pos1data.png\" imageanchor=\"1\"><img border=\"0\" height=\"180\" src=\"https://1.bp.blogspot.com/-uWuw2_ZBFb0/VrPJinb9qMI/AAAAAAAAAhk/eez6hnmTdZQ/s640/row10pos1data.png\" width=\"640\"></a>\n</div>\n<br><br>\nIf the $sample position is moved to the 0 position, still before the sort is applied, the browser shows the same result. \n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=0\">http://127.0.0.1:8080/map/?rows=10&amp;pos=0</a><br><br>\nBut, however, if the $sample is the last item (pos=2), the entire set is sorted, then 5 rows are selected. The results are no longer sorted.\n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied before</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=2\">http://127.0.0.1:8080/map/?rows=10&amp;pos=2</a>\n\n<img src=\"ttps://github.com/dfberry/mongosample/blob/master/public/images/rows10pos2data.png\"><br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-6QvMTzrrerw/VrPKCqwCBiI/AAAAAAAAAh0/SpG-1S6IAX4/s1600/rows10pos2data.png\" imageanchor=\"1\"><img border=\"0\" height=\"184\" src=\"https://2.bp.blogspot.com/-6QvMTzrrerw/VrPKCqwCBiI/AAAAAAAAAh0/SpG-1S6IAX4/s640/rows10pos2data.png\" width=\"640\"></a>\n</div>\n<br><br>\nNote that while the documents are returned, they are not in sorted order. \n<br><br>\nIf they are in sorted order, it isn't because they were sorted, but because the random pick happened that way on accident, not on purpose. \n<br><h2>\nStep 4: Prototype with $sample</h2>\nThe mongoDB $sample operator is a great way to to try out a visual design without needing all the data. At the early stage of the design, a quick visual can give you an idea if you are on the right path.\n<br><br>\nThe map with data points works well for 5 or 10 points but what about 50 or 100?\n<br><br><div>\n<b>Todo</b>: request 500 rows</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=500\">http://127.0.0.1:8080/map/?rows=500</a><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-0AiJdVWxQDo/VrPKUbH6RsI/AAAAAAAAAh4/FtK8HpfUg-8/s1600/rows500.png\" imageanchor=\"1\"><img border=\"0\" height=\"472\" src=\"https://3.bp.blogspot.com/-0AiJdVWxQDo/VrPKUbH6RsI/AAAAAAAAAh4/FtK8HpfUg-8/s640/rows500.png\" width=\"640\"></a>\n</div>\n<br><br>\nThe visual appeal and much of the meaning of the data is lost in the mess of the map. Change the size of the points on the map.\n<br><br><div>\n<b>Todo</b>: request 500 rows, with smaller points on the map using 'radius' name/value pair</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=500&amp;radius=2\">http://127.0.0.1:8080/map/?rows=500&amp;radius=2</a><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-7Zt3N6M-96Q/VrPKe6EQhnI/AAAAAAAAAiA/9I9GbqR921M/s1600/rows500radius2.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://3.bp.blogspot.com/-7Zt3N6M-96Q/VrPKe6EQhnI/AAAAAAAAAiA/9I9GbqR921M/s640/rows500radius2.png\" width=\"640\"></a>\n</div>\n<br><h2>\nSummary</h2>\nThe $sample aggregation pipeline operator in mongoDB is a great way to build a prototype testing with random data. Building the page so that the visual design is controlled by the query string works well for quick changes with immediate feedback. \n<br><br>\nEnjoy the new $sample operator. Leave comments about how you have or would use it.",
            "content": "\n<h1>\nPrototyping in MongoDB with the Aggregation Pipeline stage operator $sample</h1>\n<h2>\nThe World Map as a visual example</h2>\nIn order to show how the random sampling works in the mongoDB query, this NodeJS Express website will show the world map and display random latitude/longitude points on the map. Each refresh of the page will produce new random points. Below the map, the docs will display. \n<br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s1600/emptymap.png\" imageanchor=\"1\"><img border=\"0\" height=\"436\" src=\"https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s640/emptymap.png\" width=\"640\"></a>\n</div>\n<br><br>\nOnce the website is up and working with data points, we will play with the query to see how the data points change in response. \n<br><br>\nThe demonstration video is available on <a href=\"https://youtu.be/uyPmDZ8llk4\">YouTube</a>.\n<br><br><a href=\"https://youtu.be/uyPmDZ8llk4\"><img src=\"http://img.youtube.com/vi/uyPmDZ8llk4/0.jpg\" height=\"480\" width=\"640\"></a>\n\n<br><h2>\nSetup steps for the website</h2>\n<h3>\nSetup</h3>\nThis article assumes you have no mongoDB, no website, and no data. It does assume you have an account on <a href=\"http://www.compose.io/\">Compose</a>. Each step is broken out and explained. If there is a step you already have, such as the mongoDB with latitude/longitude data or a website that displays it, skip to the next. \n<br><ol>\n<li>get website running, display map with no data</li>\n<li>setup the mongoDB+ ssl database</li>\n<li>get mock data including latitude and longitude</li>\n<li>insert the mock data into database</li>\n<li>update database data types </li>\n<li>verify world map displays data points</li>\n</ol>\n<h3>\nPlay</h3>\nWhen the website works and the world map displays data points, let's play with it to see how $sample impacts the results.\n<br><ol>\n<li>understand the $sample operator</li>\n<li>change the row count</li>\n<li>change the aggregation pipeline order</li>\n<li>prototype with $sample</li>\n</ol>\n<h3>\nSystem architecture</h3>\nThe <b>data import script</b> is /insert.js. It opens and inserts a json file into a mongoDB collection. It doesn't do any transformation. \n<br><br>\nThe <b>data update script</b> is /update.js. It updates the data to numeric and geojson types.\n<br><br>\nThe <b>server</b> is a nodeJs Express website using the native MongoDB driver. The code uses the filesystem, url, and path libraries. This is a bare-bones express website. The /server/server.js file is the web server, with /server/query.js as the database layer. The server runs at http://127.0.0.1:8080/map/. This address is routed to /public/highmap/world.highmap.html. The data query will be made to http://127.0.0.1:8080/map/data/ from the client file /public/highmap/world.highmap.js. \n<br><br>\nThe <b>client</b> files are in the /public directory. The main web file is /highmap/world.highmap.html. It uses jQuery as the javascript framework, and <a href=\"http://www.highcharts.com/maps/demo\">highmap</a> as the mapping library which plots the points on the world map. The size of the map is controlled by the /public/highmap/world.highmap.css stylesheet for the map id. \n<br><h3>\nStep 1: The NodeJS Express Website</h3>\nIn order to get the website up and going, you need to clone this repository, make sure nodeJS is installed, and install the dependency libraries found in the package.json file. \n<br><br><div>\n<b>Todo</b>: install dependencies</div>\n<br><blockquote class=\"tr_bq\">\nnpm install</blockquote>\n<br>\nOnce the dependencies are installed, you can start the web server.\n<br><br><div>\n<b>Todo</b>: start website</div>\n<br><blockquote class=\"tr_bq\">\nnpm start</blockquote>\n<br><div>\n<b>Todo</b>: Request the website to see the world map. The map should display successfully with no data points.\n</div>\n<br><blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/\">http://127.0.0.1:8080/map/</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s1600/emptymap.png\" imageanchor=\"1\"><img border=\"0\" height=\"436\" src=\"https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s640/emptymap.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 2: Setup the <a href=\"http://compose.io/\">Compose</a> MongoDB+ Deployment and Database</h3>\nYou can move on to the next section, if you have a mongoDB deployment with SSL to use, and have the following items:\n<br><ul>\n<li>deployment public SSL key in the /server/clientcertificate.pem file\n</li>\n<li>connection string for that deployment in /server/config.json \n</li>\n</ul>\n<div>\n<b>Todo</b>: Create a new deployment on Compose for a MongoDB+ database with an SSL connection. </div>\n<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-WQyDeHwDGwM/VrO5XsJe_vI/AAAAAAAAAgM/0itG3a-PeYY/s1600/mongoDB%252BSSL.png\" imageanchor=\"1\"><img border=\"0\" height=\"308\" src=\"https://4.bp.blogspot.com/-WQyDeHwDGwM/VrO5XsJe_vI/AAAAAAAAAgM/0itG3a-PeYY/s640/mongoDB%252BSSL.png\" width=\"640\"></a>\n</div>\n<br><br>\nWhile still on the <a href=\"http://compose.io/\">Compose</a> backoffice, open the new deployment and copy the connection string. \n<br><br><div>\n<b>Todo</b>: Copy connection string </div>\n<br>\nYou will need the <b>entire connection string</b> in order to insert, update, and query the data. The connection string uses a user and password at the beginning and the database name at the end.\n<br><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-QQXZg9b0Dus/VrO5mRtcHzI/AAAAAAAAAgQ/yjNxPVjr7DE/s1600/composeio-ssl.png\" imageanchor=\"1\"><img border=\"0\" height=\"360\" src=\"https://3.bp.blogspot.com/-QQXZg9b0Dus/VrO5mRtcHzI/AAAAAAAAAgQ/yjNxPVjr7DE/s640/composeio-ssl.png\" width=\"640\"></a>\n</div>\n<br><br>\nYou also need to get the SSL Public key from the <a href=\"http://compose.io/\">Compose</a> Deployment Overview page. You will need to login with your <a href=\"http://www.compose.io/\">Compose</a> user password in order for the public key to show. \n<br><br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-BhkZqThglLo/VrO5sUpSQEI/AAAAAAAAAgU/gRc0pOedm-s/s1600/composeiosslpublickey.png\" imageanchor=\"1\"><img border=\"0\" height=\"283\" src=\"https://1.bp.blogspot.com/-BhkZqThglLo/VrO5sUpSQEI/AAAAAAAAAgU/gRc0pOedm-s/s640/composeiosslpublickey.png\" width=\"640\"></a>\n</div>\n<br><br><div>\n<b>Todo</b>: Save the entire <b>SSL Public key</b> to /server/clientcertificate.pem. </div>\n<br>\nIf you save it somewhere else, you need to change the mongodb.certificatefile setting in /server/config.json.\n<br><br>\nYou will also need to create a user in the Deployment's database. \n<br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-lwzNOrvWtSs/VrO55auLHdI/AAAAAAAAAgY/y0bwn6X9OrY/s1600/adduser.png\" imageanchor=\"1\"><img border=\"0\" height=\"243\" src=\"https://2.bp.blogspot.com/-lwzNOrvWtSs/VrO55auLHdI/AAAAAAAAAgY/y0bwn6X9OrY/s640/adduser.png\" width=\"640\"></a>\n</div>\n<br><b><br></b>\n<br><div>\n<b>Todo</b>: Create new database user and password. Once you create the <b>user name</b> and <b>user password</b>, edit the connection string for the user, password, and database name. </div>\n<br><i>connection string format</i><br><pre></pre>\n<blockquote class=\"tr_bq\">\nmongodb://USER:PASSWORD@URL:PORT,URL2:PORT2/DATABASENAME?ssl=true</blockquote>\n<i><br></i>\n<i>connection string example</i><br><pre></pre>\n<blockquote class=\"tr_bq\">\nmongodb://myname:myuser@aws-us-east-1-portal.2.dblayer.com:10907,aws-us-east-1-portal.3.dblayer.com:10962/mydatabase?ssl=true</blockquote>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Change the mongodb.url setting in the /server/config.json file to this new connection string.</div>\n<br><pre>{\n \"mongodb\": {\n \"data\": \"/data/mockdata.json\",\n \"url\": \"mongodb://DBUSER:DBPASSWORD@aws-us-east-1-portal.2.dblayer.com:10907,aws-us-east-1-portal.3.dblayer.com:10962/DATABASE?ssl=true\",\n \"collection\": \"mockdata\",\n \"certificatefile\": \"/clientcertificate.pem\",\n \"sample\": {\n \"on\": true,\n \"size\": 5,\n \"index\": 1\n }\n }\n}\n\n</pre>\n<h3>\nStep 3: The Prototype Data</h3>\nIf you already have latitude and longitude data, or want to use the mock file included at /data/mockdata.json, you can skip this step.\n<br><br>\nUse <a href=\"https://www.mockaroo.com/\">Mockeroo</a> to generate your data. This allows you to get data, including latitude and longitude quickly and easily. Make sure to add the latitude and longitude data in json format.\n<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-seG8Giyx94g/VrO6MEgA2NI/AAAAAAAAAgg/LTjfqncB0pM/s1600/mockaroo.png\" imageanchor=\"1\"><img border=\"0\" height=\"441\" src=\"https://4.bp.blogspot.com/-seG8Giyx94g/VrO6MEgA2NI/AAAAAAAAAgg/LTjfqncB0pM/s640/mockaroo.png\" width=\"640\"></a>\n</div>\n<br><br>\nMake sure you have at least 1000 records for a good show of randomness and save the file as <b>mockdata.json</b> in the data subdirectory.\n<br><br><div>\n<b>Todo</b>: Create mock data and save to /data/mockdata.json.</div>\n<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-GGBZSvkwGHQ/VrO6R1IpKrI/AAAAAAAAAgk/xWbZmkleH6E/s1600/mockdata.png\" imageanchor=\"1\"><img border=\"0\" height=\"353\" src=\"https://3.bp.blogspot.com/-GGBZSvkwGHQ/VrO6R1IpKrI/AAAAAAAAAgk/xWbZmkleH6E/s640/mockdata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 4: Insert the Mock Data into the mockdata Collection</h3>\nThe insert.js file converts the /data/mockdata.json file into the mockdata collection in the mongoDB database.<br><br><b>Note</b>: This script uses the native MongoDB driver and the filesystem node package. The <a href=\"https://www.npmjs.com/package/mongoose\">Mongoose driver</a> can also use the ssl connection and the $sample operator. If you are using any other driver, you will need to check for both ssl and $sample. \n<br><br>\nThe configuration is kept in the /server/config.json file. Make sure it is correct for your mongoDB url, user, password, database name, collection name and mock data file location. The configuration is read in and stored in the privateconfig variable of the insert.js script. \n<br><br>\nThe <b>mongos</b> section of the config variable is for the SSL mongoDB connection. You shouldn't need to change any values.\n<br><br><b>insert.js</b><br><b><br></b>\n<br><pre>var MongoClient = require('mongodb').MongoClient, \n fs = require('fs'),\n path = require('path');\n\nvar privateconfig = require(path.join(__dirname + '/config.json'));\nvar ca = [fs.readFileSync(path.join(__dirname + privateconfig.mongodb.certificatefile))];\nvar data = fs.readFileSync(path.join(__dirname + privateconfig.mongodb.data), 'utf8');\nvar json = JSON.parse(data);\n\nMongoClient.connect(privateconfig.mongodb.url, {\n mongos: {\n ssl: true,\n sslValidate: true,\n sslCA: ca,\n poolSize: 1,\n reconnectTries: 1\n },\n}, function (err, db) {\n if (err) {\n console.log(err);\n } else {\n console.log(\"connected\");\n db.collection(privateconfig.mongodb.collection).insert(json, function (err, collection) {\n if (err) console.log((err));\n db.close();\n console.log('finished');\n }); \n } \n});\n\n</pre>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Run the insert script.</div>\n<pre></pre>\n<blockquote class=\"tr_bq\">\nnode insert.js</blockquote>\nIf you create an SSL database but don't pass the certificate, you won't be able to connect to it. You will get a <b>sockets closed</b> error. \n<br><br>\nOnce you run the script, make sure you can see the documents in the database's <b>mockdata</b> collection.<br><br><div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-lgyQUtGf5Jw/VrO6nBg3-4I/AAAAAAAAAgo/HDz0dOvjs2M/s1600/insertdata.png\" imageanchor=\"1\"><img border=\"0\" height=\"299\" src=\"https://4.bp.blogspot.com/-lgyQUtGf5Jw/VrO6nBg3-4I/AAAAAAAAAgo/HDz0dOvjs2M/s640/insertdata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 5: Convert latitude and longitude from string to floats</h3>\nThe mock data's latitude and longitude are strings. Use the <b>update.js</b> file to convert the strings to floats as well as create the geojson values. \n<br><br><b>update.js</b><br><pre>var MongoClient = require('mongodb').MongoClient, \n fs = require('fs'),\n path = require('path');\n\nvar privateconfig = require(path.join(__dirname + '/config.json'));\nvar ca = [fs.readFileSync(path.join(__dirname + privateconfig.mongodb.certificatefile))];\n\nMongoClient.connect(privateconfig.mongodb.url, {\n mongos: {\n ssl: true,\n sslValidate: true,\n sslCA: ca,\n poolSize: 1,\n reconnectTries: 1\n },\n}, function (err, db) {\n if (err) console.log(err);\n if (db) console.log(\"connected\");\n \n db.collection(privateconfig.mongodb.collection).find().each(function(err, doc) { \n if (doc){\n \n console.log(doc.latitude + \",\" + doc.longitude);\n \n var numericLat = parseFloat(doc.latitude);\n var numericLon = parseFloat(doc.longitude);\n \n doc.latitude = numericLat;\n doc.longitude = numericLon;\n doc.geojson= { location: { type: 'Point', coordinates : [numericLat, numericLon]}}; // convert field to string\n db.collection(privateconfig.mongodb.collection).save(doc);\n \n } else {\n db.close();\n }\n });\n console.log('finished');\n});\n\n</pre>\n<b><br></b>\n<br><div>\n<b>Todo</b>: Run the insert script</div>\n<blockquote class=\"tr_bq\">\nnode update.js</blockquote>\nOnce you run the script, make sure you can see the documents in the database's <b>mockdata</b> collection with the updated values.\n<br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-3yQ6aXIUXTw/VrPEzLjTcTI/AAAAAAAAAg8/ORaL_gOpl04/s1600/composeiodata.png\" imageanchor=\"1\"><img border=\"0\" height=\"316\" src=\"https://2.bp.blogspot.com/-3yQ6aXIUXTw/VrPEzLjTcTI/AAAAAAAAAg8/ORaL_gOpl04/s640/composeiodata.png\" width=\"640\"></a>\n</div>\n<br><h3>\nStep 6: Verify world map displays points of latitude and longitude</h3>\nRefresh the website several times. This should show different points each time. The variation of randomness should catch your eye. Is it widely random, or not as widely random as you would like?\n<br><b><br></b>\n<br><div>\n<b>Todo</b>: Refresh several times</div>\n<blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=5\">http://127.0.0.1:8080/map/?rows=5</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-mG564Vi_P0s/VrPGrmol5vI/AAAAAAAAAhM/AR5JzfRYonw/s1600/step6-5pts.png\" imageanchor=\"1\"><img border=\"0\" height=\"324\" src=\"https://2.bp.blogspot.com/-mG564Vi_P0s/VrPGrmol5vI/AAAAAAAAAhM/AR5JzfRYonw/s640/step6-5pts.png\" width=\"640\"></a>\n</div>\n<br><br><a href=\"http://127.0.0.1:8080/map/?rows=5\">The warning of the </a><a href=\"https://docs.mongodb.org/manual/reference/operator/aggregation/sample/\">$sample behavior</a> says the data may duplicate within a single query. On this map that would appear as less than the number of requested data points. Did you see that in your tests? \n<br><h2>\nHow $sample impacts the results</h2>\nNow that the website works, let's play with it to see how $sample impacts the results.\n<br><ol>\n<li>understand the $sample code in /server/query.js</li>\n<li>change the row count</li>\n<li>change the aggregation pipeline order</li>\n<li>prototype with $sample</li>\n</ol>\n<h3>\nStep 1: Understand the $sample operator in /server/query.js</h3>\nThe <a href=\"https://docs.mongodb.org/manual/reference/operator/aggregation/sample/#pipe._S_sample\">$sample</a> operator controls random sampling of the query in the <a href=\"https://docs.mongodb.org/manual/core/aggregation-pipeline/\">aggregation pipeline</a>. \n<br>\nThe pipeline used in this article is a series of array elements in the <b>arrangeAggregationPipeline</b> function in the /server/query.js file. The first array element is the $project section which controls what data to return. \n<br><br><b>arrangeAggregationPipeline()</b>\n<br><b><br></b>\n<br><pre> \n \nvar aggregationPipeItems = [\n { $project: \n {\n last: \"$last_name\",\n first: \"$first_name\",\n lat: \"$latitude\",\n lon: \"$longitude\",\n Location: [\"$latitude\", \"$longitude\"],\n _id:0 \n }\n },\n { $sort: {'last': 1}} // sort by last name\n\n</pre>\n<br>\nThe next step in the pipeline is the sorting of the data by last name. If the pipeline runs this way (without $sample), all documents are returned and sorted by last name.\n <br><br>\nThe location of $sample is controlled by the pos value in the url. If pos isn't set, the position defaults to 1. If it is set to 1 of the zero-based array, it will be applied between $project and $sort, at the second position. If the code runs as supplied, the set of data is randomized, documents are selected, then the rows are sorted. This would be meaningful in both that the data is random, and returned sorted. \n<br><br><b>Note</b>: In order for random sampling to work, you must use it in connection with 'rows' in the query string.\n<br><br>\nWe will play with the position in step 3. \n<br><h3>\nStep 2: Change the row count</h3>\nThe count of rows is a parameter in the url to the server, when the data is requested. Change the url to indicate 10 rows returned.\n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<br><blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=10\">http://127.0.0.1:8080/map/?rows=10</a>\n</blockquote>\n<br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-EtGvDcchn-8/VrPJKS8oHiI/AAAAAAAAAhg/mtpKWQwvVU8/s1600/worldmap10datapoints.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://1.bp.blogspot.com/-EtGvDcchn-8/VrPJKS8oHiI/AAAAAAAAAhg/mtpKWQwvVU8/s640/worldmap10datapoints.png\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<br><h3>\nStep 3: Change the aggregation pipeline order</h3>\nThe aggregation pipeline order is a parameter in the url to the server. You can control it with the 'pos' name/value pair. The following url is the same as Step 2 but the aggregation pipeline index is explicitly set. \n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<blockquote class=\"tr_bq\">\n<a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=1\">http://127.0.0.1:8080/map/?rows=10&amp;pos=1</a>\n</blockquote>\n<br><b>Note</b>: Only 0, 1, and 2 are valid values<br><br><div class=\"separator\">\n</div>\n<br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-eW1xMCe5Np4/VrPJoR0ednI/AAAAAAAAAho/kgsMwrZQwDo/s1600/row10pos1.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://1.bp.blogspot.com/-eW1xMCe5Np4/VrPJoR0ednI/AAAAAAAAAho/kgsMwrZQwDo/s640/row10pos1.png\" width=\"640\"></a>\n</div>\n<br><br>\nThe results below the map should be sorted. \n<br><br><div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-uWuw2_ZBFb0/VrPJinb9qMI/AAAAAAAAAhk/eez6hnmTdZQ/s1600/row10pos1data.png\" imageanchor=\"1\"><img border=\"0\" height=\"180\" src=\"https://1.bp.blogspot.com/-uWuw2_ZBFb0/VrPJinb9qMI/AAAAAAAAAhk/eez6hnmTdZQ/s640/row10pos1data.png\" width=\"640\"></a>\n</div>\n<br><br>\nIf the $sample position is moved to the 0 position, still before the sort is applied, the browser shows the same result. \n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied after</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=0\">http://127.0.0.1:8080/map/?rows=10&amp;pos=0</a><br><br>\nBut, however, if the $sample is the last item (pos=2), the entire set is sorted, then 5 rows are selected. The results are no longer sorted.\n<br><br><div>\n<b>Todo</b>: request 10 rows, with sorting applied before</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=10&amp;pos=2\">http://127.0.0.1:8080/map/?rows=10&amp;pos=2</a>\n\n<img src=\"ttps://github.com/dfberry/mongosample/blob/master/public/images/rows10pos2data.png\"><br><br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-6QvMTzrrerw/VrPKCqwCBiI/AAAAAAAAAh0/SpG-1S6IAX4/s1600/rows10pos2data.png\" imageanchor=\"1\"><img border=\"0\" height=\"184\" src=\"https://2.bp.blogspot.com/-6QvMTzrrerw/VrPKCqwCBiI/AAAAAAAAAh0/SpG-1S6IAX4/s640/rows10pos2data.png\" width=\"640\"></a>\n</div>\n<br><br>\nNote that while the documents are returned, they are not in sorted order. \n<br><br>\nIf they are in sorted order, it isn't because they were sorted, but because the random pick happened that way on accident, not on purpose. \n<br><h2>\nStep 4: Prototype with $sample</h2>\nThe mongoDB $sample operator is a great way to to try out a visual design without needing all the data. At the early stage of the design, a quick visual can give you an idea if you are on the right path.\n<br><br>\nThe map with data points works well for 5 or 10 points but what about 50 or 100?\n<br><br><div>\n<b>Todo</b>: request 500 rows</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=500\">http://127.0.0.1:8080/map/?rows=500</a><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-0AiJdVWxQDo/VrPKUbH6RsI/AAAAAAAAAh4/FtK8HpfUg-8/s1600/rows500.png\" imageanchor=\"1\"><img border=\"0\" height=\"472\" src=\"https://3.bp.blogspot.com/-0AiJdVWxQDo/VrPKUbH6RsI/AAAAAAAAAh4/FtK8HpfUg-8/s640/rows500.png\" width=\"640\"></a>\n</div>\n<br><br>\nThe visual appeal and much of the meaning of the data is lost in the mess of the map. Change the size of the points on the map.\n<br><br><div>\n<b>Todo</b>: request 500 rows, with smaller points on the map using 'radius' name/value pair</div>\n<br><a href=\"http://127.0.0.1:8080/map/?rows=500&amp;radius=2\">http://127.0.0.1:8080/map/?rows=500&amp;radius=2</a><br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-7Zt3N6M-96Q/VrPKe6EQhnI/AAAAAAAAAiA/9I9GbqR921M/s1600/rows500radius2.png\" imageanchor=\"1\"><img border=\"0\" height=\"470\" src=\"https://3.bp.blogspot.com/-7Zt3N6M-96Q/VrPKe6EQhnI/AAAAAAAAAiA/9I9GbqR921M/s640/rows500radius2.png\" width=\"640\"></a>\n</div>\n<br><h2>\nSummary</h2>\nThe $sample aggregation pipeline operator in mongoDB is a great way to build a prototype testing with random data. Building the page so that the visual design is controlled by the query string works well for quick changes with immediate feedback. \n<br><br>\nEnjoy the new $sample operator. Leave comments about how you have or would use it.",
            "enclosure": {
                "thumbnail": "https://3.bp.blogspot.com/-Dca9QU1IIZs/VrO49HbaJ5I/AAAAAAAAAgI/gWkSYcchH4I/s72-c/emptymap.png"
            },
            "categories": [
                "data",
                "ssl",
                "jquery",
                "json",
                "Random",
                "Dina",
                "Compose",
                "geojson",
                "highmap",
                "mongo",
                "mongod",
                "Node",
                "sample"
            ]
        },
        {
            "title": "Frugal Cloud The In-Memory versus SSD Paging File",
            "pubDate": "2016-01-21 17:46:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/frugal-cloud-in-memory-versus-ssd.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-6087013630939007145",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s72-c/Pagingfile.PNG",
            "description": "Many people remember the days when you could use a USB memory stick to boost the performance of Windows. This memory caused me to ask the question: Is there a potential cost saving with little performance impact by going sparse on physical memory and configuring a paging file.<br><br>\nFor windows folks:<br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s1600/Pagingfile.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"316\" src=\"http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s400/Pagingfile.PNG\" width=\"400\"></a>\n</div>\n<div class=\"separator\">\nFor Linux, see <a href=\"http://www.techknowjoe.com/article/create-swap-file-your-ec2-instance-storage-space\" target=\"_blank\">TechTalk Joe post</a>. <b>Note</b> his \"<span>I/O requests on instance storage does not incur a cost. Only EBS volumes have I/O request charges.\" so it is not recommended to do if you are running with EBS only.</span>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThis approach is particularly significant when you are \"just over\" one the offering levels. </div>\n<div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-GXWsrE7-Tn0/VqEVG35wteI/AAAAAAAAJA0/HC9SvSnh3sw/s1600/offering.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"111\" src=\"http://3.bp.blogspot.com/-GXWsrE7-Tn0/VqEVG35wteI/AAAAAAAAJA0/HC9SvSnh3sw/s400/offering.PNG\" width=\"400\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\n<br>\n</div>\nFor some configurations, you will not get a CPU boost - by using the paging files. I know recent experience with a commercial SAAS actually had high memory usage but very log CPU (3-5%, even during peak times!). Having 1/2 or even 1/4 the CPUs would not peg the CPU. The question then becomes whether the Paging File on a SSD drive would significantly drop performance (whether you can strip for extra performance across multiple SSD on cloud instances, is an interesting question). This is a question that can only be determined experimentally.<br><br><ul>\n<li>How the paging file is configured and the actual usage of memory by the application is key. Often 80-90% of the usage hits only 10% of the memory (Pareto rule). The result could be that the median (50%ile) time may be unchanged -- and time may increase only along the long tail of the response distribution (say top 3% may be longer).</li>\n</ul>\n<div>\nThese factors cannot be academically determined. They need to be determine experimentally.</div>\n<div>\n<br>\n</div>\n<div>\nIf performance is acceptable, there is an immediate cost saving because when new instances are created due to load, they are cheaper instances.</div>\n<div>\n<br>\n</div>\n<div>\n<b>Bottom line is always:</b> Experiment,stress, time and compare cost. Between the pricing models, OS behaviors and application behaviors, there is no safe rule of thumb!<br><br><b>Second Rule</b>: Always define SLA as the Median (50%-ile) and never as an average.  Web responses are long-tailed which makes the average(mean) very volatile. The median is usually very very stable.</div>\n",
            "content": "Many people remember the days when you could use a USB memory stick to boost the performance of Windows. This memory caused me to ask the question: Is there a potential cost saving with little performance impact by going sparse on physical memory and configuring a paging file.<br><br>\nFor windows folks:<br><br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s1600/Pagingfile.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"316\" src=\"http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s400/Pagingfile.PNG\" width=\"400\"></a>\n</div>\n<div class=\"separator\">\nFor Linux, see <a href=\"http://www.techknowjoe.com/article/create-swap-file-your-ec2-instance-storage-space\" target=\"_blank\">TechTalk Joe post</a>. <b>Note</b> his \"<span>I/O requests on instance storage does not incur a cost. Only EBS volumes have I/O request charges.\" so it is not recommended to do if you are running with EBS only.</span>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThis approach is particularly significant when you are \"just over\" one the offering levels. </div>\n<div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-GXWsrE7-Tn0/VqEVG35wteI/AAAAAAAAJA0/HC9SvSnh3sw/s1600/offering.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"111\" src=\"http://3.bp.blogspot.com/-GXWsrE7-Tn0/VqEVG35wteI/AAAAAAAAJA0/HC9SvSnh3sw/s400/offering.PNG\" width=\"400\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\n<br>\n</div>\nFor some configurations, you will not get a CPU boost - by using the paging files. I know recent experience with a commercial SAAS actually had high memory usage but very log CPU (3-5%, even during peak times!). Having 1/2 or even 1/4 the CPUs would not peg the CPU. The question then becomes whether the Paging File on a SSD drive would significantly drop performance (whether you can strip for extra performance across multiple SSD on cloud instances, is an interesting question). This is a question that can only be determined experimentally.<br><br><ul>\n<li>How the paging file is configured and the actual usage of memory by the application is key. Often 80-90% of the usage hits only 10% of the memory (Pareto rule). The result could be that the median (50%ile) time may be unchanged -- and time may increase only along the long tail of the response distribution (say top 3% may be longer).</li>\n</ul>\n<div>\nThese factors cannot be academically determined. They need to be determine experimentally.</div>\n<div>\n<br>\n</div>\n<div>\nIf performance is acceptable, there is an immediate cost saving because when new instances are created due to load, they are cheaper instances.</div>\n<div>\n<br>\n</div>\n<div>\n<b>Bottom line is always:</b> Experiment,stress, time and compare cost. Between the pricing models, OS behaviors and application behaviors, there is no safe rule of thumb!<br><br><b>Second Rule</b>: Always define SLA as the Median (50%-ile) and never as an average.  Web responses are long-tailed which makes the average(mean) very volatile. The median is usually very very stable.</div>\n",
            "enclosure": {
                "thumbnail": "http://3.bp.blogspot.com/-bAERrW3ekoQ/VqEUEPriIcI/AAAAAAAAJAs/znSl86BsJak/s72-c/Pagingfile.PNG"
            },
            "categories": []
        },
        {
            "title": "Sharding Cloud Instances",
            "pubDate": "2016-01-17 18:21:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/sharding-cloud-instances.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-4258196404373176866",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "Database sharding has been with us for many years. The concept of cloud instance sharding has not been discussed much. There is a significant financial incentive to do.<br><br>\nConsider a component that provides address and/or postal code validation around the world. For the sake of illustration, let us consider 10 regions that each have the same volume of data.<br><br>\nInitial tests found that it took 25 GB of data to load all of them in memory. Working of <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">AWS EC2 price list</a>, we find that a m4.2xlarge is needed to run it, at $0.479/hr. This gives us 8 CPUs.<br><br>\nIf we run with 10 @ 2.5 GB instead, we end up with 10 t2.medium, each with 2 CPU and a cost of $0.052/hr, or $0.52/hr -- which on first impression is more expensive, except we have 20 CPUs instead of 8 CPU. We may have better performance. If one of these instances is a hot spot (like US addresses), then we may end up with  9 instances that each support one region each and perhaps 5 instances supporting the US. As a single instance model, we may need 5 instances.<br><br>\nIn this case, we could end up with<br><br><ul>\n<li>Single Instance Model: 5 * $0.479 = $2.395/hr with 40 CPU</li>\n<li>Sharded Instances Model: (9 + 5) * $0.052 = $1.02/hr with 28 CPU</li>\n</ul>\n<div>\nWe have moved from one model being 10% more expensive to the other model being 100% as expensive.</div>\n<h3>\nTake Away</h3>\n<div>\nFlexibility in initial design to support independent cloud instances with low resource requirements as well as sharding may be a key cost control mechanism for cloud applications.</div>\n<div>\n<br>\n</div>\n<div>\nIt is impossible to a-priori determine the optimal design and deployment strategy. It needs to determined by experiment. To do experiments cheaply, means that the components and architecture must be designed to support experimentation.</div>\n<div>\n<br>\n</div>\n<div>\nIn some ways, cloud computing is like many phone plans -- you are forced to pay for a resource level and may not used all of resources that you pay for. Yes, the plans have steps, but if you need 18 GB of memory you may have to also pay for 8 CPUs that will never run more than 5% CPU usage (i.e. a single CPU is sufficient). Designing to support flexibility of cloud instances is essential for cost savings.</div>\n",
            "content": "Database sharding has been with us for many years. The concept of cloud instance sharding has not been discussed much. There is a significant financial incentive to do.<br><br>\nConsider a component that provides address and/or postal code validation around the world. For the sake of illustration, let us consider 10 regions that each have the same volume of data.<br><br>\nInitial tests found that it took 25 GB of data to load all of them in memory. Working of <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">AWS EC2 price list</a>, we find that a m4.2xlarge is needed to run it, at $0.479/hr. This gives us 8 CPUs.<br><br>\nIf we run with 10 @ 2.5 GB instead, we end up with 10 t2.medium, each with 2 CPU and a cost of $0.052/hr, or $0.52/hr -- which on first impression is more expensive, except we have 20 CPUs instead of 8 CPU. We may have better performance. If one of these instances is a hot spot (like US addresses), then we may end up with  9 instances that each support one region each and perhaps 5 instances supporting the US. As a single instance model, we may need 5 instances.<br><br>\nIn this case, we could end up with<br><br><ul>\n<li>Single Instance Model: 5 * $0.479 = $2.395/hr with 40 CPU</li>\n<li>Sharded Instances Model: (9 + 5) * $0.052 = $1.02/hr with 28 CPU</li>\n</ul>\n<div>\nWe have moved from one model being 10% more expensive to the other model being 100% as expensive.</div>\n<h3>\nTake Away</h3>\n<div>\nFlexibility in initial design to support independent cloud instances with low resource requirements as well as sharding may be a key cost control mechanism for cloud applications.</div>\n<div>\n<br>\n</div>\n<div>\nIt is impossible to a-priori determine the optimal design and deployment strategy. It needs to determined by experiment. To do experiments cheaply, means that the components and architecture must be designed to support experimentation.</div>\n<div>\n<br>\n</div>\n<div>\nIn some ways, cloud computing is like many phone plans -- you are forced to pay for a resource level and may not used all of resources that you pay for. Yes, the plans have steps, but if you need 18 GB of memory you may have to also pay for 8 CPUs that will never run more than 5% CPU usage (i.e. a single CPU is sufficient). Designing to support flexibility of cloud instances is essential for cost savings.</div>\n",
            "enclosure": [],
            "categories": []
        },
        {
            "title": "An Financially Frugal Architectural Pattern for the Cloud",
            "pubDate": "2016-01-17 05:35:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/an-financially-frugal-architectural.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-7131982772554875946",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s72-c/simple.PNG",
            "description": "I have heard many companies complain about how expensive the cloud is becoming as they moved from development to production systems. In theory, the saved costs of greatly reduced staffing of Site Reliability Engineers and reduced hardware costs should compensate -- key word is should.  In reality, this reduction never happens because they are needed to support other systems that will not be migrated for years.<br><br>\nThere is actually another problem, the architecture is not designed for the pricing model.<br><br>\nIn the last few years there have been many changes in the application environment, and I suspect many current architectures are locked into past system design patterns. To understand my proposal better, we need to look at the patterns thru the decades.<br><br>\nThe starting point is the classic client server: Many Clients - one Server - one database server (possibly many databases)<br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s1600/simple.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"228\" src=\"http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s640/simple.PNG\" width=\"640\"></a>\n</div>\nAs application volume grew, we ended up with multiple servers to handle multiple clients but retaining a single database.<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-gioFIyOFxlQ/VpsXEe5FDoI/AAAAAAAAI_I/NRT-pwXOYN0/s1600/simple2.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"248\" src=\"http://3.bp.blogspot.com/-gioFIyOFxlQ/VpsXEe5FDoI/AAAAAAAAI_I/NRT-pwXOYN0/s640/simple2.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\nMany variations arose, especially with databases - federated, sharding etc. The next innovation was remote procedure calls with many dialects such as SOAP, REST, AJAX etc. The typical manifestation is shown below.</div>\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-G5727E4QGuY/VpsZNhEjF-I/AAAAAAAAI_U/_IG9FDcz3cA/s1600/web.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"250\" src=\"http://1.bp.blogspot.com/-G5727E4QGuY/VpsZNhEjF-I/AAAAAAAAI_U/_IG9FDcz3cA/s640/web.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\nWhen the cloud came along,the above architecture was too often just moved off physical machines to cloud machines without any further examination.</div>\n<div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-d7YiTOw5qlE/VpsZyLCEdpI/AAAAAAAAI_c/7TjD-tNH_C0/s1600/cloud.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"242\" src=\"http://4.bp.blogspot.com/-d7YiTOw5qlE/VpsZyLCEdpI/AAAAAAAAI_c/7TjD-tNH_C0/s640/cloud.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nOften they will be minor changes, if a queue service was being used with the onsite service concurrent with the application server, it may be spawned off to a separate cloud instance. Applications are often design for the past model of all on one machine. It is rare when an existing application is moved to the cloud that it is design-refactored significantly. I have also seen new cloud base application be implemented in the classic single machine pattern.</div>\n<h2>\nThe Design Problem</h2>\n<div>\nThe artifact architecture of an application consisting of dozens, often over 100 libraries (for example C++ dll's), It's a megalith rooted in the original design being for one PC. </div>\n<div>\n<br>\n</div>\n<div>\nConsider the following case: <i>Suppose that instead of running these 100 libraries on a high end cloud machines with say 20 instances, you run each library on it's own light-weight machine?</i> Some libraries may only need two or three light-weight machines to handle the load. Others may need 20 instances because it is computationally intense and a hot spot. If you are doing auto-scaling, then the time to spin-up a new instance is much less when instances are library based -- because it is only one library. </div>\n<div>\n<br>\n</div>\n<div>\nFor the sake of argument, suppose that each of the 100 libraries require 0.4 GB to run. So to load all of them in one instance we are talking 40GB (100 x 0.4).</div>\n<div>\n<br>\n</div>\n<div>\nLooking at the <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">current AWS EC2 pricing</a>, we could use 100 instances of the t2.nano and have $0.0065 x 100 = $0.65/hour for all 100 instances with 1 CPU each (100 CPU total). The 40GB would require c3.8xlarge at $1.68/hour, 3 times the cost and only 32 cores instead of 100 cores. Three times the cost and 1/3 of the cores... sounds like<span><b> our bill could be 9 times what is needed.</b></span>\n</div>\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-fLCSY_EnYp0/Vpsma149mQI/AAAAAAAAI_s/d-GOp9KOJqo/s1600/Pi.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"262\" src=\"http://1.bp.blogspot.com/-fLCSY_EnYp0/Vpsma149mQI/AAAAAAAAI_s/d-GOp9KOJqo/s640/Pi.PNG\" width=\"640\"></a>\n</div>\n<div>\n<span><b><br></b></span>\n</div>\n<div>\n<br>\n</div>\n<div>\nWhat about scaling, with the megalith, you have to spin up a new complete instance. With the decomposition into library components, you only need to spin up new instances<i> of the library that needs it</i>. In other words, scaling up become significantly more expensive with the megalith model.<br><br>\nWhat is another way to describe this? <b>Microservices</b>\n</div>\n<div>\n<br>\n</div>\n<div>\nThis is a constructed example but it does illustrate that moving the application to the cloud may require appropriate redesign with a heavy focus on building components to run independently on the cheapest instances. Each swarm of these component-instances are load balanced with very fast creation of new instances. <br><br>\nHaving a faster creation of instances actually save more money because the triggering condition can be set higher (and thus triggered less often - less false positives). You want to create instances so they are there when the load build to require them. The longer the time it takes to load the instance, the longer lead time you need need, which means the lower on the build curve you must set the trigger point. </div>\n<div>\n<br>\n</div>\n<div>\nThere is additional savings for deployments, because you can deploy at the library level to specific machines instead of having to deploy a big image. Deploys are faster, rollbacks are faster.</div>\n<div>\n<br>\n</div>\n<div>\nAmazon actually does this approach internally with hundreds of services (each on their own physical or virtual machine) backing their web site. A new feature is rarely integrated into the \"stack\", instead it is added as a service that can actually be turned off or on on production by setting appropriate cookies in the production environment. There is limited need for a sandbox environment because the new feature is not there for the public -- only for internal people that know how to turn it on.</div>\n<h2>\nWhat is the key rhetorical question to keep asking?</h2>\n<div>\n<b>Why</b> are we having most of the application on one instance instead of <span>\"divide and save money\"?</span>  This question should be constantly asked during design reviews.</div>\n<div>\n<br>\n</div>\n<div>\nIn some ways, a design goal would be to design the application so it could run on a room full of PI's.</div>\n<div>\n<br>\n</div>\n<div>\nThis design approach does increase complexity -- just like multi-threading and/or async operations adds complexity but with significant payback. The process of designing libraries to minimize the number of inter-instances call while striving to minimize the resource requirements is a design challenge that will likely require mathematical / operations research skills.<br><br><h3>\nHow to convert an existing application?</h3>\n</div>\n<div>\nA few simple rules to get the little gray cells firing:</div>\n<div>\n<ul>\n<li>Identify methods that are static - those are ideal for mini-instances</li>\n<li>Backtrack from these methods into the callers and build up clusters of objects that can function independently.</li>\n<ul>\n<li>There may be refactoring because often designs go bad under pressure to deliver functionality</li>\n<li>You want to minimize external (inter-component-instances) calls from each of these clusters</li>\n</ul>\n<li>If the system is not dependent on dozens of component-instance deployments there may be a problem.</li>\n<ul>\n<li>If changing the internal code of a method requires a full deployment, there is a problem</li>\n</ul>\n</ul>\n<div>\nOne of the anti-patterns for effective-frugal cloud base design is actually object-orientated (as compared to cost-orientated) design. I programmed in Simula and worked in GPSS -- the \"Adam and Eve\" of object programming. All of the early literature was based on the single CPU reality of computing then. I have often had to go in and totally refactor an academically correct objective system design in order to get performance. Today, a refactor would also need to get lower costs.</div>\n</div>\n<div>\n<br>\n</div>\n<div>\nThe worst case system code that I refactored for performance was implemented as an Entity Model in C++, a single call from a web front end went thru some 20 classes/instances in a beautiful conceptual model, with something like 45 separate calls to the database. My refactoring resulted in one class and a single stored procedure (whose result was cached for 5 minutes before rolling off or being marked stale).</div>\n<div>\n<br>\n</div>\n<div>\nI believe that similar design inefficiencies are common in cloud architecture.<br><br>\nWhen you owned the hardware, each machine increased labor cost to create, license, update and support. You have considerable financial and human pressure to minimize machines. When you move to the cloud with good script automation, having 3 instances or 3000 instances should be approximately the same work. You actually have financial pressure to shift to the model that minimizes costs -- this will often be with many many more machines.</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<br>\n",
            "content": "I have heard many companies complain about how expensive the cloud is becoming as they moved from development to production systems. In theory, the saved costs of greatly reduced staffing of Site Reliability Engineers and reduced hardware costs should compensate -- key word is should.  In reality, this reduction never happens because they are needed to support other systems that will not be migrated for years.<br><br>\nThere is actually another problem, the architecture is not designed for the pricing model.<br><br>\nIn the last few years there have been many changes in the application environment, and I suspect many current architectures are locked into past system design patterns. To understand my proposal better, we need to look at the patterns thru the decades.<br><br>\nThe starting point is the classic client server: Many Clients - one Server - one database server (possibly many databases)<br><div class=\"separator\">\n<a href=\"http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s1600/simple.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"228\" src=\"http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s640/simple.PNG\" width=\"640\"></a>\n</div>\nAs application volume grew, we ended up with multiple servers to handle multiple clients but retaining a single database.<br><div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-gioFIyOFxlQ/VpsXEe5FDoI/AAAAAAAAI_I/NRT-pwXOYN0/s1600/simple2.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"248\" src=\"http://3.bp.blogspot.com/-gioFIyOFxlQ/VpsXEe5FDoI/AAAAAAAAI_I/NRT-pwXOYN0/s640/simple2.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\nMany variations arose, especially with databases - federated, sharding etc. The next innovation was remote procedure calls with many dialects such as SOAP, REST, AJAX etc. The typical manifestation is shown below.</div>\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-G5727E4QGuY/VpsZNhEjF-I/AAAAAAAAI_U/_IG9FDcz3cA/s1600/web.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"250\" src=\"http://1.bp.blogspot.com/-G5727E4QGuY/VpsZNhEjF-I/AAAAAAAAI_U/_IG9FDcz3cA/s640/web.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\nWhen the cloud came along,the above architecture was too often just moved off physical machines to cloud machines without any further examination.</div>\n<div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-d7YiTOw5qlE/VpsZyLCEdpI/AAAAAAAAI_c/7TjD-tNH_C0/s1600/cloud.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"242\" src=\"http://4.bp.blogspot.com/-d7YiTOw5qlE/VpsZyLCEdpI/AAAAAAAAI_c/7TjD-tNH_C0/s640/cloud.PNG\" width=\"640\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nOften they will be minor changes, if a queue service was being used with the onsite service concurrent with the application server, it may be spawned off to a separate cloud instance. Applications are often design for the past model of all on one machine. It is rare when an existing application is moved to the cloud that it is design-refactored significantly. I have also seen new cloud base application be implemented in the classic single machine pattern.</div>\n<h2>\nThe Design Problem</h2>\n<div>\nThe artifact architecture of an application consisting of dozens, often over 100 libraries (for example C++ dll's), It's a megalith rooted in the original design being for one PC. </div>\n<div>\n<br>\n</div>\n<div>\nConsider the following case: <i>Suppose that instead of running these 100 libraries on a high end cloud machines with say 20 instances, you run each library on it's own light-weight machine?</i> Some libraries may only need two or three light-weight machines to handle the load. Others may need 20 instances because it is computationally intense and a hot spot. If you are doing auto-scaling, then the time to spin-up a new instance is much less when instances are library based -- because it is only one library. </div>\n<div>\n<br>\n</div>\n<div>\nFor the sake of argument, suppose that each of the 100 libraries require 0.4 GB to run. So to load all of them in one instance we are talking 40GB (100 x 0.4).</div>\n<div>\n<br>\n</div>\n<div>\nLooking at the <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">current AWS EC2 pricing</a>, we could use 100 instances of the t2.nano and have $0.0065 x 100 = $0.65/hour for all 100 instances with 1 CPU each (100 CPU total). The 40GB would require c3.8xlarge at $1.68/hour, 3 times the cost and only 32 cores instead of 100 cores. Three times the cost and 1/3 of the cores... sounds like<span><b> our bill could be 9 times what is needed.</b></span>\n</div>\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-fLCSY_EnYp0/Vpsma149mQI/AAAAAAAAI_s/d-GOp9KOJqo/s1600/Pi.PNG\" imageanchor=\"1\"><img border=\"0\" height=\"262\" src=\"http://1.bp.blogspot.com/-fLCSY_EnYp0/Vpsma149mQI/AAAAAAAAI_s/d-GOp9KOJqo/s640/Pi.PNG\" width=\"640\"></a>\n</div>\n<div>\n<span><b><br></b></span>\n</div>\n<div>\n<br>\n</div>\n<div>\nWhat about scaling, with the megalith, you have to spin up a new complete instance. With the decomposition into library components, you only need to spin up new instances<i> of the library that needs it</i>. In other words, scaling up become significantly more expensive with the megalith model.<br><br>\nWhat is another way to describe this? <b>Microservices</b>\n</div>\n<div>\n<br>\n</div>\n<div>\nThis is a constructed example but it does illustrate that moving the application to the cloud may require appropriate redesign with a heavy focus on building components to run independently on the cheapest instances. Each swarm of these component-instances are load balanced with very fast creation of new instances. <br><br>\nHaving a faster creation of instances actually save more money because the triggering condition can be set higher (and thus triggered less often - less false positives). You want to create instances so they are there when the load build to require them. The longer the time it takes to load the instance, the longer lead time you need need, which means the lower on the build curve you must set the trigger point. </div>\n<div>\n<br>\n</div>\n<div>\nThere is additional savings for deployments, because you can deploy at the library level to specific machines instead of having to deploy a big image. Deploys are faster, rollbacks are faster.</div>\n<div>\n<br>\n</div>\n<div>\nAmazon actually does this approach internally with hundreds of services (each on their own physical or virtual machine) backing their web site. A new feature is rarely integrated into the \"stack\", instead it is added as a service that can actually be turned off or on on production by setting appropriate cookies in the production environment. There is limited need for a sandbox environment because the new feature is not there for the public -- only for internal people that know how to turn it on.</div>\n<h2>\nWhat is the key rhetorical question to keep asking?</h2>\n<div>\n<b>Why</b> are we having most of the application on one instance instead of <span>\"divide and save money\"?</span>  This question should be constantly asked during design reviews.</div>\n<div>\n<br>\n</div>\n<div>\nIn some ways, a design goal would be to design the application so it could run on a room full of PI's.</div>\n<div>\n<br>\n</div>\n<div>\nThis design approach does increase complexity -- just like multi-threading and/or async operations adds complexity but with significant payback. The process of designing libraries to minimize the number of inter-instances call while striving to minimize the resource requirements is a design challenge that will likely require mathematical / operations research skills.<br><br><h3>\nHow to convert an existing application?</h3>\n</div>\n<div>\nA few simple rules to get the little gray cells firing:</div>\n<div>\n<ul>\n<li>Identify methods that are static - those are ideal for mini-instances</li>\n<li>Backtrack from these methods into the callers and build up clusters of objects that can function independently.</li>\n<ul>\n<li>There may be refactoring because often designs go bad under pressure to deliver functionality</li>\n<li>You want to minimize external (inter-component-instances) calls from each of these clusters</li>\n</ul>\n<li>If the system is not dependent on dozens of component-instance deployments there may be a problem.</li>\n<ul>\n<li>If changing the internal code of a method requires a full deployment, there is a problem</li>\n</ul>\n</ul>\n<div>\nOne of the anti-patterns for effective-frugal cloud base design is actually object-orientated (as compared to cost-orientated) design. I programmed in Simula and worked in GPSS -- the \"Adam and Eve\" of object programming. All of the early literature was based on the single CPU reality of computing then. I have often had to go in and totally refactor an academically correct objective system design in order to get performance. Today, a refactor would also need to get lower costs.</div>\n</div>\n<div>\n<br>\n</div>\n<div>\nThe worst case system code that I refactored for performance was implemented as an Entity Model in C++, a single call from a web front end went thru some 20 classes/instances in a beautiful conceptual model, with something like 45 separate calls to the database. My refactoring resulted in one class and a single stored procedure (whose result was cached for 5 minutes before rolling off or being marked stale).</div>\n<div>\n<br>\n</div>\n<div>\nI believe that similar design inefficiencies are common in cloud architecture.<br><br>\nWhen you owned the hardware, each machine increased labor cost to create, license, update and support. You have considerable financial and human pressure to minimize machines. When you move to the cloud with good script automation, having 3 instances or 3000 instances should be approximately the same work. You actually have financial pressure to shift to the model that minimizes costs -- this will often be with many many more machines.</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<div>\n<br>\n</div>\n<br>\n",
            "enclosure": {
                "thumbnail": "http://2.bp.blogspot.com/-z0TNZo_a-N0/VpsWPvsueWI/AAAAAAAAI_A/aY31cIlAWzk/s72-c/simple.PNG"
            },
            "categories": []
        },
        {
            "title": "A simple approach to getting all of the data out of Atlassian Jira",
            "pubDate": "2016-01-05 01:06:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2016/01/a-simple-approach-to-getting-all-of.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-5884079932839899214",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s72-c/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png",
            "description": "One of my current projects is getting data out of Jira into a DataMart to allow fast (and easy) analysis. A library such as <a href=\"http://techtalk.jirarestclient/\">TechTalk.JiraRestClient</a> provides a basic foundation but there is a nasty gotcha. Jira can be heavily customized, often with different projects having dozen of different and unique custom fields. So how can you do one size fits all?<br><br>\nYou could go down path of modifying the above code to enumerate all of the custom fields (and then have continuous work keeping them in sync) or try something like what I do below: exploiting that JSON and XML are interchangeable and XML in a SQL Server database can actually be real sweet to use.<br><br><h3>\nModifying JiraRestClient</h3>\nThe first step requires downloading the code from GitHub and modifying it.<br>\nIn JiraClient.cs method  <span>EnumerateIssuesByQueryInternal </span>add the following code.<br><div class=\"p1\">\n<span> <span>              <span class=\"s1\">var</span> issues = data.issues ?? <span class=\"s2\">Enumerable</span>.Empty&lt;<span class=\"s2\">Issue</span><tissuefields>&gt;();</tissuefields></span></span>\n</div>\n<div class=\"p1\">\n<span>         <span>       <span class=\"s1\">var</span> xml=<span class=\"s2\">JsonConvert</span>.DeserializeXmlNode( response.Content,<span class=\"s3\">\"json\"</span>);</span></span>\n</div>\n<div class=\"p2\">\n<span><span><span class=\"s4\">                </span>// Insert all of the XML-JSON into </span></span>\n</div>\n<div class=\"p1\">\n<span><span>                <span class=\"s1\">foreach</span> (<span class=\"s1\">var</span> issue <span class=\"s1\">in</span> issues)</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                {</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                   <span class=\"s1\">var</span> testNode=xml.SelectSingleNode(<span class=\"s1\">string</span>.Format(<span class=\"s3\">\"//issues/key[text()='{0}']/..\"</span>, issue.key));</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    <span class=\"s1\">if</span>(testNode !=<span class=\"s1\">null</span>)</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    {</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                        issue.xml = testNode.OuterXml;</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    };</span></span>\n</div>\n<br><div class=\"p1\">\n<span><span>                }</span></span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>You will also need to modify the issue class to include a string, \"xml\".  The result is an issue class containing all of the information from the REST response. </span>\n</div>\n<h3>\n<span>Moving Issues into a Data Table</span>\n</h3>\n<div>\n<span>Once you have the issue by issue REST JSON response converted to XML, we need to move it to our storage. </span><span>My destination is SQL server and I will exploit <b>SQL Table variables</b> to make the process simple and use set operations. In short, I move the enumeration of issues into a C# data table so I may pass the data to SQL Server.</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p1\">\n<span>     <span>              <span class=\"s1\">var</span> upload = <span class=\"s1\">new</span> <span class=\"s2\">DataTable</span>();</span></span>\n</div>\n<div class=\"p1\">\n<span>                   // defining columns omitted</span>\n</div>\n<div class=\"p1\">\n<span>                    <span class=\"s1\">var</span> data = client.GetIssues(branch);</span>\n</div>\n<div class=\"p1\">\n<span>                    <span class=\"s1\">foreach</span> (<span class=\"s1\">var</span> issue <span class=\"s1\">in</span> data)</span>\n</div>\n<div class=\"p1\">\n<span>                        <span class=\"s1\">try</span></span>\n</div>\n<div class=\"p1\">\n<span>                        {</span>\n</div>\n<div class=\"p1\">\n<span>                            <span class=\"s1\">var</span> newRow = upload.NewRow();</span>\n</div>\n<div class=\"p1\">\n<span>                            newRow[Key] = issue.key;</span>\n</div>\n<div class=\"p1\">\n<span>                            newRow[Self] = issue.self;</span>\n</div>\n<div class=\"p1\">\n<span>                            // Other columns extracted</span>\n</div>\n<div class=\"p1\">\n<span><span>                            </span><span>newRow[Xml] = issue.xml;</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                            </span><span>upload.Rows.Add(newRow);</span></span>\n</div>\n<div class=\"p1\">\n<span>                        }</span>\n</div>\n<div class=\"p1\">\n<span>                        <span class=\"s1\">catch</span> (<span class=\"s2\">Exception</span> exc)</span>\n</div>\n<div class=\"p1\">\n<span>                        {</span>\n</div>\n<div class=\"p1\">\n<span>                            <span class=\"s2\">Console</span>.WriteLine(exc);</span>\n</div>\n<div class=\"p1\">\n<span>                        }</span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p1\">\n<span>     </span>               </div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<span>The upload code is also clean and simple:</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>         <span class=\"s1\">using</span> (<span class=\"s1\">var</span> cmd = <span class=\"s1\">new</span> <span class=\"s2\">SqlCommand</span> { CommandType = <span class=\"s2\">CommandType</span>.StoredProcedure, CommandText =<span>\"Jira.Upload1\"</span>})</span>\n</div>\n<div class=\"p1\">\n<span>                                <span class=\"s1\">using</span> (cmd.Connection = MyDataConnection)</span>\n</div>\n<div class=\"p1\">\n<span>                                {</span>\n</div>\n<div class=\"p1\">\n<span>                                    cmd.Parameters.AddWithValue(<span class=\"s3\">\"@Data\"</span>, upload);</span>\n</div>\n<div class=\"p1\">\n<span>                                     cmd.ExecuteNonQuery();</span>\n</div>\n<div class=\"p1\">\n<span>                                }</span>\n</div>\n<h3>\n<span>SQL Code</span>\n</h3>\n<div class=\"p1\">\n<span>For many C# developers, SQL is an unknown country, so I will go into some detail. <b>First</b>, we need to define the table in SQL, just match the DataTable in C# above (same column names in same sequence is best)</span>\n</div>\n<div class=\"p1\">\n<span><span><br></span></span>\n</div>\n<div class=\"p1\">\n<span>    CREATE TYPE [Jira].[JiraUpload1Type] AS TABLE(</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Key] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Assignee] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Description] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Reporter] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Status] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Summary] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[OriginalEstimate] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Labels] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Self] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[XmlData] [Xml] Null</span>\n</div>\n<div class=\"p1\">\n<span>        )</span>\n</div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<b>Note:</b> that I use (max) always -- which is pretty much how the C# datatable sees each column. Any data conversion to decimals will be done by SQL itself.</div>\n<div class=\"p1\">\n<b><br></b>\n</div>\n<div class=\"p1\">\n<b>Second</b>, we create the stored procedure. We want to update existing records and insert missing records. The code is simple and clean</div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<span>    CREATE PROC  [Jira].[Upload1] @Data [Jira].[JiraUpload1Type] READONLY</span>\n</div>\n<div class=\"p1\">\n<span>    AS </span>\n</div>\n<div class=\"p1\">\n<span>    Update Jira.Issue SET</span>\n</div>\n<div class=\"p1\">\n<span>      [Assignee] = D.Assignee</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Description] = D.Description</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Reporter] = D.Reporter</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Status] = D.Status</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Summary] = D.Summary</span>\n</div>\n<div class=\"p1\">\n<span>     ,[OriginalEstimate] = D.OriginalEstimate</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Labels] = D.Labels</span>\n</div>\n<div class=\"p1\">\n<span>     ,[XmlData] = D.XmlData</span>\n</div>\n<div class=\"p1\">\n<span>    From @Data D</span>\n</div>\n<div class=\"p1\">\n<span>    JOIN Jira.Issue S ON D.[Key]=S.[Key]</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>    INSERT INTO [Jira].[Issue]</span>\n</div>\n<div class=\"p1\">\n<span>           ([Key]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Assignee]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Description]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Reporter]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Status]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Summary]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[OriginalEstimate]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Labels]</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>   ,[XmlData])</span>\n</div>\n<div class=\"p1\">\n<span>    SELECT D.[Key]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Assignee]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Description]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Reporter]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Status]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Summary]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[OriginalEstimate]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Labels]</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>   ,D.[XmlData]</span>\n</div>\n<div class=\"p1\">\n<span>    From @Data D</span>\n</div>\n<div class=\"p1\">\n<span>    LEFT JOIN Jira.Issue S ON D.[Key]=S.[Key]</span>\n</div>\n<div class=\"p1\">\n<span>    WHERE S.[Key] Is Null</span>\n</div>\n<h3>\nAll of the Json is now in XML and can be search by Xpath</h3>\n<div>\nUpon executing the above, we see our table is populated as shown below. The far right column is XML.This is the SQL Xml data type and contains the REST JSON converted to XML for each issue.</div>\n<div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s1600/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png\" imageanchor=\"1\"><img border=\"0\" height=\"219\" src=\"http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s640/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png\" width=\"640\"></a>\n</div>\n<div>\n<br>\n</div>\nThe next step is often to add computed columns using the SQL XML and a xpath. An example of a generic solution is below.<br><h2>\nSo what is the advantage?</h2>\n<div>\nNo matter how many additional fields are added to Jira, you have 100% data capture here. There is no need to touch the Extract Transform Load (ETL) job. You can create (and index) the data in the XML in SQL server, or just hand back the XML to whatever is calling it.  While SQL Server 2016 supports JSON, XML is superior because of the ability to do XPaths into it as well as indices.</div>\n<div>\n<br>\n</div>\n<div>\nIn many implementations of JIRA, the number of fields can get unreal.. as shown below</div>\n<div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-S1-oG_53ix8/VosQu4qkNDI/AAAAAAAAI5U/FemeQ7o9drg/s1600/Screen%2BShot%2B2016-01-04%2Bat%2B4.37.56%2BPM.png\" imageanchor=\"1\"><img border=\"0\" height=\"320\" src=\"http://4.bp.blogspot.com/-S1-oG_53ix8/VosQu4qkNDI/AAAAAAAAI5U/FemeQ7o9drg/s320/Screen%2BShot%2B2016-01-04%2Bat%2B4.37.56%2BPM.png\" width=\"110\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nWith the same data table, you could create multiple views that contain computed columns showing precisely the data that you are interested in.</div>\n<div class=\"separator\">\n<br>\n</div>\n<h4>\nExample of Computed column definitions</h4>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[ProductionReleaseDate]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10705'</span><span class=\"s2\">,</span>[XmlData]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[EpicName]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10009'</span><span class=\"s2\">,</span>[</span><span>XmlData</span><span>]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"separator\">\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Sprint]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10007'</span><span class=\"s2\">,</span>[</span><span>XmlData</span><span>]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"p1\">\n<span class=\"s2\">With this Sql Function doing all of the work:</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s1\">    CREATE</span> <span class=\"s1\">FUNCTION</span> [dbo]<span class=\"s2\">.</span>[GetCustomField]</span>\n</div>\n<div class=\"p2\">\n<span>    (</span>\n</div>\n<div class=\"p1\">\n<span>    @Name <span class=\"s1\">varchar</span><span class=\"s2\">(</span>32<span class=\"s2\">),</span></span>\n</div>\n<div class=\"p1\">\n<span>    <span class=\"Apple-tab-span\"></span>@Data <span class=\"s1\">Xml</span></span>\n</div>\n<div class=\"p2\">\n<span>    )</span>\n</div>\n<div class=\"p3\">\n<span>    RETURNS<span class=\"s3\"> </span>varchar<span class=\"s2\">(</span><span class=\"s4\">max</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p3\">\n<span>    AS</span>\n</div>\n<div class=\"p3\">\n<span>    BEGIN</span>\n</div>\n<div class=\"p3\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span>DECLARE<span class=\"s3\"> @ResultVar </span>varchar<span class=\"s2\">(</span><span class=\"s4\">max</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p4\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span><span class=\"s1\">SELECT</span><span class=\"s3\">  @ResultVar </span><span class=\"s2\">=</span><span class=\"s3\"> c</span><span class=\"s2\">.</span><span class=\"s3\">value</span><span class=\"s2\">(</span>'customfieldvalues[1]'<span class=\"s2\">,</span>'varchar(max)'<span class=\"s2\">)</span><span class=\"s3\"> </span><span class=\"s1\">FROM</span><span class=\"s3\"> </span>    <span class=\"s3\">@Data</span><span class=\"s2\">.</span><span class=\"s3\">nodes</span><span class=\"s2\">(</span>'//customfield[@id]'<span class=\"s2\">)</span><span class=\"s3\"> </span><span class=\"s1\">as</span><span class=\"s3\"> t</span><span class=\"s2\">(</span><span class=\"s3\">c</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p4\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span><span class=\"s1\">WHERE</span><span class=\"s3\"> c</span><span class=\"s2\">.</span><span class=\"s3\">value</span><span class=\"s2\">(</span>'@id'<span class=\"s2\">,</span>'varchar(50)'<span class=\"s2\">)=</span><span class=\"s3\">@Name</span></span>\n</div>\n<div class=\"p1\">\n<span>    <span class=\"Apple-tab-span\"></span><span class=\"s1\">RETURN</span> @ResultVar</span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p3\">\n<span>    END</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThe net result is clean flexible code feeding into a database with very quick ability to extend. </div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nYou want to expose a new field? it's literally a one liner to add it as a column to the SQL Server table or view. Consider creating custom views on top of the table as a clean organized solution.</div>\n<div>\n<br>\n</div>\n",
            "content": "One of my current projects is getting data out of Jira into a DataMart to allow fast (and easy) analysis. A library such as <a href=\"http://techtalk.jirarestclient/\">TechTalk.JiraRestClient</a> provides a basic foundation but there is a nasty gotcha. Jira can be heavily customized, often with different projects having dozen of different and unique custom fields. So how can you do one size fits all?<br><br>\nYou could go down path of modifying the above code to enumerate all of the custom fields (and then have continuous work keeping them in sync) or try something like what I do below: exploiting that JSON and XML are interchangeable and XML in a SQL Server database can actually be real sweet to use.<br><br><h3>\nModifying JiraRestClient</h3>\nThe first step requires downloading the code from GitHub and modifying it.<br>\nIn JiraClient.cs method  <span>EnumerateIssuesByQueryInternal </span>add the following code.<br><div class=\"p1\">\n<span> <span>              <span class=\"s1\">var</span> issues = data.issues ?? <span class=\"s2\">Enumerable</span>.Empty&lt;<span class=\"s2\">Issue</span><tissuefields>&gt;();</tissuefields></span></span>\n</div>\n<div class=\"p1\">\n<span>         <span>       <span class=\"s1\">var</span> xml=<span class=\"s2\">JsonConvert</span>.DeserializeXmlNode( response.Content,<span class=\"s3\">\"json\"</span>);</span></span>\n</div>\n<div class=\"p2\">\n<span><span><span class=\"s4\">                </span>// Insert all of the XML-JSON into </span></span>\n</div>\n<div class=\"p1\">\n<span><span>                <span class=\"s1\">foreach</span> (<span class=\"s1\">var</span> issue <span class=\"s1\">in</span> issues)</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                {</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                   <span class=\"s1\">var</span> testNode=xml.SelectSingleNode(<span class=\"s1\">string</span>.Format(<span class=\"s3\">\"//issues/key[text()='{0}']/..\"</span>, issue.key));</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    <span class=\"s1\">if</span>(testNode !=<span class=\"s1\">null</span>)</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    {</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                        issue.xml = testNode.OuterXml;</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                    };</span></span>\n</div>\n<br><div class=\"p1\">\n<span><span>                }</span></span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>You will also need to modify the issue class to include a string, \"xml\".  The result is an issue class containing all of the information from the REST response. </span>\n</div>\n<h3>\n<span>Moving Issues into a Data Table</span>\n</h3>\n<div>\n<span>Once you have the issue by issue REST JSON response converted to XML, we need to move it to our storage. </span><span>My destination is SQL server and I will exploit <b>SQL Table variables</b> to make the process simple and use set operations. In short, I move the enumeration of issues into a C# data table so I may pass the data to SQL Server.</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p1\">\n<span>     <span>              <span class=\"s1\">var</span> upload = <span class=\"s1\">new</span> <span class=\"s2\">DataTable</span>();</span></span>\n</div>\n<div class=\"p1\">\n<span>                   // defining columns omitted</span>\n</div>\n<div class=\"p1\">\n<span>                    <span class=\"s1\">var</span> data = client.GetIssues(branch);</span>\n</div>\n<div class=\"p1\">\n<span>                    <span class=\"s1\">foreach</span> (<span class=\"s1\">var</span> issue <span class=\"s1\">in</span> data)</span>\n</div>\n<div class=\"p1\">\n<span>                        <span class=\"s1\">try</span></span>\n</div>\n<div class=\"p1\">\n<span>                        {</span>\n</div>\n<div class=\"p1\">\n<span>                            <span class=\"s1\">var</span> newRow = upload.NewRow();</span>\n</div>\n<div class=\"p1\">\n<span>                            newRow[Key] = issue.key;</span>\n</div>\n<div class=\"p1\">\n<span>                            newRow[Self] = issue.self;</span>\n</div>\n<div class=\"p1\">\n<span>                            // Other columns extracted</span>\n</div>\n<div class=\"p1\">\n<span><span>                            </span><span>newRow[Xml] = issue.xml;</span></span>\n</div>\n<div class=\"p1\">\n<span><span>                            </span><span>upload.Rows.Add(newRow);</span></span>\n</div>\n<div class=\"p1\">\n<span>                        }</span>\n</div>\n<div class=\"p1\">\n<span>                        <span class=\"s1\">catch</span> (<span class=\"s2\">Exception</span> exc)</span>\n</div>\n<div class=\"p1\">\n<span>                        {</span>\n</div>\n<div class=\"p1\">\n<span>                            <span class=\"s2\">Console</span>.WriteLine(exc);</span>\n</div>\n<div class=\"p1\">\n<span>                        }</span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p1\">\n<span>     </span>               </div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<span>The upload code is also clean and simple:</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>         <span class=\"s1\">using</span> (<span class=\"s1\">var</span> cmd = <span class=\"s1\">new</span> <span class=\"s2\">SqlCommand</span> { CommandType = <span class=\"s2\">CommandType</span>.StoredProcedure, CommandText =<span>\"Jira.Upload1\"</span>})</span>\n</div>\n<div class=\"p1\">\n<span>                                <span class=\"s1\">using</span> (cmd.Connection = MyDataConnection)</span>\n</div>\n<div class=\"p1\">\n<span>                                {</span>\n</div>\n<div class=\"p1\">\n<span>                                    cmd.Parameters.AddWithValue(<span class=\"s3\">\"@Data\"</span>, upload);</span>\n</div>\n<div class=\"p1\">\n<span>                                     cmd.ExecuteNonQuery();</span>\n</div>\n<div class=\"p1\">\n<span>                                }</span>\n</div>\n<h3>\n<span>SQL Code</span>\n</h3>\n<div class=\"p1\">\n<span>For many C# developers, SQL is an unknown country, so I will go into some detail. <b>First</b>, we need to define the table in SQL, just match the DataTable in C# above (same column names in same sequence is best)</span>\n</div>\n<div class=\"p1\">\n<span><span><br></span></span>\n</div>\n<div class=\"p1\">\n<span>    CREATE TYPE [Jira].[JiraUpload1Type] AS TABLE(</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Key] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Assignee] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Description] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Reporter] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Status] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Summary] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[OriginalEstimate] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Labels] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Self] [varchar](max) NULL,</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[XmlData] [Xml] Null</span>\n</div>\n<div class=\"p1\">\n<span>        )</span>\n</div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<b>Note:</b> that I use (max) always -- which is pretty much how the C# datatable sees each column. Any data conversion to decimals will be done by SQL itself.</div>\n<div class=\"p1\">\n<b><br></b>\n</div>\n<div class=\"p1\">\n<b>Second</b>, we create the stored procedure. We want to update existing records and insert missing records. The code is simple and clean</div>\n<div class=\"p1\">\n<br>\n</div>\n<div class=\"p1\">\n<span>    CREATE PROC  [Jira].[Upload1] @Data [Jira].[JiraUpload1Type] READONLY</span>\n</div>\n<div class=\"p1\">\n<span>    AS </span>\n</div>\n<div class=\"p1\">\n<span>    Update Jira.Issue SET</span>\n</div>\n<div class=\"p1\">\n<span>      [Assignee] = D.Assignee</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Description] = D.Description</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Reporter] = D.Reporter</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Status] = D.Status</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Summary] = D.Summary</span>\n</div>\n<div class=\"p1\">\n<span>     ,[OriginalEstimate] = D.OriginalEstimate</span>\n</div>\n<div class=\"p1\">\n<span>     ,[Labels] = D.Labels</span>\n</div>\n<div class=\"p1\">\n<span>     ,[XmlData] = D.XmlData</span>\n</div>\n<div class=\"p1\">\n<span>    From @Data D</span>\n</div>\n<div class=\"p1\">\n<span>    JOIN Jira.Issue S ON D.[Key]=S.[Key]</span>\n</div>\n<div class=\"p1\">\n<span><br></span>\n</div>\n<div class=\"p1\">\n<span>    INSERT INTO [Jira].[Issue]</span>\n</div>\n<div class=\"p1\">\n<span>           ([Key]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Assignee]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Description]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Reporter]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Status]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Summary]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[OriginalEstimate]</span>\n</div>\n<div class=\"p1\">\n<span>           ,[Labels]</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>   ,[XmlData])</span>\n</div>\n<div class=\"p1\">\n<span>    SELECT D.[Key]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Assignee]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Description]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Reporter]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Status]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Summary]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[OriginalEstimate]</span>\n</div>\n<div class=\"p1\">\n<span>           ,D.[Labels]</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>   ,D.[XmlData]</span>\n</div>\n<div class=\"p1\">\n<span>    From @Data D</span>\n</div>\n<div class=\"p1\">\n<span>    LEFT JOIN Jira.Issue S ON D.[Key]=S.[Key]</span>\n</div>\n<div class=\"p1\">\n<span>    WHERE S.[Key] Is Null</span>\n</div>\n<h3>\nAll of the Json is now in XML and can be search by Xpath</h3>\n<div>\nUpon executing the above, we see our table is populated as shown below. The far right column is XML.This is the SQL Xml data type and contains the REST JSON converted to XML for each issue.</div>\n<div class=\"separator\">\n<a href=\"http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s1600/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png\" imageanchor=\"1\"><img border=\"0\" height=\"219\" src=\"http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s640/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png\" width=\"640\"></a>\n</div>\n<div>\n<br>\n</div>\nThe next step is often to add computed columns using the SQL XML and a xpath. An example of a generic solution is below.<br><h2>\nSo what is the advantage?</h2>\n<div>\nNo matter how many additional fields are added to Jira, you have 100% data capture here. There is no need to touch the Extract Transform Load (ETL) job. You can create (and index) the data in the XML in SQL server, or just hand back the XML to whatever is calling it.  While SQL Server 2016 supports JSON, XML is superior because of the ability to do XPaths into it as well as indices.</div>\n<div>\n<br>\n</div>\n<div>\nIn many implementations of JIRA, the number of fields can get unreal.. as shown below</div>\n<div class=\"separator\">\n<a href=\"http://4.bp.blogspot.com/-S1-oG_53ix8/VosQu4qkNDI/AAAAAAAAI5U/FemeQ7o9drg/s1600/Screen%2BShot%2B2016-01-04%2Bat%2B4.37.56%2BPM.png\" imageanchor=\"1\"><img border=\"0\" height=\"320\" src=\"http://4.bp.blogspot.com/-S1-oG_53ix8/VosQu4qkNDI/AAAAAAAAI5U/FemeQ7o9drg/s320/Screen%2BShot%2B2016-01-04%2Bat%2B4.37.56%2BPM.png\" width=\"110\"></a>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nWith the same data table, you could create multiple views that contain computed columns showing precisely the data that you are interested in.</div>\n<div class=\"separator\">\n<br>\n</div>\n<h4>\nExample of Computed column definitions</h4>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[ProductionReleaseDate]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10705'</span><span class=\"s2\">,</span>[XmlData]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[EpicName]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10009'</span><span class=\"s2\">,</span>[</span><span>XmlData</span><span>]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"separator\">\n</div>\n<div class=\"p1\">\n<span><span class=\"Apple-tab-span\"> </span>[Sprint]  <span class=\"s1\">AS </span><span class=\"s2\">(</span>[dbo]<span class=\"s2\">.</span>[GetCustomField]<span class=\"s2\">(</span><span class=\"s3\">'customfield_10007'</span><span class=\"s2\">,</span>[</span><span>XmlData</span><span>]<span class=\"s2\">)),</span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"p1\">\n<span class=\"s2\">With this Sql Function doing all of the work:</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s1\">    CREATE</span> <span class=\"s1\">FUNCTION</span> [dbo]<span class=\"s2\">.</span>[GetCustomField]</span>\n</div>\n<div class=\"p2\">\n<span>    (</span>\n</div>\n<div class=\"p1\">\n<span>    @Name <span class=\"s1\">varchar</span><span class=\"s2\">(</span>32<span class=\"s2\">),</span></span>\n</div>\n<div class=\"p1\">\n<span>    <span class=\"Apple-tab-span\"></span>@Data <span class=\"s1\">Xml</span></span>\n</div>\n<div class=\"p2\">\n<span>    )</span>\n</div>\n<div class=\"p3\">\n<span>    RETURNS<span class=\"s3\"> </span>varchar<span class=\"s2\">(</span><span class=\"s4\">max</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p3\">\n<span>    AS</span>\n</div>\n<div class=\"p3\">\n<span>    BEGIN</span>\n</div>\n<div class=\"p3\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span>DECLARE<span class=\"s3\"> @ResultVar </span>varchar<span class=\"s2\">(</span><span class=\"s4\">max</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p4\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span><span class=\"s1\">SELECT</span><span class=\"s3\">  @ResultVar </span><span class=\"s2\">=</span><span class=\"s3\"> c</span><span class=\"s2\">.</span><span class=\"s3\">value</span><span class=\"s2\">(</span>'customfieldvalues[1]'<span class=\"s2\">,</span>'varchar(max)'<span class=\"s2\">)</span><span class=\"s3\"> </span><span class=\"s1\">FROM</span><span class=\"s3\"> </span>    <span class=\"s3\">@Data</span><span class=\"s2\">.</span><span class=\"s3\">nodes</span><span class=\"s2\">(</span>'//customfield[@id]'<span class=\"s2\">)</span><span class=\"s3\"> </span><span class=\"s1\">as</span><span class=\"s3\"> t</span><span class=\"s2\">(</span><span class=\"s3\">c</span><span class=\"s2\">)</span></span>\n</div>\n<div class=\"p4\">\n<span>    <span class=\"s3\"><span class=\"Apple-tab-span\"></span></span><span class=\"s1\">WHERE</span><span class=\"s3\"> c</span><span class=\"s2\">.</span><span class=\"s3\">value</span><span class=\"s2\">(</span>'@id'<span class=\"s2\">,</span>'varchar(50)'<span class=\"s2\">)=</span><span class=\"s3\">@Name</span></span>\n</div>\n<div class=\"p1\">\n<span>    <span class=\"Apple-tab-span\"></span><span class=\"s1\">RETURN</span> @ResultVar</span>\n</div>\n<div class=\"p1\">\n</div>\n<div class=\"p3\">\n<span>    END</span>\n</div>\n<div class=\"p1\">\n<span><span class=\"s2\"><br></span></span>\n</div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nThe net result is clean flexible code feeding into a database with very quick ability to extend. </div>\n<div class=\"separator\">\n<br>\n</div>\n<div class=\"separator\">\nYou want to expose a new field? it's literally a one liner to add it as a column to the SQL Server table or view. Consider creating custom views on top of the table as a clean organized solution.</div>\n<div>\n<br>\n</div>\n",
            "enclosure": {
                "thumbnail": "http://3.bp.blogspot.com/-2uy1BLsWlFY/VosPBLAAwoI/AAAAAAAAI5I/Dw6ixsGkMMU/s72-c/Screen%2BShot%2B2016-01-04%2Bat%2B4.30.55%2BPM.png"
            },
            "categories": [
                "REST",
                "jira",
                "json",
                "Newtonsoft",
                "Atlassian"
            ]
        },
        {
            "title": "Effortless getting data out of a JSON REST response",
            "pubDate": "2015-11-17 01:13:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2015/11/effortless-getting-data-out-of-json.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-7674720015899059426",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "One of the pains with using REST is getting the data from JSON into something usable. There is a very simple solution:  Take the JSON, pass it to a magic black box and get a dataset back that has foreign keys and other joys.   That sounds very nice --<br><br><ul>\n<li>Make the JSON REST call and then</li>\n<li>Query or filter data tables to do future processing. No need to define classes to deserialize the JSON into.....</li>\n</ul>\n<div>\nThe code<i><b> is horrible</b></i> and shown below...</div>\n<pre>using System;\nusing System.Linq;\nusing System.Data;\nusing System.Xml;\nusing Newtonsoft.Json;\nnamespace Avalara.AvaTax.JsonUtilities\n{\n public static class Utility\n {\n public static DataSet ConvertJsonToDataSet(string jsonText, string rootElementName)\n {\n var xd1 = new XmlDocument();\n xd1 =JsonConvert.DeserializeXmlNode( jsonText,rootElementName);\n var result = new DataSet();\n result.ReadXml(new XmlNodeReader(xd1));\n return result;\n } \n }\n}\n</pre>\n<br>\nTo put this into a fuller context, consider the code below that does a get to any REST JSON url and returns a dataset<br><pre></pre>\n<pre>public static DataSet GetDataSet(string url, string rootName = \"myroot\")\n{\n var webRequest = (HttpWebRequest)WebRequest.Create(url);\n webRequest.Method = \"GET\";\n webRequest.ContentType = \"application/json, *.*\";\n webRequest.UserAgent = \"Mozilla/5.0 (Windows NT 5.1; rv:28.0) Gecko/20100101 Firefox/28.0\";\n webRequest.Headers.Add(\"AUTHORIZATION\", Authorization);\n var webResponse = (HttpWebResponse)webRequest.GetResponse();\n if (webResponse.StatusCode != HttpStatusCode.OK) Console.WriteLine(\"{0}\", webResponse.Headers);\n var json = String.Empty;\n using (StreamReader reader = new StreamReader(webResponse.GetResponseStream()))\n {\n json = reader.ReadToEnd();\n reader.Close();\n }\n // We must name the root element\n return DropUnusedTables(Utility.ConvertJsonToDataSet(json, rootName));\n}\n</pre>\n<pre></pre>\n<pre><b><span>No longer do you need to deserialize to hand constructed classes to consume the data.</span></b></pre>\n",
            "content": "One of the pains with using REST is getting the data from JSON into something usable. There is a very simple solution:  Take the JSON, pass it to a magic black box and get a dataset back that has foreign keys and other joys.   That sounds very nice --<br><br><ul>\n<li>Make the JSON REST call and then</li>\n<li>Query or filter data tables to do future processing. No need to define classes to deserialize the JSON into.....</li>\n</ul>\n<div>\nThe code<i><b> is horrible</b></i> and shown below...</div>\n<pre>using System;\nusing System.Linq;\nusing System.Data;\nusing System.Xml;\nusing Newtonsoft.Json;\nnamespace Avalara.AvaTax.JsonUtilities\n{\n public static class Utility\n {\n public static DataSet ConvertJsonToDataSet(string jsonText, string rootElementName)\n {\n var xd1 = new XmlDocument();\n xd1 =JsonConvert.DeserializeXmlNode( jsonText,rootElementName);\n var result = new DataSet();\n result.ReadXml(new XmlNodeReader(xd1));\n return result;\n } \n }\n}\n</pre>\n<br>\nTo put this into a fuller context, consider the code below that does a get to any REST JSON url and returns a dataset<br><pre></pre>\n<pre>public static DataSet GetDataSet(string url, string rootName = \"myroot\")\n{\n var webRequest = (HttpWebRequest)WebRequest.Create(url);\n webRequest.Method = \"GET\";\n webRequest.ContentType = \"application/json, *.*\";\n webRequest.UserAgent = \"Mozilla/5.0 (Windows NT 5.1; rv:28.0) Gecko/20100101 Firefox/28.0\";\n webRequest.Headers.Add(\"AUTHORIZATION\", Authorization);\n var webResponse = (HttpWebResponse)webRequest.GetResponse();\n if (webResponse.StatusCode != HttpStatusCode.OK) Console.WriteLine(\"{0}\", webResponse.Headers);\n var json = String.Empty;\n using (StreamReader reader = new StreamReader(webResponse.GetResponseStream()))\n {\n json = reader.ReadToEnd();\n reader.Close();\n }\n // We must name the root element\n return DropUnusedTables(Utility.ConvertJsonToDataSet(json, rootName));\n}\n</pre>\n<pre></pre>\n<pre><b><span>No longer do you need to deserialize to hand constructed classes to consume the data.</span></b></pre>\n",
            "enclosure": [],
            "categories": [
                "REST",
                "json",
                "Newtonsoft"
            ]
        },
        {
            "title": "An example of the risk of not versioning REST",
            "pubDate": "2015-11-16 18:29:00",
            "link": "http://www.31a2ba2a-b718-11dc-8314-0800200c9a66.com/2015/11/an-example-of-risk-of-not-versioning.html",
            "guid": "tag:blogger.com,1999:blog-3923359343089034996.post-888974367645026220",
            "author": "Ken Lassesen, Dr.Gui (MSDN) retired",
            "thumbnail": "",
            "description": "A while back I was contacted to solve a quasi-nasty issue. An existing REST implementation had been updated with an extra field being added to the response. This worked fine for 99% of the consumers, but for one consumer it broke. This consumer wrote a package that they sold on to others and their customers were screaming.<br><br>\nThe reason it broke was that it was not coded to handled additional fields. Technically, <i><b>REST is an architectural pattern without standards</b></i>. Robustness in handling extra fields and data is what most developers would expect -- but that is not a requirement of REST. It is hopeful thinking.<br><br>\nIf the REST was well versioned, this issue would not have arisen. It did arise.<br><br>\nWhile this consumer can patch their code, getting the patch to all of their customers was problematic hence there was a need to do an immediate fix, somehow. Fortunately, their package allows the REST Url to be specified and that allow a simple quick solution. Create a \"Relay Website\" up on Azure that relays the data from the customers and remove this extra field in the response. All of the data was in JSON which reduced the scope of the issue.<br><br>\nThe code was actually trivial (<span class=\"s1\">using</span> Newtonsoft.Json.Linq;). As you can see, it is easy to eliminate as many fields as desired by just adding case statements:<br><br><div class=\"p1\">\n<br>\n</div>\n<pre> public class CorgiController : ApiController\n {\n [HttpPost]\n public JObject Get()\n {\n var jresponse = RestEcho.EchoPost();\n foreach (JToken item in jresponse.Children())\n WalkChildrenAndRemove(item);\n return jresponse;\n }\n\n\n [HttpPost]\n public JObject Cancel()\n {\n return RestEcho.EchoPost();\n }\n\n private void WalkChildrenAndRemove(JToken jitem)\n {\n if (jitem is JProperty)\n {\n var prop = jitem as JProperty;\n switch (prop.Name)\n {\n case \"Foobar\": jitem.Remove(); break;\n default:\n foreach (JToken item in jitem.Children())\n WalkChildrenAndRemove(item);\n break;\n }\n }\n else if (jitem is JArray)\n {\n var arr = (JArray)jitem;\n foreach (JToken item in arr)\n WalkChildrenAndRemove(item);\n }\n else\n {\n foreach (JToken item in jitem.Children().ToArray<jtoken>())\n WalkChildrenAndRemove(item);\n }\n }\n }\n}\n</jtoken></pre>\nWith the RestEcho class being also trivial,\n<br><br><pre> public static class RestEcho\n {\n public static JObject EchoPost()\n {\n var url = GetServer() + HttpContext.Current.Request.Path;\n var stream = new StreamReader(HttpContext.Current.Request.InputStream);\n var body = stream.ReadToEnd();\n var value = JObject.Parse(body);\n // Get the login and password sent\n HttpContext httpContext = HttpContext.Current;\n NameValueCollection headerList = httpContext.Request.Headers;\n var testHeader = headerList.Get(\"Authorization\");\n if (testHeader == null || testHeader.Length &lt; 7)\n {\n HttpContext.Current.Response.StatusCode = 401;\n HttpContext.Current.Response.StatusDescription = \"Basic Authentication Is Required\";\n HttpContext.Current.Response.Write(\"Failed to Authenticate\");\n HttpContext.Current.Response.End();\n }\n // remove \"BASIC \" from field\n var authorizationField = headerList.Get(\"Authorization\").Substring(6);\n HttpClient client = new HttpClient();\n client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Basic\", authorizationField);\n var response = client.PostAsJsonAsync<jobject>(url, value).Result;\n var jresponse = new JObject();\n try\n {\n jresponse = (JObject)response.Content.ReadAsAsync<jobject>().Result;\n }\n catch\n {\n\n }\n return jresponse;\n }\n </jobject></jobject></pre>\n<jobject><jobject>This pattern can also be used to reduce a version X to version 0 with likely less coding than alternative approaches -- after all, you just have to add case statements if the structure is the same.</jobject></jobject><br><jobject><jobject><br></jobject></jobject><jobject><jobject>This was tossed up onto Azure with running costs being well less than $10/month. </jobject></jobject><br><jobject><jobject><br></jobject></jobject><jobject><jobject>Happy customer. Happy customer's customers.</jobject></jobject>\n",
            "content": "A while back I was contacted to solve a quasi-nasty issue. An existing REST implementation had been updated with an extra field being added to the response. This worked fine for 99% of the consumers, but for one consumer it broke. This consumer wrote a package that they sold on to others and their customers were screaming.<br><br>\nThe reason it broke was that it was not coded to handled additional fields. Technically, <i><b>REST is an architectural pattern without standards</b></i>. Robustness in handling extra fields and data is what most developers would expect -- but that is not a requirement of REST. It is hopeful thinking.<br><br>\nIf the REST was well versioned, this issue would not have arisen. It did arise.<br><br>\nWhile this consumer can patch their code, getting the patch to all of their customers was problematic hence there was a need to do an immediate fix, somehow. Fortunately, their package allows the REST Url to be specified and that allow a simple quick solution. Create a \"Relay Website\" up on Azure that relays the data from the customers and remove this extra field in the response. All of the data was in JSON which reduced the scope of the issue.<br><br>\nThe code was actually trivial (<span class=\"s1\">using</span> Newtonsoft.Json.Linq;). As you can see, it is easy to eliminate as many fields as desired by just adding case statements:<br><br><div class=\"p1\">\n<br>\n</div>\n<pre> public class CorgiController : ApiController\n {\n [HttpPost]\n public JObject Get()\n {\n var jresponse = RestEcho.EchoPost();\n foreach (JToken item in jresponse.Children())\n WalkChildrenAndRemove(item);\n return jresponse;\n }\n\n\n [HttpPost]\n public JObject Cancel()\n {\n return RestEcho.EchoPost();\n }\n\n private void WalkChildrenAndRemove(JToken jitem)\n {\n if (jitem is JProperty)\n {\n var prop = jitem as JProperty;\n switch (prop.Name)\n {\n case \"Foobar\": jitem.Remove(); break;\n default:\n foreach (JToken item in jitem.Children())\n WalkChildrenAndRemove(item);\n break;\n }\n }\n else if (jitem is JArray)\n {\n var arr = (JArray)jitem;\n foreach (JToken item in arr)\n WalkChildrenAndRemove(item);\n }\n else\n {\n foreach (JToken item in jitem.Children().ToArray<jtoken>())\n WalkChildrenAndRemove(item);\n }\n }\n }\n}\n</jtoken></pre>\nWith the RestEcho class being also trivial,\n<br><br><pre> public static class RestEcho\n {\n public static JObject EchoPost()\n {\n var url = GetServer() + HttpContext.Current.Request.Path;\n var stream = new StreamReader(HttpContext.Current.Request.InputStream);\n var body = stream.ReadToEnd();\n var value = JObject.Parse(body);\n // Get the login and password sent\n HttpContext httpContext = HttpContext.Current;\n NameValueCollection headerList = httpContext.Request.Headers;\n var testHeader = headerList.Get(\"Authorization\");\n if (testHeader == null || testHeader.Length &lt; 7)\n {\n HttpContext.Current.Response.StatusCode = 401;\n HttpContext.Current.Response.StatusDescription = \"Basic Authentication Is Required\";\n HttpContext.Current.Response.Write(\"Failed to Authenticate\");\n HttpContext.Current.Response.End();\n }\n // remove \"BASIC \" from field\n var authorizationField = headerList.Get(\"Authorization\").Substring(6);\n HttpClient client = new HttpClient();\n client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Basic\", authorizationField);\n var response = client.PostAsJsonAsync<jobject>(url, value).Result;\n var jresponse = new JObject();\n try\n {\n jresponse = (JObject)response.Content.ReadAsAsync<jobject>().Result;\n }\n catch\n {\n\n }\n return jresponse;\n }\n </jobject></jobject></pre>\n<jobject><jobject>This pattern can also be used to reduce a version X to version 0 with likely less coding than alternative approaches -- after all, you just have to add case statements if the structure is the same.</jobject></jobject><br><jobject><jobject><br></jobject></jobject><jobject><jobject>This was tossed up onto Azure with running costs being well less than $10/month. </jobject></jobject><br><jobject><jobject><br></jobject></jobject><jobject><jobject>Happy customer. Happy customer's customers.</jobject></jobject>\n",
            "enclosure": [],
            "categories": [
                "REST",
                "json",
                "Newtonsoft"
            ]
        }
    ]
}